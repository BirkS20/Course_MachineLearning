<!--
Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Data Analysis and Machine Learning: Logistic Regression">

<title>Data Analysis and Machine Learning: Logistic Regression</title>

<!-- Bootstrap style: bootstrap -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->

<style type="text/css">

/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}

/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>


</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Plans for week 38', 2, None, '___sec0'),
              ('Thursday September 17', 2, None, '___sec1'),
              ('Ridge and LASSO Regression, reminder', 2, None, '___sec2'),
              ('Various steps in cross-validation', 2, None, '___sec3'),
              ('How to set up the cross-validation for Ridge and/or Lasso',
               2,
               None,
               '___sec4'),
              ('Cross-validation in brief', 2, None, '___sec5'),
              ('Code Example for Cross-validation and $k$-fold '
               'Cross-validation',
               2,
               None,
               '___sec6'),
              ('Bias-Variance tradeoff with Bootstrap', 2, None, '___sec7'),
              ("Another Example from Scikit-Learn's Repository",
               2,
               None,
               '___sec8'),
              ('Cross-validation with Ridge', 2, None, '___sec9'),
              ('The Ising model', 2, None, '___sec10'),
              ('Reformulating the problem to suit regression',
               2,
               None,
               '___sec11'),
              ('Linear regression', 2, None, '___sec12'),
              ('Singular Value decomposition', 2, None, '___sec13'),
              ('The one-dimensional Ising model', 2, None, '___sec14'),
              ('Ridge regression', 2, None, '___sec15'),
              ('LASSO regression', 2, None, '___sec16'),
              ('Performance as  function of the regularization parameter',
               2,
               None,
               '___sec17'),
              ('Finding the optimal value of $\\lambda$', 2, None, '___sec18'),
              ('Friday September 18: Intro to Logistic Regression',
               2,
               None,
               '___sec19'),
              ('Logistic Regression', 2, None, '___sec20'),
              ('Classification problems', 2, None, '___sec21'),
              ('Optimization and Deep learning', 2, None, '___sec22'),
              ('Basics', 2, None, '___sec23'),
              ('Linear classifier', 2, None, '___sec24'),
              ('Some selected properties', 2, None, '___sec25'),
              ('Simple example', 2, None, '___sec26'),
              ('Plotting the mean value for each group', 2, None, '___sec27'),
              ('The logistic function', 2, None, '___sec28'),
              ('Examples of likelihood functions used in logistic regression '
               'and nueral networks',
               2,
               None,
               '___sec29'),
              ('Two parameters', 2, None, '___sec30'),
              ('Maximum likelihood', 2, None, '___sec31'),
              ('The cost function rewritten', 2, None, '___sec32'),
              ('Minimizing the cross entropy', 2, None, '___sec33'),
              ('A more compact expression', 2, None, '___sec34'),
              ('Extending to more predictors', 2, None, '___sec35'),
              ('Including more classes', 2, None, '___sec36'),
              ('More classes', 2, None, '___sec37'),
              ('Cancer Data again now with Decision Trees and other Methods',
               2,
               None,
               '___sec38'),
              ('Other measures in classification studies: Cancer Data  again',
               2,
               None,
               '___sec39')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="week38-bs.html">Data Analysis and Machine Learning: Logistic Regression</a>
  </div>

  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="._week38-bs001.html#___sec0" style="font-size: 80%;">Plans for week 38</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs002.html#___sec1" style="font-size: 80%;">Thursday September 17</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs003.html#___sec2" style="font-size: 80%;">Ridge and LASSO Regression, reminder</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs004.html#___sec3" style="font-size: 80%;">Various steps in cross-validation</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs005.html#___sec4" style="font-size: 80%;">How to set up the cross-validation for Ridge and/or Lasso</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs006.html#___sec5" style="font-size: 80%;">Cross-validation in brief</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs007.html#___sec6" style="font-size: 80%;">Code Example for Cross-validation and \( k \)-fold Cross-validation</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs008.html#___sec7" style="font-size: 80%;">Bias-Variance tradeoff with Bootstrap</a></li>
     <!-- navigation toc: --> <li><a href="#___sec8" style="font-size: 80%;">Another Example from Scikit-Learn's Repository</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs010.html#___sec9" style="font-size: 80%;">Cross-validation with Ridge</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs011.html#___sec10" style="font-size: 80%;">The Ising model</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs012.html#___sec11" style="font-size: 80%;">Reformulating the problem to suit regression</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs013.html#___sec12" style="font-size: 80%;">Linear regression</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs014.html#___sec13" style="font-size: 80%;">Singular Value decomposition</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs015.html#___sec14" style="font-size: 80%;">The one-dimensional Ising model</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs016.html#___sec15" style="font-size: 80%;">Ridge regression</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs017.html#___sec16" style="font-size: 80%;">LASSO regression</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs018.html#___sec17" style="font-size: 80%;">Performance as  function of the regularization parameter</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs019.html#___sec18" style="font-size: 80%;">Finding the optimal value of \( \lambda \)</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs020.html#___sec19" style="font-size: 80%;">Friday September 18: Intro to Logistic Regression</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs021.html#___sec20" style="font-size: 80%;">Logistic Regression</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs022.html#___sec21" style="font-size: 80%;">Classification problems</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs023.html#___sec22" style="font-size: 80%;">Optimization and Deep learning</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs024.html#___sec23" style="font-size: 80%;">Basics</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs025.html#___sec24" style="font-size: 80%;">Linear classifier</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs026.html#___sec25" style="font-size: 80%;">Some selected properties</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs027.html#___sec26" style="font-size: 80%;">Simple example</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs028.html#___sec27" style="font-size: 80%;">Plotting the mean value for each group</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs029.html#___sec28" style="font-size: 80%;">The logistic function</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs030.html#___sec29" style="font-size: 80%;">Examples of likelihood functions used in logistic regression and nueral networks</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs031.html#___sec30" style="font-size: 80%;">Two parameters</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs032.html#___sec31" style="font-size: 80%;">Maximum likelihood</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs033.html#___sec32" style="font-size: 80%;">The cost function rewritten</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs034.html#___sec33" style="font-size: 80%;">Minimizing the cross entropy</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs035.html#___sec34" style="font-size: 80%;">A more compact expression</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs036.html#___sec35" style="font-size: 80%;">Extending to more predictors</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs037.html#___sec36" style="font-size: 80%;">Including more classes</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs038.html#___sec37" style="font-size: 80%;">More classes</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs039.html#___sec38" style="font-size: 80%;">Cancer Data again now with Decision Trees and other Methods</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs040.html#___sec39" style="font-size: 80%;">Other measures in classification studies: Cancer Data  again</a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->

<div class="container">

<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->

<a name="part0009"></a>
<!-- !split -->

<h2 id="___sec8" class="anchor">Another Example from Scikit-Learn's Repository </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">============================</span>
<span style="color: #BA2121; font-style: italic">Underfitting vs. Overfitting</span>
<span style="color: #BA2121; font-style: italic">============================</span>

<span style="color: #BA2121; font-style: italic">This example demonstrates the problems of underfitting and overfitting and</span>
<span style="color: #BA2121; font-style: italic">how we can use linear regression with polynomial features to approximate</span>
<span style="color: #BA2121; font-style: italic">nonlinear functions. The plot shows the function that we want to approximate,</span>
<span style="color: #BA2121; font-style: italic">which is a part of the cosine function. In addition, the samples from the</span>
<span style="color: #BA2121; font-style: italic">real function and the approximations of different models are displayed. The</span>
<span style="color: #BA2121; font-style: italic">models have polynomial features of different degrees. We can see that a</span>
<span style="color: #BA2121; font-style: italic">linear function (polynomial with degree 1) is not sufficient to fit the</span>
<span style="color: #BA2121; font-style: italic">training samples. This is called **underfitting**. A polynomial of degree 4</span>
<span style="color: #BA2121; font-style: italic">approximates the true function almost perfectly. However, for higher degrees</span>
<span style="color: #BA2121; font-style: italic">the model will **overfit** the training data, i.e. it learns the noise of the</span>
<span style="color: #BA2121; font-style: italic">training data.</span>
<span style="color: #BA2121; font-style: italic">We evaluate quantitatively **overfitting** / **underfitting** by using</span>
<span style="color: #BA2121; font-style: italic">cross-validation. We calculate the mean squared error (MSE) on the validation</span>
<span style="color: #BA2121; font-style: italic">set, the higher, the less likely the model generalizes correctly from the</span>
<span style="color: #BA2121; font-style: italic">training data.</span>
<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>

<span style="color: #008000; font-weight: bold">print</span>(<span style="color: #19177C">__doc__</span>)

<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.pipeline</span> <span style="color: #008000; font-weight: bold">import</span> Pipeline
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.preprocessing</span> <span style="color: #008000; font-weight: bold">import</span> PolynomialFeatures
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> LinearRegression
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> cross_val_score


<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">true_fun</span>(X):
    <span style="color: #008000; font-weight: bold">return</span> np<span style="color: #666666">.</span>cos(<span style="color: #666666">1.5</span> <span style="color: #666666">*</span> np<span style="color: #666666">.</span>pi <span style="color: #666666">*</span> X)

np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>seed(<span style="color: #666666">0</span>)

n_samples <span style="color: #666666">=</span> <span style="color: #666666">30</span>
degrees <span style="color: #666666">=</span> [<span style="color: #666666">1</span>, <span style="color: #666666">4</span>, <span style="color: #666666">15</span>]

X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>sort(np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(n_samples))
y <span style="color: #666666">=</span> true_fun(X) <span style="color: #666666">+</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randn(n_samples) <span style="color: #666666">*</span> <span style="color: #666666">0.1</span>

plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(<span style="color: #666666">14</span>, <span style="color: #666666">5</span>))
<span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #008000">len</span>(degrees)):
    ax <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>subplot(<span style="color: #666666">1</span>, <span style="color: #008000">len</span>(degrees), i <span style="color: #666666">+</span> <span style="color: #666666">1</span>)
    plt<span style="color: #666666">.</span>setp(ax, xticks<span style="color: #666666">=</span>(), yticks<span style="color: #666666">=</span>())

    polynomial_features <span style="color: #666666">=</span> PolynomialFeatures(degree<span style="color: #666666">=</span>degrees[i],
                                             include_bias<span style="color: #666666">=</span><span style="color: #008000">False</span>)
    linear_regression <span style="color: #666666">=</span> LinearRegression()
    pipeline <span style="color: #666666">=</span> Pipeline([(<span style="color: #BA2121">&quot;polynomial_features&quot;</span>, polynomial_features),
                         (<span style="color: #BA2121">&quot;linear_regression&quot;</span>, linear_regression)])
    pipeline<span style="color: #666666">.</span>fit(X[:, np<span style="color: #666666">.</span>newaxis], y)

    <span style="color: #408080; font-style: italic"># Evaluate the models using crossvalidation</span>
    scores <span style="color: #666666">=</span> cross_val_score(pipeline, X[:, np<span style="color: #666666">.</span>newaxis], y,
                             scoring<span style="color: #666666">=</span><span style="color: #BA2121">&quot;neg_mean_squared_error&quot;</span>, cv<span style="color: #666666">=10</span>)

    X_test <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(<span style="color: #666666">0</span>, <span style="color: #666666">1</span>, <span style="color: #666666">100</span>)
    plt<span style="color: #666666">.</span>plot(X_test, pipeline<span style="color: #666666">.</span>predict(X_test[:, np<span style="color: #666666">.</span>newaxis]), label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;Model&quot;</span>)
    plt<span style="color: #666666">.</span>plot(X_test, true_fun(X_test), label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;True function&quot;</span>)
    plt<span style="color: #666666">.</span>scatter(X, y, edgecolor<span style="color: #666666">=</span><span style="color: #BA2121">&#39;b&#39;</span>, s<span style="color: #666666">=20</span>, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;Samples&quot;</span>)
    plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&quot;x&quot;</span>)
    plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&quot;y&quot;</span>)
    plt<span style="color: #666666">.</span>xlim((<span style="color: #666666">0</span>, <span style="color: #666666">1</span>))
    plt<span style="color: #666666">.</span>ylim((<span style="color: #666666">-2</span>, <span style="color: #666666">2</span>))
    plt<span style="color: #666666">.</span>legend(loc<span style="color: #666666">=</span><span style="color: #BA2121">&quot;best&quot;</span>)
    plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&quot;Degree {}</span><span style="color: #BB6622; font-weight: bold">\n</span><span style="color: #BA2121">MSE = {:.2e}(+/- {:.2e})&quot;</span><span style="color: #666666">.</span>format(
        degrees[i], <span style="color: #666666">-</span>scores<span style="color: #666666">.</span>mean(), scores<span style="color: #666666">.</span>std()))
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
<p>
<!-- navigation buttons at the bottom of the page -->
<ul class="pagination">
<li><a href="._week38-bs008.html">&laquo;</a></li>
  <li><a href="._week38-bs000.html">1</a></li>
  <li><a href="._week38-bs001.html">2</a></li>
  <li><a href="._week38-bs002.html">3</a></li>
  <li><a href="._week38-bs003.html">4</a></li>
  <li><a href="._week38-bs004.html">5</a></li>
  <li><a href="._week38-bs005.html">6</a></li>
  <li><a href="._week38-bs006.html">7</a></li>
  <li><a href="._week38-bs007.html">8</a></li>
  <li><a href="._week38-bs008.html">9</a></li>
  <li class="active"><a href="._week38-bs009.html">10</a></li>
  <li><a href="._week38-bs010.html">11</a></li>
  <li><a href="._week38-bs011.html">12</a></li>
  <li><a href="._week38-bs012.html">13</a></li>
  <li><a href="._week38-bs013.html">14</a></li>
  <li><a href="._week38-bs014.html">15</a></li>
  <li><a href="._week38-bs015.html">16</a></li>
  <li><a href="._week38-bs016.html">17</a></li>
  <li><a href="._week38-bs017.html">18</a></li>
  <li><a href="._week38-bs018.html">19</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week38-bs040.html">41</a></li>
  <li><a href="._week38-bs010.html">&raquo;</a></li>
</ul>
<!-- ------------------- end of main content --------------- -->

</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>

<!-- Bootstrap footer
<footer>
<a href="http://..."><img width="250" align=right src="http://..."></a>
</footer>
-->


<center style="font-size:80%">
<!-- copyright only on the titlepage -->
</center>


</body>
</html>
    


<!--
Automatically generated HTML file from DocOnce source
(https://github.com/doconce/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Data Analysis and Machine Learning: Logistic Regression">

<title>Data Analysis and Machine Learning: Logistic Regression</title>

<!-- Bootstrap style: bootstrap -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->

<style type="text/css">

/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}

/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>


</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Plans for week 38', 2, None, 'plans-for-week-38'),
              ('Ridge and LASSO Regression, reminder',
               2,
               None,
               'ridge-and-lasso-regression-reminder'),
              ('Various steps in cross-validation',
               2,
               None,
               'various-steps-in-cross-validation'),
              ('How to set up the cross-validation for Ridge and/or Lasso',
               2,
               None,
               'how-to-set-up-the-cross-validation-for-ridge-and-or-lasso'),
              ('Cross-validation in brief',
               2,
               None,
               'cross-validation-in-brief'),
              ('Code Example for Cross-validation and $k$-fold '
               'Cross-validation',
               2,
               None,
               'code-example-for-cross-validation-and-k-fold-cross-validation'),
              ('To think about, first part',
               2,
               None,
               'to-think-about-first-part'),
              ('More complicated Example: The Ising model',
               2,
               None,
               'more-complicated-example-the-ising-model'),
              ('Reformulating the problem to suit regression',
               2,
               None,
               'reformulating-the-problem-to-suit-regression'),
              ('Linear regression', 2, None, 'linear-regression'),
              ('Singular Value decomposition',
               2,
               None,
               'singular-value-decomposition'),
              ('The one-dimensional Ising model',
               2,
               None,
               'the-one-dimensional-ising-model'),
              ('Ridge regression', 2, None, 'ridge-regression'),
              ('LASSO regression', 2, None, 'lasso-regression'),
              ('Performance as  function of the regularization parameter',
               2,
               None,
               'performance-as-function-of-the-regularization-parameter'),
              ('Finding the optimal value of $\\lambda$',
               2,
               None,
               'finding-the-optimal-value-of-lambda'),
              ('Logistic Regression', 2, None, 'logistic-regression'),
              ('Classification problems', 2, None, 'classification-problems'),
              ('Optimization and Deep learning',
               2,
               None,
               'optimization-and-deep-learning'),
              ('Basics', 2, None, 'basics'),
              ('Linear classifier', 2, None, 'linear-classifier'),
              ('Some selected properties', 2, None, 'some-selected-properties'),
              ('Simple example', 2, None, 'simple-example'),
              ('Plotting the mean value for each group',
               2,
               None,
               'plotting-the-mean-value-for-each-group'),
              ('The logistic function', 2, None, 'the-logistic-function'),
              ('Examples of likelihood functions used in logistic regression '
               'and nueral networks',
               2,
               None,
               'examples-of-likelihood-functions-used-in-logistic-regression-and-nueral-networks'),
              ('Two parameters', 2, None, 'two-parameters'),
              ('Maximum likelihood', 2, None, 'maximum-likelihood'),
              ('The cost function rewritten',
               2,
               None,
               'the-cost-function-rewritten'),
              ('Minimizing the cross entropy',
               2,
               None,
               'minimizing-the-cross-entropy'),
              ('A more compact expression',
               2,
               None,
               'a-more-compact-expression'),
              ('Extending to more predictors',
               2,
               None,
               'extending-to-more-predictors'),
              ('Including more classes', 2, None, 'including-more-classes'),
              ('More classes', 2, None, 'more-classes'),
              ('Wisconsin Cancer Data', 2, None, 'wisconsin-cancer-data'),
              ('Using the correlation matrix',
               2,
               None,
               'using-the-correlation-matrix'),
              ('Discussing the correlation data',
               2,
               None,
               'discussing-the-correlation-data'),
              ('Other measures in classification studies: Cancer Data  again',
               2,
               None,
               'other-measures-in-classification-studies-cancer-data-again')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="week38-bs.html">Data Analysis and Machine Learning: Logistic Regression</a>
  </div>

  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="._week38-bs001.html#plans-for-week-38" style="font-size: 80%;">Plans for week 38</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs002.html#ridge-and-lasso-regression-reminder" style="font-size: 80%;">Ridge and LASSO Regression, reminder</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs003.html#various-steps-in-cross-validation" style="font-size: 80%;">Various steps in cross-validation</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs004.html#how-to-set-up-the-cross-validation-for-ridge-and-or-lasso" style="font-size: 80%;">How to set up the cross-validation for Ridge and/or Lasso</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs005.html#cross-validation-in-brief" style="font-size: 80%;">Cross-validation in brief</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs006.html#code-example-for-cross-validation-and-k-fold-cross-validation" style="font-size: 80%;">Code Example for Cross-validation and \( k \)-fold Cross-validation</a></li>
     <!-- navigation toc: --> <li><a href="#to-think-about-first-part" style="font-size: 80%;">To think about, first part</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs008.html#more-complicated-example-the-ising-model" style="font-size: 80%;">More complicated Example: The Ising model</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs009.html#reformulating-the-problem-to-suit-regression" style="font-size: 80%;">Reformulating the problem to suit regression</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs010.html#linear-regression" style="font-size: 80%;">Linear regression</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs011.html#singular-value-decomposition" style="font-size: 80%;">Singular Value decomposition</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs012.html#the-one-dimensional-ising-model" style="font-size: 80%;">The one-dimensional Ising model</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs013.html#ridge-regression" style="font-size: 80%;">Ridge regression</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs014.html#lasso-regression" style="font-size: 80%;">LASSO regression</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs015.html#performance-as-function-of-the-regularization-parameter" style="font-size: 80%;">Performance as  function of the regularization parameter</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs016.html#finding-the-optimal-value-of-lambda" style="font-size: 80%;">Finding the optimal value of \( \lambda \)</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs017.html#logistic-regression" style="font-size: 80%;">Logistic Regression</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs018.html#classification-problems" style="font-size: 80%;">Classification problems</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs019.html#optimization-and-deep-learning" style="font-size: 80%;">Optimization and Deep learning</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs020.html#basics" style="font-size: 80%;">Basics</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs021.html#linear-classifier" style="font-size: 80%;">Linear classifier</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs022.html#some-selected-properties" style="font-size: 80%;">Some selected properties</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs023.html#simple-example" style="font-size: 80%;">Simple example</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs024.html#plotting-the-mean-value-for-each-group" style="font-size: 80%;">Plotting the mean value for each group</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs025.html#the-logistic-function" style="font-size: 80%;">The logistic function</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs026.html#examples-of-likelihood-functions-used-in-logistic-regression-and-nueral-networks" style="font-size: 80%;">Examples of likelihood functions used in logistic regression and nueral networks</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs027.html#two-parameters" style="font-size: 80%;">Two parameters</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs028.html#maximum-likelihood" style="font-size: 80%;">Maximum likelihood</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs029.html#the-cost-function-rewritten" style="font-size: 80%;">The cost function rewritten</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs030.html#minimizing-the-cross-entropy" style="font-size: 80%;">Minimizing the cross entropy</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs031.html#a-more-compact-expression" style="font-size: 80%;">A more compact expression</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs032.html#extending-to-more-predictors" style="font-size: 80%;">Extending to more predictors</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs033.html#including-more-classes" style="font-size: 80%;">Including more classes</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs034.html#more-classes" style="font-size: 80%;">More classes</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs035.html#wisconsin-cancer-data" style="font-size: 80%;">Wisconsin Cancer Data</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs036.html#using-the-correlation-matrix" style="font-size: 80%;">Using the correlation matrix</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs037.html#discussing-the-correlation-data" style="font-size: 80%;">Discussing the correlation data</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs038.html#other-measures-in-classification-studies-cancer-data-again" style="font-size: 80%;">Other measures in classification studies: Cancer Data  again</a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->

<div class="container">

<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->

<a name="part0007"></a>
<!-- !split -->

<h2 id="to-think-about-first-part" class="anchor">To think about, first part </h2>

<p>
When you are comparing your own code with for example <b>Scikit-Learn</b>'s
library, there are some things to keep in mind.  The examples
here demonstrate some of these aspects with potential pitfalls.

<p>
The discussion here focuses on the role of the intercept, how we can
set up the design matrix, what scaling we should use and other topics
which may confuse us.

<p>
Yes, it could be a bad idea to include the intercept column for the
exact reason you stated. If no transformation is applied to your&#160;data,
the intercept can be interpreted as the expected value of your target
variable when all your predictors are put to zero. Therefore, whenever
you cannot assume that the expected target variable is zero when all
your predictors are zero, it could be a bad idea to apply a model
which penalizes the intercept. Also, the analytical solution to the
ridge regression coefficients (when not shrinking $$\beta_0$$) is
derived&#160;under the assumption that both y and X are zero centered (mean
subtracted). What you are doing is correct, but you should&#160;also zero
center X (subtracting the mean of each column from the corresponding
column).&#160;

<p>
If your predictors&#160;are of different&#160;scales, I would advice you to
standardize&#160;X by&#160;subtracting the mean of each column from the
corresponding column and dividing the column with its standard
deviation.&#160;If you dont do this, you will give an "unfair"&#160;penalization
of the parameters since their magnitude&#160;depends on the scale of their
corresponding&#160;predictor.&#160;Suppose that you have an input&#160;variable
"height". Human height might be measured in inches or meters or
kilometers. If measured in kilometers, a&#160;standard linear regression
model with this predictor would probably give a much bigger
coefficient term, than if measured in millimeters. You may&#160;see how
this could become a problem when considering the loss function for
ridge regression.

<p>
Remember that when you do any transformation to your dataset before
training, the exact same transformation has to be applied to new data
before making a prediction. In your case, this means:

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #408080; font-style: italic">#Model training:</span>
y_train_mean <span style="color: #666666">=</span> np<span style="color: #666666">.</span>mean(y_train)
X_train_mean <span style="color: #666666">=</span> np<span style="color: #666666">.</span>mean(X_train,axis<span style="color: #666666">=0</span>)
X_train <span style="color: #666666">=</span> X_train <span style="color: #666666">-</span> X_train_mean
y_train <span style="color: #666666">=</span> y_train <span style="color: #666666">-</span> y_train_mean

trained_model <span style="color: #666666">=</span> some_model<span style="color: #666666">.</span>fit(X_train,y_train)

<span style="color: #408080; font-style: italic">#Model prediction:</span>
X_test <span style="color: #666666">=</span> X_test <span style="color: #666666">-</span> X_train_mean <span style="color: #408080; font-style: italic">#Use mean from training data</span>
y_pred <span style="color: #666666">=</span> trained_model(X_test)
y_pred <span style="color: #666666">=</span> y_pred <span style="color: #666666">+</span> y_train_mean
</pre></div>
<p>
Here is a mathematical explanation of the&#160;zero centering:

<p>
The cost/loss function  for Ridge regression is:

$$
C(\beta_0, \beta_1, ... , \beta_P) = \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{p=1}^P X_{ip}\beta_p)^2 + \lambda \sum_{p=1}^P \beta_p^2.
$$

<p>
Notice that the intercept is left out of the \( L_2 \) regularization term. The design matrix
\( X \) does in this case not contain any intercept column. We want

$$
\frac{\partial L}{\partial \beta_j} = 0,
$$

<p>
for all \( j \), so lets start with \( \beta_0 \). This means that we have

$$
\frac{\partial L}{\partial \beta_0} = -2\sum_{i=1}^{n} (y_i - \beta_0 - \sum_{p=1}^P X_{ip} \beta_p).
$$

<p>
We want to solve
$$
-2\sum_{i=1}^{n} (y_i - \beta_0 - \sum_{p=1}^P X_{ip} \beta_p) = 0,
$$

<p>
which gives
$$
\sum_{i=1}^{n} \beta_0 = \sum_{i=1}^{n}y_i - \sum_{i=1}^{n} \sum_{p=1}^P X_{ip} \beta_p,
$$

<p>
or
$ n\beta_0 = \sum_{i=1}^{n} y_i - \sum_{p=1}^P\beta_p \sum_{i=1}^{n} X_{ip}$.

<p>
If we assume that every column of \( X \) is centered, whic we can do by subtracting the mean,
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span>X <span style="color: #666666">=</span> X <span style="color: #666666">-</span> np<span style="color: #666666">.</span>mean(X,axis<span style="color: #666666">=0</span>)
</pre></div>
<p>
the sum $ \sum_{i=1}^{n} X_{ip} $

<p>
can be rewritten as
$$
\sum_{i=1}^{n} (X_{ip} - \frac{1}{n}\sum_{i=1}^{n} X_{ip}) = \sum_{i=1}^{n} X_{ip} - \sum_{i=1}^{n} \frac{1}{n} \sum_{i=1}^{n}X_{ip},
$$

resulting in
$$
\sum_{i=1}^{n} X_{ip} - n \frac{1}{n} \sum_{i=1}^{n}X_{ip} = 0.
$$

<p>
Finally we have
$$
n\beta_0 = \sum_{i=1}^{n} y_i - \sum_{p=1}^P\beta_p \sum_{i=1}^{n} X_{ip},
$$

or
$$
\beta_0 = \frac{1}{n}\sum_{i=1}^{n} y_i = y_{average}.
$$

<p>
Replacing \( y_i \) with \( y_i - \beta_0 = y_i - y_{average} \) in the loss function will give us (in vector-matrix disguise)
$$
C(\boldsymbol{\beta}) = (\boldsymbol{\tilde{y}} - \tilde{X}\boldsymbol{\beta})^T(\boldsymbol{\tilde{y}} - \tilde{X}\boldsymbol{\beta}) + \lambda \boldsymbol{\beta}^T\boldsymbol{\beta},
$$

<p>
which has the solution

<p>
\( \beta = (\tilde{X}^T\tilde{X} + \lambda I)^{-1}\tilde{X}^T\boldsymbol{\tilde{y}} \).
where \( \boldsymbol{\tilde{y}} = \boldsymbol{y} - y_{average} \)
and \( \tilde{X}_{ij} = X_{ij} - \frac{1}{n}\sum_{k=1}^{n-1}X_{kj} \).

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">pandas</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">pd</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> train_test_split
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn</span> <span style="color: #008000; font-weight: bold">import</span> linear_model

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">R2</span>(y_data, y_model):
    <span style="color: #008000; font-weight: bold">return</span> <span style="color: #666666">1</span> <span style="color: #666666">-</span> np<span style="color: #666666">.</span>sum((y_data <span style="color: #666666">-</span> y_model) <span style="color: #666666">**</span> <span style="color: #666666">2</span>) <span style="color: #666666">/</span> np<span style="color: #666666">.</span>sum((y_data <span style="color: #666666">-</span> np<span style="color: #666666">.</span>mean(y_data)) <span style="color: #666666">**</span> <span style="color: #666666">2</span>)
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">MSE</span>(y_data,y_model):
    n <span style="color: #666666">=</span> np<span style="color: #666666">.</span>size(y_model)
    <span style="color: #008000; font-weight: bold">return</span> np<span style="color: #666666">.</span>sum((y_data<span style="color: #666666">-</span>y_model)<span style="color: #666666">**2</span>)<span style="color: #666666">/</span>n


<span style="color: #408080; font-style: italic"># A seed just to ensure that the random numbers are the same for every run.</span>
<span style="color: #408080; font-style: italic"># Useful for eventual debugging.</span>
np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>seed(<span style="color: #666666">3155</span>)

n <span style="color: #666666">=</span> <span style="color: #666666">100</span>
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(n)
y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>x<span style="color: #666666">**2</span>) <span style="color: #666666">+</span> <span style="color: #666666">1.5</span> <span style="color: #666666">*</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(x<span style="color: #666666">-2</span>)<span style="color: #666666">**2</span>)

Maxpolydegree <span style="color: #666666">=</span> <span style="color: #666666">20</span>
X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros((n,Maxpolydegree))
X[:,<span style="color: #666666">0</span>] <span style="color: #666666">=</span> <span style="color: #666666">1.0</span>

<span style="color: #008000; font-weight: bold">for</span> polydegree <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #666666">1</span>, Maxpolydegree):
    <span style="color: #008000; font-weight: bold">for</span> degree <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(polydegree):
        X[:,degree] <span style="color: #666666">=</span> x<span style="color: #666666">**</span>degree


<span style="color: #408080; font-style: italic"># We split the data in test and training data</span>
X_train, X_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(X, y, test_size<span style="color: #666666">=0.2</span>)

<span style="color: #408080; font-style: italic"># matrix inversion to find beta</span>
OLSbeta <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linalg<span style="color: #666666">.</span>pinv(X_train<span style="color: #666666">.</span>T <span style="color: #666666">@</span> X_train) <span style="color: #666666">@</span> X_train<span style="color: #666666">.</span>T <span style="color: #666666">@</span> y_train
<span style="color: #008000">print</span>(OLSbeta)
<span style="color: #408080; font-style: italic"># and then make the prediction</span>
ytildeOLS <span style="color: #666666">=</span> X_train <span style="color: #666666">@</span> OLSbeta
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Training MSE for OLS&quot;</span>)
<span style="color: #008000">print</span>(MSE(y_train,ytildeOLS))
ypredictOLS <span style="color: #666666">=</span> X_test <span style="color: #666666">@</span> OLSbeta
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test MSE OLS&quot;</span>)
<span style="color: #008000">print</span>(MSE(y_test,ypredictOLS))

p <span style="color: #666666">=</span> <span style="color: #008000">len</span>(OLSbeta)
I <span style="color: #666666">=</span> np<span style="color: #666666">.</span>eye(p,p)
<span style="color: #408080; font-style: italic"># Decide which values of lambda to use</span>
nlambdas <span style="color: #666666">=</span> <span style="color: #666666">4</span>
MSEOwnRidgePredict <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(nlambdas)
MSEOwnRidgeTrain <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(nlambdas)
MSERidgePredict <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(nlambdas)
MSERidgeTrain <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(nlambdas)

lambdas <span style="color: #666666">=</span> np<span style="color: #666666">.</span>logspace(<span style="color: #666666">-4</span>, <span style="color: #666666">4</span>, nlambdas)
<span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(nlambdas):
    lmb <span style="color: #666666">=</span> lambdas[i]
    OwnRidgeBeta <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linalg<span style="color: #666666">.</span>pinv(X_train<span style="color: #666666">.</span>T <span style="color: #666666">@</span> X_train<span style="color: #666666">+</span>lmb<span style="color: #666666">*</span>I) <span style="color: #666666">@</span> X_train<span style="color: #666666">.</span>T <span style="color: #666666">@</span> y_train
    <span style="color: #408080; font-style: italic"># include lasso using Scikit-Learn</span>
    <span style="color: #408080; font-style: italic"># Note: we include the intercept</span>
    RegRidge <span style="color: #666666">=</span> linear_model<span style="color: #666666">.</span>Ridge(lmb,fit_intercept<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>)
    RegRidge<span style="color: #666666">.</span>fit(X_train,y_train)
    <span style="color: #408080; font-style: italic"># and then make the prediction</span>
    ytildeOwnRidge <span style="color: #666666">=</span> X_train <span style="color: #666666">@</span> OwnRidgeBeta
    ypredictOwnRidge <span style="color: #666666">=</span> X_test <span style="color: #666666">@</span> OwnRidgeBeta
    ytildeRidge <span style="color: #666666">=</span> RegRidge<span style="color: #666666">.</span>predict(X_train)
    ypredictRidge <span style="color: #666666">=</span> RegRidge<span style="color: #666666">.</span>predict(X_test)
    MSEOwnRidgePredict[i] <span style="color: #666666">=</span> MSE(y_test,ypredictOwnRidge)
    MSEOwnRidgeTrain[i] <span style="color: #666666">=</span> MSE(y_train,ytildeOwnRidge)
    MSERidgePredict[i] <span style="color: #666666">=</span> MSE(y_test,ypredictRidge)
    MSERidgeTrain[i] <span style="color: #666666">=</span> MSE(y_train,ytildeRidge)
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Beta values for own Ridge implementation&quot;</span>)
    <span style="color: #008000">print</span>(OwnRidgeBeta)
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Beta values for Scikit-Learn Ridge implementation&quot;</span>)
    <span style="color: #008000">print</span>(RegRidge<span style="color: #666666">.</span>coef_)
<span style="color: #408080; font-style: italic"># Now plot the results</span>
plt<span style="color: #666666">.</span>figure()
plt<span style="color: #666666">.</span>plot(np<span style="color: #666666">.</span>log10(lambdas), MSEOwnRidgeTrain, <span style="color: #BA2121">&#39;b&#39;</span>, label <span style="color: #666666">=</span> <span style="color: #BA2121">&#39;MSE Ridge train&#39;</span>)
plt<span style="color: #666666">.</span>plot(np<span style="color: #666666">.</span>log10(lambdas), MSEOwnRidgePredict, <span style="color: #BA2121">&#39;r&#39;</span>, label <span style="color: #666666">=</span> <span style="color: #BA2121">&#39;MSE Ridge Test&#39;</span>)
plt<span style="color: #666666">.</span>plot(np<span style="color: #666666">.</span>log10(lambdas), MSERidgeTrain, <span style="color: #BA2121">&#39;y&#39;</span>, label <span style="color: #666666">=</span> <span style="color: #BA2121">&#39;MSE Ridge train&#39;</span>)
plt<span style="color: #666666">.</span>plot(np<span style="color: #666666">.</span>log10(lambdas), MSERidgePredict, <span style="color: #BA2121">&#39;g&#39;</span>, label <span style="color: #666666">=</span> <span style="color: #BA2121">&#39;MSE Ridge Test&#39;</span>)

plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&#39;log10(lambda)&#39;</span>)
plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&#39;MSE&#39;</span>)
plt<span style="color: #666666">.</span>legend()
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">pandas</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">pd</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> train_test_split
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn</span> <span style="color: #008000; font-weight: bold">import</span> linear_model
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.preprocessing</span> <span style="color: #008000; font-weight: bold">import</span> StandardScaler

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">R2</span>(y_data, y_model):
    <span style="color: #008000; font-weight: bold">return</span> <span style="color: #666666">1</span> <span style="color: #666666">-</span> np<span style="color: #666666">.</span>sum((y_data <span style="color: #666666">-</span> y_model) <span style="color: #666666">**</span> <span style="color: #666666">2</span>) <span style="color: #666666">/</span> np<span style="color: #666666">.</span>sum((y_data <span style="color: #666666">-</span> np<span style="color: #666666">.</span>mean(y_data)) <span style="color: #666666">**</span> <span style="color: #666666">2</span>)
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">MSE</span>(y_data,y_model):
    n <span style="color: #666666">=</span> np<span style="color: #666666">.</span>size(y_model)
    <span style="color: #008000; font-weight: bold">return</span> np<span style="color: #666666">.</span>sum((y_data<span style="color: #666666">-</span>y_model)<span style="color: #666666">**2</span>)<span style="color: #666666">/</span>n


<span style="color: #408080; font-style: italic"># A seed just to ensure that the random numbers are the same for every run.</span>
<span style="color: #408080; font-style: italic"># Useful for eventual debugging.</span>
np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>seed(<span style="color: #666666">315</span>)

n <span style="color: #666666">=</span> <span style="color: #666666">100</span>
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(n)
y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>x<span style="color: #666666">**2</span>) <span style="color: #666666">+</span> <span style="color: #666666">1.5</span> <span style="color: #666666">*</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(x<span style="color: #666666">-2</span>)<span style="color: #666666">**2</span>)

Maxpolydegree <span style="color: #666666">=</span> <span style="color: #666666">5</span>
X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros((n,Maxpolydegree<span style="color: #666666">-1</span>))

<span style="color: #008000; font-weight: bold">for</span> degree <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #666666">1</span>,Maxpolydegree): <span style="color: #408080; font-style: italic">#No intercept column</span>
    X[:,degree<span style="color: #666666">-1</span>] <span style="color: #666666">=</span> x<span style="color: #666666">**</span>(degree)




<span style="color: #408080; font-style: italic"># We split the data in test and training data</span>
X_train, X_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(X, y, test_size<span style="color: #666666">=0.2</span>)





<span style="color: #408080; font-style: italic">#For our own implementation, we will need to deal with the intercept by centering the design matrix and the target variable</span>
X_train_mean <span style="color: #666666">=</span> np<span style="color: #666666">.</span>mean(X_train,axis<span style="color: #666666">=0</span>)
X_train_scaled <span style="color: #666666">=</span> X_train <span style="color: #666666">-</span> X_train_mean <span style="color: #408080; font-style: italic">#Center by removing mean from each feature</span>
X_test_scaled <span style="color: #666666">=</span> X_test <span style="color: #666666">-</span> X_train_mean

y_scaler <span style="color: #666666">=</span> np<span style="color: #666666">.</span>mean(y_train)           <span style="color: #408080; font-style: italic">#The model intercept (called y_scaler) is given by the mean of target variable (IF X is centered)</span>
y_train_scaled <span style="color: #666666">=</span> y_train <span style="color: #666666">-</span> y_scaler   <span style="color: #408080; font-style: italic">#Remove the intercept from the training data.</span>


p <span style="color: #666666">=</span> Maxpolydegree<span style="color: #666666">-1</span>
I <span style="color: #666666">=</span> np<span style="color: #666666">.</span>eye(p,p)
<span style="color: #408080; font-style: italic"># Decide which values of lambda to use</span>
nlambdas <span style="color: #666666">=</span> <span style="color: #666666">4</span>
MSEOwnRidgePredict <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(nlambdas)
MSERidgePredict <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(nlambdas)

lambdas <span style="color: #666666">=</span> np<span style="color: #666666">.</span>logspace(<span style="color: #666666">-4</span>, <span style="color: #666666">1</span>, nlambdas)
<span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(nlambdas):
    lmb <span style="color: #666666">=</span> lambdas[i]
    OwnRidgeBeta <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linalg<span style="color: #666666">.</span>pinv(X_train_scaled<span style="color: #666666">.</span>T <span style="color: #666666">@</span> X_train_scaled<span style="color: #666666">+</span>lmb<span style="color: #666666">*</span>I) <span style="color: #666666">@</span> X_train_scaled<span style="color: #666666">.</span>T <span style="color: #666666">@</span> (y_train_scaled)
    intercept_ <span style="color: #666666">=</span> y_scaler <span style="color: #666666">-</span> X_train_mean<span style="color: #AA22FF">@OwnRidgeBeta</span> <span style="color: #408080; font-style: italic">#The intercept can be shifted so the model can predict on uncentered data</span>
    
    ypredictOwnRidge <span style="color: #666666">=</span> X_test <span style="color: #666666">@</span> OwnRidgeBeta <span style="color: #666666">+</span> intercept_ <span style="color: #408080; font-style: italic">#Add intercept to prediction</span>
    <span style="color: #408080; font-style: italic">#EQUIVALENT PREDICTION:</span>
    ypredictOwnRidge <span style="color: #666666">=</span> X_test_scaled <span style="color: #666666">@</span> OwnRidgeBeta <span style="color: #666666">+</span> y_scaler <span style="color: #408080; font-style: italic">#Add intercept to prediction</span>
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Values for own Ridge prediction&quot;</span>)
    <span style="color: #008000">print</span>(ypredictOwnRidge)

    

    RegRidge <span style="color: #666666">=</span> linear_model<span style="color: #666666">.</span>Ridge(lmb)
    RegRidge<span style="color: #666666">.</span>fit(X_train,y_train)
    ypredictRidge <span style="color: #666666">=</span> RegRidge<span style="color: #666666">.</span>predict(X_test)
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Values for SL Ridge prediction&quot;</span>)
    <span style="color: #008000">print</span>(ypredictRidge)


    MSEOwnRidgePredict[i] <span style="color: #666666">=</span> MSE(y_test,ypredictOwnRidge)
    MSERidgePredict[i] <span style="color: #666666">=</span> MSE(y_test,ypredictRidge)

    <span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Beta values for own Ridge implementation&quot;</span>)
    <span style="color: #008000">print</span>(OwnRidgeBeta) <span style="color: #408080; font-style: italic">#Intercept is given by mean of target variable</span>
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Beta values for Scikit-Learn Ridge implementation&quot;</span>)
    <span style="color: #008000">print</span>(RegRidge<span style="color: #666666">.</span>coef_)
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Intercept from own implementation:&#39;</span>)
    <span style="color: #008000">print</span>(intercept_)
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Intercept from Scikit-Learn Ridge implementation&#39;</span>)
    <span style="color: #008000">print</span>(RegRidge<span style="color: #666666">.</span>intercept_)



<span style="color: #408080; font-style: italic"># Now plot the results</span>

plt<span style="color: #666666">.</span>figure()
plt<span style="color: #666666">.</span>plot(np<span style="color: #666666">.</span>log10(lambdas), MSEOwnRidgePredict, <span style="color: #BA2121">&#39;b--&#39;</span>, label <span style="color: #666666">=</span> <span style="color: #BA2121">&#39;MSE own Ridge Test&#39;</span>)
plt<span style="color: #666666">.</span>plot(np<span style="color: #666666">.</span>log10(lambdas), MSERidgePredict, <span style="color: #BA2121">&#39;g--&#39;</span>, label <span style="color: #666666">=</span> <span style="color: #BA2121">&#39;MSE SL Ridge Test&#39;</span>)

plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&#39;log10(lambda)&#39;</span>)
plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&#39;MSE&#39;</span>)
plt<span style="color: #666666">.</span>legend()
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>

<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> LinearRegression


np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>seed(<span style="color: #666666">2021</span>)


<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">fit_beta</span>(X, y):
    <span style="color: #008000; font-weight: bold">return</span> np<span style="color: #666666">.</span>linalg<span style="color: #666666">.</span>pinv(X<span style="color: #666666">.</span>T <span style="color: #666666">@</span> X) <span style="color: #666666">@</span> X<span style="color: #666666">.</span>T <span style="color: #666666">@</span> y


true_beta <span style="color: #666666">=</span> [<span style="color: #666666">2</span>, <span style="color: #666666">0.5</span>, <span style="color: #666666">3.7</span>]

x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(<span style="color: #666666">0</span>, <span style="color: #666666">1</span>, <span style="color: #666666">11</span>)
y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>sum(
    np<span style="color: #666666">.</span>asarray([x <span style="color: #666666">**</span> p <span style="color: #666666">*</span> b <span style="color: #008000; font-weight: bold">for</span> p, b <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">enumerate</span>(true_beta)]), axis<span style="color: #666666">=0</span>
) <span style="color: #666666">+</span> <span style="color: #666666">0.1</span> <span style="color: #666666">*</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>normal(size<span style="color: #666666">=</span><span style="color: #008000">len</span>(x))

degree <span style="color: #666666">=</span> <span style="color: #666666">3</span>
X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros((<span style="color: #008000">len</span>(x), degree))

<span style="color: #408080; font-style: italic"># Include the intercept in the design matrix</span>
<span style="color: #008000; font-weight: bold">for</span> p <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(degree):
    X[:, p] <span style="color: #666666">=</span> x <span style="color: #666666">**</span> p

beta <span style="color: #666666">=</span> fit_beta(X, y)

<span style="color: #408080; font-style: italic"># Intercept is included in the design matrix</span>
clf <span style="color: #666666">=</span> LinearRegression(fit_intercept<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>)<span style="color: #666666">.</span>fit(X, y)

<span style="color: #008000">print</span>(<span style="color: #BA2121">f&quot;True beta: </span><span style="color: #BB6688; font-weight: bold">{</span>true_beta<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&quot;</span>)
<span style="color: #008000">print</span>(<span style="color: #BA2121">f&quot;Fitted beta: </span><span style="color: #BB6688; font-weight: bold">{</span>beta<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&quot;</span>)
<span style="color: #008000">print</span>(<span style="color: #BA2121">f&quot;Sklearn fitted beta: </span><span style="color: #BB6688; font-weight: bold">{</span>clf<span style="color: #666666">.</span>coef_<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&quot;</span>)


plt<span style="color: #666666">.</span>figure()
plt<span style="color: #666666">.</span>scatter(x, y, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;Data&quot;</span>)
plt<span style="color: #666666">.</span>plot(x, X <span style="color: #666666">@</span> beta, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;Fit&quot;</span>)
plt<span style="color: #666666">.</span>plot(x, clf<span style="color: #666666">.</span>predict(X), label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;Sklearn (fit_intercept=False)&quot;</span>)


<span style="color: #408080; font-style: italic"># Do not include the intercept in the design matrix</span>
X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros((<span style="color: #008000">len</span>(x), degree <span style="color: #666666">-</span> <span style="color: #666666">1</span>))

<span style="color: #008000; font-weight: bold">for</span> p <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(degree <span style="color: #666666">-</span> <span style="color: #666666">1</span>):
    X[:, p] <span style="color: #666666">=</span> x <span style="color: #666666">**</span> (p <span style="color: #666666">+</span> <span style="color: #666666">1</span>)

<span style="color: #408080; font-style: italic"># Intercept is not included in the design matrix</span>
clf <span style="color: #666666">=</span> LinearRegression(fit_intercept<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)<span style="color: #666666">.</span>fit(X, y)

<span style="color: #408080; font-style: italic"># Use centered values for X and y when computing coefficients</span>
y_offset <span style="color: #666666">=</span> np<span style="color: #666666">.</span>average(y, axis<span style="color: #666666">=0</span>)
X_offset <span style="color: #666666">=</span> np<span style="color: #666666">.</span>average(X, axis<span style="color: #666666">=0</span>)

beta <span style="color: #666666">=</span> fit_beta(X <span style="color: #666666">-</span> X_offset, y <span style="color: #666666">-</span> y_offset)
intercept <span style="color: #666666">=</span> np<span style="color: #666666">.</span>mean(y_offset <span style="color: #666666">-</span> X_offset <span style="color: #666666">@</span> beta)

<span style="color: #008000">print</span>(<span style="color: #BA2121">f&quot;Manual intercept: </span><span style="color: #BB6688; font-weight: bold">{</span>intercept<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&quot;</span>)
<span style="color: #008000">print</span>(<span style="color: #BA2121">f&quot;Fitted beta (sans intercept): </span><span style="color: #BB6688; font-weight: bold">{</span>beta<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&quot;</span>)
<span style="color: #008000">print</span>(<span style="color: #BA2121">f&quot;Sklearn intercept: </span><span style="color: #BB6688; font-weight: bold">{</span>clf<span style="color: #666666">.</span>intercept_<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&quot;</span>)
<span style="color: #008000">print</span>(<span style="color: #BA2121">f&quot;Sklearn fitted beta (sans intercept): </span><span style="color: #BB6688; font-weight: bold">{</span>clf<span style="color: #666666">.</span>coef_<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&quot;</span>)

plt<span style="color: #666666">.</span>plot(x, X <span style="color: #666666">@</span> beta <span style="color: #666666">+</span> intercept, <span style="color: #BA2121">&quot;--&quot;</span>, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;Fit (manual intercept)&quot;</span>)
plt<span style="color: #666666">.</span>plot(x, clf<span style="color: #666666">.</span>predict(X), <span style="color: #BA2121">&quot;--&quot;</span>, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;Sklearn (fit_intercept=True)&quot;</span>)
plt<span style="color: #666666">.</span>grid()
plt<span style="color: #666666">.</span>legend()

plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
<p>
<!-- navigation buttons at the bottom of the page -->
<ul class="pagination">
<li><a href="._week38-bs006.html">&laquo;</a></li>
  <li><a href="._week38-bs000.html">1</a></li>
  <li><a href="._week38-bs001.html">2</a></li>
  <li><a href="._week38-bs002.html">3</a></li>
  <li><a href="._week38-bs003.html">4</a></li>
  <li><a href="._week38-bs004.html">5</a></li>
  <li><a href="._week38-bs005.html">6</a></li>
  <li><a href="._week38-bs006.html">7</a></li>
  <li class="active"><a href="._week38-bs007.html">8</a></li>
  <li><a href="._week38-bs008.html">9</a></li>
  <li><a href="._week38-bs009.html">10</a></li>
  <li><a href="._week38-bs010.html">11</a></li>
  <li><a href="._week38-bs011.html">12</a></li>
  <li><a href="._week38-bs012.html">13</a></li>
  <li><a href="._week38-bs013.html">14</a></li>
  <li><a href="._week38-bs014.html">15</a></li>
  <li><a href="._week38-bs015.html">16</a></li>
  <li><a href="._week38-bs016.html">17</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week38-bs038.html">39</a></li>
  <li><a href="._week38-bs008.html">&raquo;</a></li>
</ul>
<!-- ------------------- end of main content --------------- -->

</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>

<!-- Bootstrap footer
<footer>
<a href="https://..."><img width="250" align=right src="https://..."></a>
</footer>
-->


<center style="font-size:80%">
<!-- copyright only on the titlepage -->
</center>


</body>
</html>
    


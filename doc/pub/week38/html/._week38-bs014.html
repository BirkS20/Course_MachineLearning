<!--
Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Data Analysis and Machine Learning: Logistic Regression">

<title>Data Analysis and Machine Learning: Logistic Regression</title>

<!-- Bootstrap style: bootstrap -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->

<style type="text/css">

/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}

/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>


</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Plans for week 38', 2, None, '___sec0'),
              ('Thursday September 23', 2, None, '___sec1'),
              ('Ridge and LASSO Regression, reminder', 2, None, '___sec2'),
              ('Various steps in cross-validation', 2, None, '___sec3'),
              ('How to set up the cross-validation for Ridge and/or Lasso',
               2,
               None,
               '___sec4'),
              ('Cross-validation in brief', 2, None, '___sec5'),
              ('Code Example for Cross-validation and $k$-fold '
               'Cross-validation',
               2,
               None,
               '___sec6'),
              ('To think about, first part', 2, None, '___sec7'),
              ('More thinking', 2, None, '___sec8'),
              ('Still thinking', 2, None, '___sec9'),
              ('Linear Regression code, Intercept handling first',
               2,
               None,
               '___sec10'),
              ('What does centering (subtracting the mean values) mean '
               'mathematically?',
               2,
               None,
<<<<<<< HEAD
               'what-does-centering-subtracting-the-mean-values-mean-mathematically'),
              ('Further Manipulations', 2, None, 'further-manipulations'),
              ('Wrapping it up', 2, None, 'wrapping-it-up'),
              ('Code Examples', 2, None, 'code-examples'),
              ('Taking out the mean', 2, None, 'taking-out-the-mean'),
=======
               '___sec11'),
              ('Code Examples', 2, None, '___sec12'),
              ('Taking out the mean', 2, None, '___sec13'),
>>>>>>> 7c4b51b2e2a6a4ece2432ee81fe3530f2502d8e3
              ('More complicated Example: The Ising model',
               2,
               None,
               '___sec14'),
              ('Reformulating the problem to suit regression',
               2,
               None,
               '___sec15'),
              ('Linear regression', 2, None, '___sec16'),
              ('Singular Value decomposition', 2, None, '___sec17'),
              ('The one-dimensional Ising model', 2, None, '___sec18'),
              ('Ridge regression', 2, None, '___sec19'),
              ('LASSO regression', 2, None, '___sec20'),
              ('Performance as  function of the regularization parameter',
               2,
               None,
               '___sec21'),
              ('Finding the optimal value of $\\lambda$', 2, None, '___sec22'),
              ('Logistic Regression', 2, None, '___sec23'),
              ('Classification problems', 2, None, '___sec24'),
              ('Optimization and Deep learning', 2, None, '___sec25'),
              ('Basics', 2, None, '___sec26'),
              ('Linear classifier', 2, None, '___sec27'),
              ('Some selected properties', 2, None, '___sec28'),
              ('Simple example', 2, None, '___sec29'),
              ('Plotting the mean value for each group', 2, None, '___sec30'),
              ('The logistic function', 2, None, '___sec31'),
              ('Examples of likelihood functions used in logistic regression '
               'and nueral networks',
               2,
               None,
               '___sec32'),
              ('Two parameters', 2, None, '___sec33'),
              ('Maximum likelihood', 2, None, '___sec34'),
              ('The cost function rewritten', 2, None, '___sec35'),
              ('Minimizing the cross entropy', 2, None, '___sec36'),
              ('A more compact expression', 2, None, '___sec37'),
              ('Extending to more predictors', 2, None, '___sec38'),
              ('Including more classes', 2, None, '___sec39'),
              ('More classes', 2, None, '___sec40'),
              ('Friday September 24', 2, None, '___sec41'),
              ('Wisconsin Cancer Data', 2, None, '___sec42'),
              ('Using the correlation matrix', 2, None, '___sec43'),
              ('Discussing the correlation data', 2, None, '___sec44'),
              ('Other measures in classification studies: Cancer Data  again',
               2,
               None,
               '___sec45'),
              ('Optimization, the central part of any Machine Learning '
               'algortithm',
               2,
               None,
               '___sec46'),
              ('Revisiting our Logistic Regression case', 2, None, '___sec47'),
              ('The equations to solve', 2, None, '___sec48'),
              ("Solving using Newton-Raphson's method", 2, None, '___sec49'),
              ("Brief reminder on Newton-Raphson's method",
               2,
               None,
               '___sec50'),
              ('The equations', 2, None, '___sec51'),
              ('Simple geometric interpretation', 2, None, '___sec52'),
              ('Extending to more than one variable', 2, None, '___sec53'),
              ('Steepest descent', 2, None, '___sec54'),
              ('More on Steepest descent', 2, None, '___sec55'),
              ('The ideal', 2, None, '___sec56'),
              ('The sensitiveness of the gradient descent',
               2,
               None,
               '___sec57'),
              ('Convex functions', 2, None, '___sec58'),
              ('Convex function', 2, None, '___sec59'),
              ('Conditions on convex functions', 2, None, '___sec60'),
              ('More on convex functions', 2, None, '___sec61'),
              ('Some simple problems', 2, None, '___sec62'),
              ('Friday September 25', 2, None, '___sec63'),
              ('Standard steepest descent', 2, None, '___sec64'),
              ('Gradient method', 2, None, '___sec65'),
              ('Steepest descent  method', 2, None, '___sec66'),
              ('Steepest descent  method', 2, None, '___sec67'),
              ('Final expressions', 2, None, '___sec68'),
              ('Steepest descent example', 2, None, '___sec69'),
              ('Conjugate gradient method', 2, None, '___sec70'),
              ('Conjugate gradient method', 2, None, '___sec71'),
              ('Conjugate gradient method', 2, None, '___sec72'),
              ('Conjugate gradient method', 2, None, '___sec73'),
              ('Conjugate gradient method and iterations', 2, None, '___sec74'),
              ('Conjugate gradient method', 2, None, '___sec75'),
              ('Conjugate gradient method', 2, None, '___sec76'),
              ('Conjugate gradient method', 2, None, '___sec77'),
              ('Revisiting some of our first Linear Regression Encounters',
               2,
               None,
               '___sec78'),
              ('Gradient descent example', 2, None, '___sec79'),
              ('The derivative of the cost/loss function', 2, None, '___sec80'),
              ('The Hessian matrix', 2, None, '___sec81'),
              ('Simple program', 2, None, '___sec82'),
              ('Gradient Descent Example', 2, None, '___sec83'),
              ('And a corresponding example using _scikit-learn_',
               2,
               None,
               '___sec84'),
              ('Gradient descent and Ridge', 2, None, '___sec85'),
              ('Program example for gradient descent with Ridge Regression',
               2,
               None,
               '___sec86'),
              ('Using gradient descent methods, limitations',
               2,
               None,
               '___sec87')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="week38-bs.html">Data Analysis and Machine Learning: Logistic Regression</a>
  </div>

  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
<<<<<<< HEAD
     <!-- navigation toc: --> <li><a href="._week38-bs001.html#plans-for-week-38" style="font-size: 80%;">Plans for week 38</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs002.html#thursday-september-23" style="font-size: 80%;">Thursday September 23</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs003.html#ridge-and-lasso-regression-reminder" style="font-size: 80%;">Ridge and LASSO Regression, reminder</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs004.html#various-steps-in-cross-validation" style="font-size: 80%;">Various steps in cross-validation</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs005.html#how-to-set-up-the-cross-validation-for-ridge-and-or-lasso" style="font-size: 80%;">How to set up the cross-validation for Ridge and/or Lasso</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs006.html#cross-validation-in-brief" style="font-size: 80%;">Cross-validation in brief</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs007.html#code-example-for-cross-validation-and-k-fold-cross-validation" style="font-size: 80%;">Code Example for Cross-validation and \( k \)-fold Cross-validation</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs008.html#to-think-about-first-part" style="font-size: 80%;">To think about, first part</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs009.html#more-thinking" style="font-size: 80%;">More thinking</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs010.html#still-thinking" style="font-size: 80%;">Still thinking</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs011.html#linear-regression-code-intercept-handling-first" style="font-size: 80%;">Linear Regression code, Intercept handling first</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs012.html#what-does-centering-subtracting-the-mean-values-mean-mathematically" style="font-size: 80%;">What does centering (subtracting the mean values) mean mathematically?</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs013.html#further-manipulations" style="font-size: 80%;">Further Manipulations</a></li>
     <!-- navigation toc: --> <li><a href="#wrapping-it-up" style="font-size: 80%;">Wrapping it up</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs015.html#code-examples" style="font-size: 80%;">Code Examples</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs016.html#taking-out-the-mean" style="font-size: 80%;">Taking out the mean</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs017.html#more-complicated-example-the-ising-model" style="font-size: 80%;">More complicated Example: The Ising model</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs018.html#reformulating-the-problem-to-suit-regression" style="font-size: 80%;">Reformulating the problem to suit regression</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs019.html#linear-regression" style="font-size: 80%;">Linear regression</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs020.html#singular-value-decomposition" style="font-size: 80%;">Singular Value decomposition</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs021.html#the-one-dimensional-ising-model" style="font-size: 80%;">The one-dimensional Ising model</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs022.html#ridge-regression" style="font-size: 80%;">Ridge regression</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs023.html#lasso-regression" style="font-size: 80%;">LASSO regression</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs024.html#performance-as-function-of-the-regularization-parameter" style="font-size: 80%;">Performance as  function of the regularization parameter</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs025.html#finding-the-optimal-value-of-lambda" style="font-size: 80%;">Finding the optimal value of \( \lambda \)</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs026.html#logistic-regression" style="font-size: 80%;">Logistic Regression</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs027.html#classification-problems" style="font-size: 80%;">Classification problems</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs028.html#optimization-and-deep-learning" style="font-size: 80%;">Optimization and Deep learning</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs029.html#basics" style="font-size: 80%;">Basics</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs030.html#linear-classifier" style="font-size: 80%;">Linear classifier</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs031.html#some-selected-properties" style="font-size: 80%;">Some selected properties</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs032.html#simple-example" style="font-size: 80%;">Simple example</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs033.html#plotting-the-mean-value-for-each-group" style="font-size: 80%;">Plotting the mean value for each group</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs034.html#the-logistic-function" style="font-size: 80%;">The logistic function</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs035.html#examples-of-likelihood-functions-used-in-logistic-regression-and-nueral-networks" style="font-size: 80%;">Examples of likelihood functions used in logistic regression and nueral networks</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs036.html#two-parameters" style="font-size: 80%;">Two parameters</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs037.html#maximum-likelihood" style="font-size: 80%;">Maximum likelihood</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs038.html#the-cost-function-rewritten" style="font-size: 80%;">The cost function rewritten</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs039.html#minimizing-the-cross-entropy" style="font-size: 80%;">Minimizing the cross entropy</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs040.html#a-more-compact-expression" style="font-size: 80%;">A more compact expression</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs041.html#extending-to-more-predictors" style="font-size: 80%;">Extending to more predictors</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs042.html#including-more-classes" style="font-size: 80%;">Including more classes</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs043.html#more-classes" style="font-size: 80%;">More classes</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs044.html#friday-september-24" style="font-size: 80%;">Friday September 24</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs045.html#wisconsin-cancer-data" style="font-size: 80%;">Wisconsin Cancer Data</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs046.html#using-the-correlation-matrix" style="font-size: 80%;">Using the correlation matrix</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs047.html#discussing-the-correlation-data" style="font-size: 80%;">Discussing the correlation data</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs048.html#other-measures-in-classification-studies-cancer-data-again" style="font-size: 80%;">Other measures in classification studies: Cancer Data  again</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs049.html#optimization-the-central-part-of-any-machine-learning-algortithm" style="font-size: 80%;">Optimization, the central part of any Machine Learning algortithm</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs050.html#revisiting-our-logistic-regression-case" style="font-size: 80%;">Revisiting our Logistic Regression case</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs051.html#the-equations-to-solve" style="font-size: 80%;">The equations to solve</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs052.html#solving-using-newton-raphson-s-method" style="font-size: 80%;">Solving using Newton-Raphson's method</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs053.html#brief-reminder-on-newton-raphson-s-method" style="font-size: 80%;">Brief reminder on Newton-Raphson's method</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs054.html#the-equations" style="font-size: 80%;">The equations</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs055.html#simple-geometric-interpretation" style="font-size: 80%;">Simple geometric interpretation</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs056.html#extending-to-more-than-one-variable" style="font-size: 80%;">Extending to more than one variable</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs057.html#steepest-descent" style="font-size: 80%;">Steepest descent</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs058.html#more-on-steepest-descent" style="font-size: 80%;">More on Steepest descent</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs059.html#the-ideal" style="font-size: 80%;">The ideal</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs060.html#the-sensitiveness-of-the-gradient-descent" style="font-size: 80%;">The sensitiveness of the gradient descent</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs061.html#convex-functions" style="font-size: 80%;">Convex functions</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs062.html#convex-function" style="font-size: 80%;">Convex function</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs063.html#conditions-on-convex-functions" style="font-size: 80%;">Conditions on convex functions</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs064.html#more-on-convex-functions" style="font-size: 80%;">More on convex functions</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs065.html#some-simple-problems" style="font-size: 80%;">Some simple problems</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs066.html#friday-september-25" style="font-size: 80%;">Friday September 25</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs067.html#standard-steepest-descent" style="font-size: 80%;">Standard steepest descent</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs068.html#gradient-method" style="font-size: 80%;">Gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs070.html#steepest-descent-method" style="font-size: 80%;">Steepest descent  method</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs070.html#steepest-descent-method" style="font-size: 80%;">Steepest descent  method</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs071.html#final-expressions" style="font-size: 80%;">Final expressions</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs072.html#steepest-descent-example" style="font-size: 80%;">Steepest descent example</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs080.html#conjugate-gradient-method" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs080.html#conjugate-gradient-method" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs080.html#conjugate-gradient-method" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs080.html#conjugate-gradient-method" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs077.html#conjugate-gradient-method-and-iterations" style="font-size: 80%;">Conjugate gradient method and iterations</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs080.html#conjugate-gradient-method" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs080.html#conjugate-gradient-method" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs080.html#conjugate-gradient-method" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs081.html#revisiting-some-of-our-first-linear-regression-encounters" style="font-size: 80%;">Revisiting some of our first Linear Regression Encounters</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs086.html#gradient-descent-example" style="font-size: 80%;">Gradient descent example</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs083.html#the-derivative-of-the-cost-loss-function" style="font-size: 80%;">The derivative of the cost/loss function</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs084.html#the-hessian-matrix" style="font-size: 80%;">The Hessian matrix</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs085.html#simple-program" style="font-size: 80%;">Simple program</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs086.html#gradient-descent-example" style="font-size: 80%;">Gradient Descent Example</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs087.html#and-a-corresponding-example-using-_scikit-learn_" style="font-size: 80%;">And a corresponding example using <b>scikit-learn</b></a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs088.html#gradient-descent-and-ridge" style="font-size: 80%;">Gradient descent and Ridge</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs089.html#program-example-for-gradient-descent-with-ridge-regression" style="font-size: 80%;">Program example for gradient descent with Ridge Regression</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs090.html#using-gradient-descent-methods-limitations" style="font-size: 80%;">Using gradient descent methods, limitations</a></li>
=======
     <!-- navigation toc: --> <li><a href="._week38-bs001.html#___sec0" style="font-size: 80%;">Plans for week 38</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs002.html#___sec1" style="font-size: 80%;">Thursday September 23</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs003.html#___sec2" style="font-size: 80%;">Ridge and LASSO Regression, reminder</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs004.html#___sec3" style="font-size: 80%;">Various steps in cross-validation</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs005.html#___sec4" style="font-size: 80%;">How to set up the cross-validation for Ridge and/or Lasso</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs006.html#___sec5" style="font-size: 80%;">Cross-validation in brief</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs007.html#___sec6" style="font-size: 80%;">Code Example for Cross-validation and \( k \)-fold Cross-validation</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs008.html#___sec7" style="font-size: 80%;">To think about, first part</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs009.html#___sec8" style="font-size: 80%;">More thinking</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs010.html#___sec9" style="font-size: 80%;">Still thinking</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs011.html#___sec10" style="font-size: 80%;">Linear Regression code, Intercept handling first</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs012.html#___sec11" style="font-size: 80%;">What does centering (subtracting the mean values) mean mathematically?</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs013.html#___sec12" style="font-size: 80%;">Code Examples</a></li>
     <!-- navigation toc: --> <li><a href="#___sec13" style="font-size: 80%;">Taking out the mean</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs015.html#___sec14" style="font-size: 80%;">More complicated Example: The Ising model</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs016.html#___sec15" style="font-size: 80%;">Reformulating the problem to suit regression</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs017.html#___sec16" style="font-size: 80%;">Linear regression</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs018.html#___sec17" style="font-size: 80%;">Singular Value decomposition</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs019.html#___sec18" style="font-size: 80%;">The one-dimensional Ising model</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs020.html#___sec19" style="font-size: 80%;">Ridge regression</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs021.html#___sec20" style="font-size: 80%;">LASSO regression</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs022.html#___sec21" style="font-size: 80%;">Performance as  function of the regularization parameter</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs023.html#___sec22" style="font-size: 80%;">Finding the optimal value of \( \lambda \)</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs024.html#___sec23" style="font-size: 80%;">Logistic Regression</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs025.html#___sec24" style="font-size: 80%;">Classification problems</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs026.html#___sec25" style="font-size: 80%;">Optimization and Deep learning</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs027.html#___sec26" style="font-size: 80%;">Basics</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs028.html#___sec27" style="font-size: 80%;">Linear classifier</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs029.html#___sec28" style="font-size: 80%;">Some selected properties</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs030.html#___sec29" style="font-size: 80%;">Simple example</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs031.html#___sec30" style="font-size: 80%;">Plotting the mean value for each group</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs032.html#___sec31" style="font-size: 80%;">The logistic function</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs033.html#___sec32" style="font-size: 80%;">Examples of likelihood functions used in logistic regression and nueral networks</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs034.html#___sec33" style="font-size: 80%;">Two parameters</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs035.html#___sec34" style="font-size: 80%;">Maximum likelihood</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs036.html#___sec35" style="font-size: 80%;">The cost function rewritten</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs037.html#___sec36" style="font-size: 80%;">Minimizing the cross entropy</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs038.html#___sec37" style="font-size: 80%;">A more compact expression</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs039.html#___sec38" style="font-size: 80%;">Extending to more predictors</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs040.html#___sec39" style="font-size: 80%;">Including more classes</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs041.html#___sec40" style="font-size: 80%;">More classes</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs042.html#___sec41" style="font-size: 80%;">Friday September 24</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs043.html#___sec42" style="font-size: 80%;">Wisconsin Cancer Data</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs044.html#___sec43" style="font-size: 80%;">Using the correlation matrix</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs045.html#___sec44" style="font-size: 80%;">Discussing the correlation data</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs046.html#___sec45" style="font-size: 80%;">Other measures in classification studies: Cancer Data  again</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs047.html#___sec46" style="font-size: 80%;">Optimization, the central part of any Machine Learning algortithm</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs048.html#___sec47" style="font-size: 80%;">Revisiting our Logistic Regression case</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs049.html#___sec48" style="font-size: 80%;">The equations to solve</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs050.html#___sec49" style="font-size: 80%;">Solving using Newton-Raphson's method</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs051.html#___sec50" style="font-size: 80%;">Brief reminder on Newton-Raphson's method</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs052.html#___sec51" style="font-size: 80%;">The equations</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs053.html#___sec52" style="font-size: 80%;">Simple geometric interpretation</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs054.html#___sec53" style="font-size: 80%;">Extending to more than one variable</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs055.html#___sec54" style="font-size: 80%;">Steepest descent</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs056.html#___sec55" style="font-size: 80%;">More on Steepest descent</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs057.html#___sec56" style="font-size: 80%;">The ideal</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs058.html#___sec57" style="font-size: 80%;">The sensitiveness of the gradient descent</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs059.html#___sec58" style="font-size: 80%;">Convex functions</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs060.html#___sec59" style="font-size: 80%;">Convex function</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs061.html#___sec60" style="font-size: 80%;">Conditions on convex functions</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs062.html#___sec61" style="font-size: 80%;">More on convex functions</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs063.html#___sec62" style="font-size: 80%;">Some simple problems</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs064.html#___sec63" style="font-size: 80%;">Friday September 25</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs065.html#___sec64" style="font-size: 80%;">Standard steepest descent</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs066.html#___sec65" style="font-size: 80%;">Gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs067.html#___sec66" style="font-size: 80%;">Steepest descent  method</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs068.html#___sec67" style="font-size: 80%;">Steepest descent  method</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs069.html#___sec68" style="font-size: 80%;">Final expressions</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs070.html#___sec69" style="font-size: 80%;">Steepest descent example</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs071.html#___sec70" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs072.html#___sec71" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs073.html#___sec72" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs074.html#___sec73" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs075.html#___sec74" style="font-size: 80%;">Conjugate gradient method and iterations</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs076.html#___sec75" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs077.html#___sec76" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs078.html#___sec77" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs079.html#___sec78" style="font-size: 80%;">Revisiting some of our first Linear Regression Encounters</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs080.html#___sec79" style="font-size: 80%;">Gradient descent example</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs081.html#___sec80" style="font-size: 80%;">The derivative of the cost/loss function</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs082.html#___sec81" style="font-size: 80%;">The Hessian matrix</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs083.html#___sec82" style="font-size: 80%;">Simple program</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs084.html#___sec83" style="font-size: 80%;">Gradient Descent Example</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs085.html#___sec84" style="font-size: 80%;">And a corresponding example using <b>scikit-learn</b></a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs086.html#___sec85" style="font-size: 80%;">Gradient descent and Ridge</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs087.html#___sec86" style="font-size: 80%;">Program example for gradient descent with Ridge Regression</a></li>
     <!-- navigation toc: --> <li><a href="._week38-bs088.html#___sec87" style="font-size: 80%;">Using gradient descent methods, limitations</a></li>
>>>>>>> 7c4b51b2e2a6a4ece2432ee81fe3530f2502d8e3

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->

<div class="container">

<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->

<a name="part0014"></a>
<!-- !split -->

<<<<<<< HEAD
<h2 id="wrapping-it-up" class="anchor">Wrapping it up </h2>

<p>
If we minimize with respect to \( \boldsymbol{\beta} \) we have then

$$
\hat{\boldsymbol{\beta}} = (\tilde{X}^T\tilde{X})^{-1}\tilde{X}^T\boldsymbol{\tilde{y}},
$$

<p>
where \( \boldsymbol{\tilde{y}} = \boldsymbol{y} - \overline{\boldsymbol{y}} \)
and \( \tilde{X}_{ij} = X_{ij} - \frac{1}{n}\sum_{k=0}^{n-1}X_{kj} \).

<p>
For Ridge regression we need to add \( \lambda \boldsymbol{\beta}^T\boldsymbol{\beta} \) to the cost function and get then
$$
\hat{\boldsymbol{\beta}} = (\tilde{X}^T\tilde{X} + \lambda I)^{-1}\tilde{X}^T\boldsymbol{\tilde{y}}.
$$

=======
<h2 id="___sec13" class="anchor">Taking out the mean </h2>
>>>>>>> 7c4b51b2e2a6a4ece2432ee81fe3530f2502d8e3
<p>
What does this mean for thus? And why do we insist on all this?

<<<<<<< HEAD
=======
<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">pandas</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">pd</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> train_test_split
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn</span> <span style="color: #008000; font-weight: bold">import</span> linear_model
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.preprocessing</span> <span style="color: #008000; font-weight: bold">import</span> StandardScaler

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">MSE</span>(y_data,y_model):
    n <span style="color: #666666">=</span> np<span style="color: #666666">.</span>size(y_model)
    <span style="color: #008000; font-weight: bold">return</span> np<span style="color: #666666">.</span>sum((y_data<span style="color: #666666">-</span>y_model)<span style="color: #666666">**2</span>)<span style="color: #666666">/</span>n
<span style="color: #408080; font-style: italic"># A seed just to ensure that the random numbers are the same for every run.</span>
<span style="color: #408080; font-style: italic"># Useful for eventual debugging.</span>
np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>seed(<span style="color: #666666">315</span>)

n <span style="color: #666666">=</span> <span style="color: #666666">100</span>
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(n)
y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>x<span style="color: #666666">**2</span>) <span style="color: #666666">+</span> <span style="color: #666666">1.5</span> <span style="color: #666666">*</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(x<span style="color: #666666">-2</span>)<span style="color: #666666">**2</span>)

Maxpolydegree <span style="color: #666666">=</span> <span style="color: #666666">20</span>
X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros((n,Maxpolydegree<span style="color: #666666">-1</span>))

<span style="color: #008000; font-weight: bold">for</span> degree <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #666666">1</span>,Maxpolydegree): <span style="color: #408080; font-style: italic">#No intercept column</span>
    X[:,degree<span style="color: #666666">-1</span>] <span style="color: #666666">=</span> x<span style="color: #666666">**</span>(degree)

<span style="color: #408080; font-style: italic"># We split the data in test and training data</span>
X_train, X_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(X, y, test_size<span style="color: #666666">=0.2</span>)





<span style="color: #408080; font-style: italic">#For our own implementation, we will need to deal with the intercept by centering the design matrix and the target variable</span>
X_train_mean <span style="color: #666666">=</span> np<span style="color: #666666">.</span>mean(X_train,axis<span style="color: #666666">=0</span>)
<span style="color: #408080; font-style: italic">#Center by removing mean from each feature</span>
X_train_scaled <span style="color: #666666">=</span> X_train <span style="color: #666666">-</span> X_train_mean 
X_test_scaled <span style="color: #666666">=</span> X_test <span style="color: #666666">-</span> X_train_mean
<span style="color: #408080; font-style: italic">#The model intercept (called y_scaler) is given by the mean of target variable (IF X is centered)</span>
<span style="color: #408080; font-style: italic">#Remove the intercept from the training data.</span>
y_scaler <span style="color: #666666">=</span> np<span style="color: #666666">.</span>mean(y_train)           
y_train_scaled <span style="color: #666666">=</span> y_train <span style="color: #666666">-</span> y_scaler   


p <span style="color: #666666">=</span> Maxpolydegree<span style="color: #666666">-1</span>
I <span style="color: #666666">=</span> np<span style="color: #666666">.</span>eye(p,p)
<span style="color: #408080; font-style: italic"># Decide which values of lambda to use</span>
nlambdas <span style="color: #666666">=</span> <span style="color: #666666">4</span>
MSEOwnRidgePredict <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(nlambdas)
MSERidgePredict <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(nlambdas)

lambdas <span style="color: #666666">=</span> np<span style="color: #666666">.</span>logspace(<span style="color: #666666">-4</span>, <span style="color: #666666">1</span>, nlambdas)
<span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(nlambdas):
    lmb <span style="color: #666666">=</span> lambdas[i]
    OwnRidgeBeta <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linalg<span style="color: #666666">.</span>pinv(X_train_scaled<span style="color: #666666">.</span>T <span style="color: #666666">@</span> X_train_scaled<span style="color: #666666">+</span>lmb<span style="color: #666666">*</span>I) <span style="color: #666666">@</span> X_train_scaled<span style="color: #666666">.</span>T <span style="color: #666666">@</span> (y_train_scaled)
    intercept_ <span style="color: #666666">=</span> y_scaler <span style="color: #666666">-</span> X_train_mean<span style="color: #AA22FF">@OwnRidgeBeta</span> <span style="color: #408080; font-style: italic">#The intercept can be shifted so the model can predict on uncentered data</span>
    <span style="color: #408080; font-style: italic">#Add intercept to prediction</span>
    ypredictOwnRidge <span style="color: #666666">=</span> X_test <span style="color: #666666">@</span> OwnRidgeBeta <span style="color: #666666">+</span> intercept_ 
    <span style="color: #408080; font-style: italic">#EQUIVALENT PREDICTION:</span>
    <span style="color: #408080; font-style: italic">#Add intercept to prediction</span>
    ypredictOwnRidge <span style="color: #666666">=</span> X_test_scaled <span style="color: #666666">@</span> OwnRidgeBeta <span style="color: #666666">+</span> y_scaler 
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Values for own Ridge prediction&quot;</span>)
    <span style="color: #008000">print</span>(ypredictOwnRidge)
    RegRidge <span style="color: #666666">=</span> linear_model<span style="color: #666666">.</span>Ridge(lmb)
    RegRidge<span style="color: #666666">.</span>fit(X_train,y_train)
    ypredictRidge <span style="color: #666666">=</span> RegRidge<span style="color: #666666">.</span>predict(X_test)
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Values for SL Ridge prediction&quot;</span>)
    <span style="color: #008000">print</span>(ypredictRidge)
    MSEOwnRidgePredict[i] <span style="color: #666666">=</span> MSE(y_test,ypredictOwnRidge)
    MSERidgePredict[i] <span style="color: #666666">=</span> MSE(y_test,ypredictRidge)
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Beta values for own Ridge implementation&quot;</span>)
    <span style="color: #008000">print</span>(OwnRidgeBeta) <span style="color: #408080; font-style: italic">#Intercept is given by mean of target variable</span>
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Beta values for Scikit-Learn Ridge implementation&quot;</span>)
    <span style="color: #008000">print</span>(RegRidge<span style="color: #666666">.</span>coef_)
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Intercept from own implementation:&#39;</span>)
    <span style="color: #008000">print</span>(intercept_)
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Intercept from Scikit-Learn Ridge implementation&#39;</span>)
    <span style="color: #008000">print</span>(RegRidge<span style="color: #666666">.</span>intercept_)

<span style="color: #408080; font-style: italic"># Now plot the results</span>
plt<span style="color: #666666">.</span>figure()
plt<span style="color: #666666">.</span>plot(np<span style="color: #666666">.</span>log10(lambdas), MSEOwnRidgePredict, <span style="color: #BA2121">&#39;b--&#39;</span>, label <span style="color: #666666">=</span> <span style="color: #BA2121">&#39;MSE own Ridge Test&#39;</span>)
plt<span style="color: #666666">.</span>plot(np<span style="color: #666666">.</span>log10(lambdas), MSERidgePredict, <span style="color: #BA2121">&#39;g--&#39;</span>, label <span style="color: #666666">=</span> <span style="color: #BA2121">&#39;MSE SL Ridge Test&#39;</span>)
plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&#39;log10(lambda)&#39;</span>)
plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&#39;MSE&#39;</span>)
plt<span style="color: #666666">.</span>legend()
plt<span style="color: #666666">.</span>show()
</pre></div>
>>>>>>> 7c4b51b2e2a6a4ece2432ee81fe3530f2502d8e3
<p>
<p>
<!-- navigation buttons at the bottom of the page -->
<ul class="pagination">
<li><a href="._week38-bs013.html">&laquo;</a></li>
  <li><a href="._week38-bs000.html">1</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week38-bs006.html">7</a></li>
  <li><a href="._week38-bs007.html">8</a></li>
  <li><a href="._week38-bs008.html">9</a></li>
  <li><a href="._week38-bs009.html">10</a></li>
  <li><a href="._week38-bs010.html">11</a></li>
  <li><a href="._week38-bs011.html">12</a></li>
  <li><a href="._week38-bs012.html">13</a></li>
  <li><a href="._week38-bs013.html">14</a></li>
  <li class="active"><a href="._week38-bs014.html">15</a></li>
  <li><a href="._week38-bs015.html">16</a></li>
  <li><a href="._week38-bs016.html">17</a></li>
  <li><a href="._week38-bs017.html">18</a></li>
  <li><a href="._week38-bs018.html">19</a></li>
  <li><a href="._week38-bs019.html">20</a></li>
  <li><a href="._week38-bs020.html">21</a></li>
  <li><a href="._week38-bs021.html">22</a></li>
  <li><a href="._week38-bs022.html">23</a></li>
  <li><a href="._week38-bs023.html">24</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week38-bs090.html">91</a></li>
  <li><a href="._week38-bs015.html">&raquo;</a></li>
</ul>
<!-- ------------------- end of main content --------------- -->

</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>

<!-- Bootstrap footer
<footer>
<a href="http://..."><img width="250" align=right src="http://..."></a>
</footer>
-->


<center style="font-size:80%">
<!-- copyright only on the titlepage -->
</center>


</body>
</html>
    


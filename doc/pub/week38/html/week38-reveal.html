<!DOCTYPE html>

<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Data Analysis and Machine Learning: Logistic Regression">

<title>Data Analysis and Machine Learning: Logistic Regression</title>







<!-- reveal.js: https://lab.hakim.se/reveal-js/ -->

<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

<link rel="stylesheet" href="reveal.js/css/reveal.css">
<link rel="stylesheet" href="reveal.js/css/theme/beige.css" id="theme">
<!--
<link rel="stylesheet" href="reveal.js/css/reveal.css">
<link rel="stylesheet" href="reveal.js/css/theme/beige.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/beigesmall.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/solarized.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/serif.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/night.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/moon.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/simple.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/sky.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/darkgray.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/default.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/cbc.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/simula.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/white.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/league.css" id="theme">
-->

<!-- For syntax highlighting -->
<link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">

<!-- Printing and PDF exports -->
<script>
var link = document.createElement( 'link' );
link.rel = 'stylesheet';
link.type = 'text/css';
link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
document.getElementsByTagName( 'head' )[0].appendChild( link );
</script>

<style type="text/css">
    hr { border: 0; width: 80%; border-bottom: 1px solid #aaa}
    p.caption { width: 80%; font-size: 60%; font-style: italic; text-align: left; }
    hr.figure { border: 0; width: 80%; border-bottom: 1px solid #aaa}
    .reveal .alert-text-small   { font-size: 80%;  }
    .reveal .alert-text-large   { font-size: 130%; }
    .reveal .alert-text-normal  { font-size: 90%;  }
    .reveal .alert {
             padding:8px 35px 8px 14px; margin-bottom:18px;
             text-shadow:0 1px 0 rgba(255,255,255,0.5);
             border:5px solid #bababa;
             -webkit-border-radius: 14px; -moz-border-radius: 14px;
             border-radius:14px;
             background-position: 10px 10px;
             background-repeat: no-repeat;
             background-size: 38px;
             padding-left: 30px; /* 55px; if icon */
     }
     .reveal .alert-block {padding-top:14px; padding-bottom:14px}
     .reveal .alert-block > p, .alert-block > ul {margin-bottom:1em}
     /*.reveal .alert li {margin-top: 1em}*/
     .reveal .alert-block p+p {margin-top:5px}
     /*.reveal .alert-notice { background-image: url(https://hplgit.github.io/doconce/bundled/html_images/small_gray_notice.png); }
     .reveal .alert-summary  { background-image:url(https://hplgit.github.io/doconce/bundled/html_images/small_gray_summary.png); }
     .reveal .alert-warning { background-image: url(https://hplgit.github.io/doconce/bundled/html_images/small_gray_warning.png); }
     .reveal .alert-question {background-image:url(https://hplgit.github.io/doconce/bundled/html_images/small_gray_question.png); } */

</style>



<!-- Styles for table layout of slides -->
<style type="text/css">
td.padding {
  padding-top:20px;
  padding-bottom:20px;
  padding-right:50px;
  padding-left:50px;
}
</style>

</head>

<body>
<div class="reveal">

<!-- Any section element inside the <div class="slides"> container
     is displayed as a slide -->

<div class="slides">





<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    



<section>
<!-- ------------------- main content ---------------------- -->



<center><h1 style="text-align: center;">Data Analysis and Machine Learning: Logistic Regression</h1></center>  <!-- document title -->

<p>
<!-- author(s): Morten Hjorth-Jensen -->

<center>
<b>Morten Hjorth-Jensen</b> [1, 2]
</center>

<p>&nbsp;<br>
<!-- institution(s) -->

<center>[1] <b>Department of Physics, University of Oslo</b></center>
<center>[2] <b>Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University</b></center>
<br>
<p>&nbsp;<br>
<center><h4>Sep 21, 2021</h4></center> <!-- date -->
<br>
<p>

<center style="font-size:80%">
<!-- copyright --> &copy; 1999-2021, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license
</center>
</section>


<section>
<h2 id="plans-for-week-38">Plans for week 38 </h2>

<ul>
<p><li> Thursday: Summary of regression methods and discussion of project 1.  Start Logistic Regression</li>
<p><li> Friday: Logistic Regression and Optimization methods</li>
</ul>
</section>


<section>
<h2 id="ridge-and-lasso-regression-reminder">Ridge and LASSO Regression, reminder </h2>

<p>
The expression for the standard Mean Squared Error (MSE) which we used to define our cost function and the equations for the ordinary least squares (OLS) method, that is 
our optimization problem is
<p>&nbsp;<br>
$$
{\displaystyle \min_{\boldsymbol{\beta}\in {\mathbb{R}}^{p}}}\frac{1}{n}\left\{\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\right)^T\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\right)\right\}.
$$
<p>&nbsp;<br>

or we can state it as
<p>&nbsp;<br>
$$
{\displaystyle \min_{\boldsymbol{\beta}\in
{\mathbb{R}}^{p}}}\frac{1}{n}\sum_{i=0}^{n-1}\left(y_i-\tilde{y}_i\right)^2=\frac{1}{n}\vert\vert \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\vert\vert_2^2,
$$
<p>&nbsp;<br>

where we have used the definition of  a norm-2 vector, that is
<p>&nbsp;<br>
$$
\vert\vert \boldsymbol{x}\vert\vert_2 = \sqrt{\sum_i x_i^2}. 
$$
<p>&nbsp;<br>

<p>
By minimizing the above equation with respect to the parameters
\( \boldsymbol{\beta} \) we could then obtain an analytical expression for the
parameters \( \boldsymbol{\beta} \).  We can add a regularization parameter \( \lambda \) by
defining a new cost function to be optimized, that is

<p>&nbsp;<br>
$$
{\displaystyle \min_{\boldsymbol{\beta}\in
{\mathbb{R}}^{p}}}\frac{1}{n}\vert\vert \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\vert\vert_2^2+\lambda\vert\vert \boldsymbol{\beta}\vert\vert_2^2
$$
<p>&nbsp;<br>

<p>
which leads to the Ridge regression minimization problem where we
require that \( \vert\vert \boldsymbol{\beta}\vert\vert_2^2\le t \), where \( t \) is
a finite number larger than zero. By defining

<p>&nbsp;<br>
$$
C(\boldsymbol{X},\boldsymbol{\beta})=\frac{1}{n}\vert\vert \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\vert\vert_2^2+\lambda\vert\vert \boldsymbol{\beta}\vert\vert_1,
$$
<p>&nbsp;<br>

<p>
we have a new optimization equation
<p>&nbsp;<br>
$$
{\displaystyle \min_{\boldsymbol{\beta}\in
{\mathbb{R}}^{p}}}\frac{1}{n}\vert\vert \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\vert\vert_2^2+\lambda\vert\vert \boldsymbol{\beta}\vert\vert_1
$$
<p>&nbsp;<br>

which leads to Lasso regression. Lasso stands for least absolute shrinkage and selection operator.

<p>
Here we have defined the norm-1 as 
<p>&nbsp;<br>
$$
\vert\vert \boldsymbol{x}\vert\vert_1 = \sum_i \vert x_i\vert. 
$$
<p>&nbsp;<br>
</section>


<section>
<h2 id="various-steps-in-cross-validation">Various steps in cross-validation </h2>

<p>
When the repetitive splitting of the data set is done randomly,
samples may accidently end up in a fast majority of the splits in
either training or test set. Such samples may have an unbalanced
influence on either model building or prediction evaluation. To avoid
this \( k \)-fold cross-validation structures the data splitting. The
samples are divided into \( k \) more or less equally sized exhaustive and
mutually exclusive subsets. In turn (at each split) one of these
subsets plays the role of the test set while the union of the
remaining subsets constitutes the training set. Such a splitting
warrants a balanced representation of each sample in both training and
test set over the splits. Still the division into the \( k \) subsets
involves a degree of randomness. This may be fully excluded when
choosing \( k=n \). This particular case is referred to as leave-one-out
cross-validation (LOOCV).
</section>


<section>
<h2 id="how-to-set-up-the-cross-validation-for-ridge-and-or-lasso">How to set up the cross-validation for Ridge and/or Lasso </h2>

<ul>
<p><li> Define a range of interest for the penalty parameter.</li>
<p><li> Divide the data set into training and test set comprising samples \( \{1, \ldots, n\} \setminus i \) and \( \{ i \} \), respectively.</li>
<p><li> Fit the linear regression model by means of ridge estimation  for each \( \lambda \) in the grid using the training set, and the corresponding estimate of the error variance \( \boldsymbol{\sigma}_{-i}^2(\lambda) \), as</li>
</ul>
<p>&nbsp;<br>
$$
\begin{align*}
\boldsymbol{\beta}_{-i}(\lambda) & =  ( \boldsymbol{X}_{-i, \ast}^{T}
\boldsymbol{X}_{-i, \ast} + \lambda \boldsymbol{I}_{pp})^{-1}
\boldsymbol{X}_{-i, \ast}^{T} \boldsymbol{y}_{-i}
\end{align*}
$$
<p>&nbsp;<br>


<ul>
<p><li> Evaluate the prediction performance of these models on the test set by \( \log\{L[y_i, \boldsymbol{X}_{i, \ast}; \boldsymbol{\beta}_{-i}(\lambda), \boldsymbol{\sigma}_{-i}^2(\lambda)]\} \). Or, by the prediction error \( |y_i - \boldsymbol{X}_{i, \ast} \boldsymbol{\beta}_{-i}(\lambda)| \), the relative error, the error squared or the R2 score function.</li>
<p><li> Repeat the first three steps  such that each sample plays the role of the test set once.</li>
<p><li> Average the prediction performances of the test sets at each grid point of the penalty bias/parameter. It is an estimate of the prediction performance of the model corresponding to this value of the penalty parameter on novel data. It is defined as</li>
</ul>
<p>&nbsp;<br>
$$
\begin{align*}
\frac{1}{n} \sum_{i = 1}^n \log\{L[y_i, \mathbf{X}_{i, \ast}; \boldsymbol{\beta}_{-i}(\lambda), \boldsymbol{\sigma}_{-i}^2(\lambda)]\}.
\end{align*}
$$
<p>&nbsp;<br>
</section>


<section>
<h2 id="cross-validation-in-brief">Cross-validation in brief </h2>

<p>
For the various values of \( k \)

<ol>
<p><li> shuffle the dataset randomly.</li>
<p><li> Split the dataset into \( k \) groups.</li>
<p><li> For each unique group:

<ol type="a"></li>
<p><li> Decide which group to use as set for test data</li>
<p><li> Take the remaining groups as a training data set</li>
<p><li> Fit a model on the training set and evaluate it on the test set</li>
<p><li> Retain the evaluation score and discard the model</li>
</ol>
<p><li> Summarize the model using the sample of model evaluation scores</li>
</ol>
</section>


<section>
<h2 id="code-example-for-cross-validation-and-k-fold-cross-validation">Code Example for Cross-validation and \( k \)-fold Cross-validation </h2>

<p>
The code here uses Ridge regression with cross-validation (CV)  resampling and \( k \)-fold CV in order to fit a specific polynomial. 
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.model_selection</span> <span style="color: #8B008B; font-weight: bold">import</span> KFold
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.linear_model</span> <span style="color: #8B008B; font-weight: bold">import</span> Ridge
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.model_selection</span> <span style="color: #8B008B; font-weight: bold">import</span> cross_val_score
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.preprocessing</span> <span style="color: #8B008B; font-weight: bold">import</span> PolynomialFeatures

<span style="color: #228B22"># A seed just to ensure that the random numbers are the same for every run.</span>
<span style="color: #228B22"># Useful for eventual debugging.</span>
np.random.seed(<span style="color: #B452CD">3155</span>)

<span style="color: #228B22"># Generate the data.</span>
nsamples = <span style="color: #B452CD">100</span>
x = np.random.randn(nsamples)
y = <span style="color: #B452CD">3</span>*x**<span style="color: #B452CD">2</span> + np.random.randn(nsamples)

<span style="color: #228B22">## Cross-validation on Ridge regression using KFold only</span>

<span style="color: #228B22"># Decide degree on polynomial to fit</span>
poly = PolynomialFeatures(degree = <span style="color: #B452CD">6</span>)

<span style="color: #228B22"># Decide which values of lambda to use</span>
nlambdas = <span style="color: #B452CD">500</span>
lambdas = np.logspace(-<span style="color: #B452CD">3</span>, <span style="color: #B452CD">5</span>, nlambdas)

<span style="color: #228B22"># Initialize a KFold instance</span>
k = <span style="color: #B452CD">5</span>
kfold = KFold(n_splits = k)

<span style="color: #228B22"># Perform the cross-validation to estimate MSE</span>
scores_KFold = np.zeros((nlambdas, k))

i = <span style="color: #B452CD">0</span>
<span style="color: #8B008B; font-weight: bold">for</span> lmb <span style="color: #8B008B">in</span> lambdas:
    ridge = Ridge(alpha = lmb)
    j = <span style="color: #B452CD">0</span>
    <span style="color: #8B008B; font-weight: bold">for</span> train_inds, test_inds <span style="color: #8B008B">in</span> kfold.split(x):
        xtrain = x[train_inds]
        ytrain = y[train_inds]

        xtest = x[test_inds]
        ytest = y[test_inds]

        Xtrain = poly.fit_transform(xtrain[:, np.newaxis])
        ridge.fit(Xtrain, ytrain[:, np.newaxis])

        Xtest = poly.fit_transform(xtest[:, np.newaxis])
        ypred = ridge.predict(Xtest)

        scores_KFold[i,j] = np.sum((ypred - ytest[:, np.newaxis])**<span style="color: #B452CD">2</span>)/np.size(ypred)

        j += <span style="color: #B452CD">1</span>
    i += <span style="color: #B452CD">1</span>


estimated_mse_KFold = np.mean(scores_KFold, axis = <span style="color: #B452CD">1</span>)

<span style="color: #228B22">## Cross-validation using cross_val_score from sklearn along with KFold</span>

<span style="color: #228B22"># kfold is an instance initialized above as:</span>
<span style="color: #228B22"># kfold = KFold(n_splits = k)</span>

estimated_mse_sklearn = np.zeros(nlambdas)
i = <span style="color: #B452CD">0</span>
<span style="color: #8B008B; font-weight: bold">for</span> lmb <span style="color: #8B008B">in</span> lambdas:
    ridge = Ridge(alpha = lmb)

    X = poly.fit_transform(x[:, np.newaxis])
    estimated_mse_folds = cross_val_score(ridge, X, y[:, np.newaxis], scoring=<span style="color: #CD5555">&#39;neg_mean_squared_error&#39;</span>, cv=kfold)

    <span style="color: #228B22"># cross_val_score return an array containing the estimated negative mse for every fold.</span>
    <span style="color: #228B22"># we have to the the mean of every array in order to get an estimate of the mse of the model</span>
    estimated_mse_sklearn[i] = np.mean(-estimated_mse_folds)

    i += <span style="color: #B452CD">1</span>

<span style="color: #228B22">## Plot and compare the slightly different ways to perform cross-validation</span>

plt.figure()

plt.plot(np.log10(lambdas), estimated_mse_sklearn, label = <span style="color: #CD5555">&#39;cross_val_score&#39;</span>)
plt.plot(np.log10(lambdas), estimated_mse_KFold, <span style="color: #CD5555">&#39;r--&#39;</span>, label = <span style="color: #CD5555">&#39;KFold&#39;</span>)

plt.xlabel(<span style="color: #CD5555">&#39;log10(lambda)&#39;</span>)
plt.ylabel(<span style="color: #CD5555">&#39;mse&#39;</span>)

plt.legend()

plt.show()
</pre></div>
</section>


<section>
<h2 id="to-think-about-first-part">To think about, first part </h2>

<p>
When you are comparing your own code with for example <b>Scikit-Learn</b>'s
library, there are some things to keep in mind.  The examples
here demonstrate some of these aspects with potential pitfalls.

<p>
The discussion here focuses on the role of the intercept, how we can
set up the design matrix, what scaling we should use and other topics
which may confuse us.

<p>
Yes, it could be a bad idea to include the intercept column for the
exact reason you stated. If no transformation is applied to your&#160;data,
the intercept can be interpreted as the expected value of your target
variable when all your predictors are put to zero. Therefore, whenever
you cannot assume that the expected target variable is zero when all
your predictors are zero, it could be a bad idea to apply a model
which penalizes the intercept. Also, the analytical solution to the
ridge regression coefficients (when not shrinking <p>&nbsp;<br>
$$\beta_0$$
<p>&nbsp;<br>) is
derived&#160;under the assumption that both y and X are zero centered (mean
subtracted). What you are doing is correct, but you should&#160;also zero
center X (subtracting the mean of each column from the corresponding
column).&#160;

<p>
If your predictors&#160;are of different&#160;scales, I would advice you to
standardize&#160;X by&#160;subtracting the mean of each column from the
corresponding column and dividing the column with its standard
deviation.&#160;If you dont do this, you will give an "unfair"&#160;penalization
of the parameters since their magnitude&#160;depends on the scale of their
corresponding&#160;predictor.&#160;Suppose that you have an input&#160;variable
"height". Human height might be measured in inches or meters or
kilometers. If measured in kilometers, a&#160;standard linear regression
model with this predictor would probably give a much bigger
coefficient term, than if measured in millimeters. You may&#160;see how
this could become a problem when considering the loss function for
ridge regression.

<p>
Remember that when you do any transformation to your dataset before
training, the exact same transformation has to be applied to new data
before making a prediction. In your case, this means:

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span><span style="color: #228B22">#Model training:</span>
y_train_mean = np.mean(y_train)
X_train_mean = np.mean(X_train,axis=<span style="color: #B452CD">0</span>)
X_train = X_train - X_train_mean
y_train = y_train - y_train_mean

trained_model = some_model.fit(X_train,y_train)

<span style="color: #228B22">#Model prediction:</span>
X_test = X_test - X_train_mean <span style="color: #228B22">#Use mean from training data</span>
y_pred = trained_model(X_test)
y_pred = y_pred + y_train_mean
</pre></div>
<p>
Here is a mathematical explanation of the&#160;zero centering:

<p>
The cost/loss function  for Ridge regression is:

<p>&nbsp;<br>
$$
C(\beta_0, \beta_1, ... , \beta_P) = \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{p=1}^P X_{ip}\beta_p)^2 + \lambda \sum_{p=1}^P \beta_p^2.
$$
<p>&nbsp;<br>

<p>
Notice that the intercept is left out of the \( L_2 \) regularization term. The design matrix
\( X \) does in this case not contain any intercept column. We want

<p>&nbsp;<br>
$$
\frac{\partial L}{\partial \beta_j} = 0,
$$
<p>&nbsp;<br>

<p>
for all \( j \), so lets start with \( \beta_0 \). This means that we have

<p>&nbsp;<br>
$$
\frac{\partial L}{\partial \beta_0} = -2\sum_{i=1}^{n} (y_i - \beta_0 - \sum_{p=1}^P X_{ip} \beta_p).
$$
<p>&nbsp;<br>

<p>
We want to solve
<p>&nbsp;<br>
$$
-2\sum_{i=1}^{n} (y_i - \beta_0 - \sum_{p=1}^P X_{ip} \beta_p) = 0,
$$
<p>&nbsp;<br>

<p>
which gives
<p>&nbsp;<br>
$$
\sum_{i=1}^{n} \beta_0 = \sum_{i=1}^{n}y_i - \sum_{i=1}^{n} \sum_{p=1}^P X_{ip} \beta_p,
$$
<p>&nbsp;<br>

<p>
or
$ n\beta_0 = \sum_{i=1}^{n} y_i - \sum_{p=1}^P\beta_p \sum_{i=1}^{n} X_{ip}$.

<p>
If we assume that every column of \( X \) is centered, whic we can do by subtracting the mean,
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span>X = X - np.mean(X,axis=<span style="color: #B452CD">0</span>)
</pre></div>
<p>
the sum $ \sum_{i=1}^{n} X_{ip} $

<p>
can be rewritten as
<p>&nbsp;<br>
$$
\sum_{i=1}^{n} (X_{ip} - \frac{1}{n}\sum_{i=1}^{n} X_{ip}) = \sum_{i=1}^{n} X_{ip} - \sum_{i=1}^{n} \frac{1}{n} \sum_{i=1}^{n}X_{ip},
$$
<p>&nbsp;<br>

resulting in
<p>&nbsp;<br>
$$
\sum_{i=1}^{n} X_{ip} - n \frac{1}{n} \sum_{i=1}^{n}X_{ip} = 0.
$$
<p>&nbsp;<br>

<p>
Finally we have
<p>&nbsp;<br>
$$
n\beta_0 = \sum_{i=1}^{n} y_i - \sum_{p=1}^P\beta_p \sum_{i=1}^{n} X_{ip},
$$
<p>&nbsp;<br>

or
<p>&nbsp;<br>
$$
\beta_0 = \frac{1}{n}\sum_{i=1}^{n} y_i = y_{average}.
$$
<p>&nbsp;<br>

<p>
Replacing \( y_i \) with \( y_i - \beta_0 = y_i - y_{average} \) in the loss function will give us (in vector-matrix disguise)
<p>&nbsp;<br>
$$
C(\boldsymbol{\beta}) = (\boldsymbol{\tilde{y}} - \tilde{X}\boldsymbol{\beta})^T(\boldsymbol{\tilde{y}} - \tilde{X}\boldsymbol{\beta}) + \lambda \boldsymbol{\beta}^T\boldsymbol{\beta},
$$
<p>&nbsp;<br>

<p>
which has the solution

<p>
\( \beta = (\tilde{X}^T\tilde{X} + \lambda I)^{-1}\tilde{X}^T\boldsymbol{\tilde{y}} \).
where \( \boldsymbol{\tilde{y}} = \boldsymbol{y} - y_{average} \)
and \( \tilde{X}_{ij} = X_{ij} - \frac{1}{n}\sum_{k=1}^{n-1}X_{kj} \).

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">pandas</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">pd</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.model_selection</span> <span style="color: #8B008B; font-weight: bold">import</span> train_test_split
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn</span> <span style="color: #8B008B; font-weight: bold">import</span> linear_model

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">R2</span>(y_data, y_model):
    <span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #B452CD">1</span> - np.sum((y_data - y_model) ** <span style="color: #B452CD">2</span>) / np.sum((y_data - np.mean(y_data)) ** <span style="color: #B452CD">2</span>)
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">MSE</span>(y_data,y_model):
    n = np.size(y_model)
    <span style="color: #8B008B; font-weight: bold">return</span> np.sum((y_data-y_model)**<span style="color: #B452CD">2</span>)/n


<span style="color: #228B22"># A seed just to ensure that the random numbers are the same for every run.</span>
<span style="color: #228B22"># Useful for eventual debugging.</span>
np.random.seed(<span style="color: #B452CD">3155</span>)

n = <span style="color: #B452CD">100</span>
x = np.random.rand(n)
y = np.exp(-x**<span style="color: #B452CD">2</span>) + <span style="color: #B452CD">1.5</span> * np.exp(-(x-<span style="color: #B452CD">2</span>)**<span style="color: #B452CD">2</span>)

Maxpolydegree = <span style="color: #B452CD">20</span>
X = np.zeros((n,Maxpolydegree))
X[:,<span style="color: #B452CD">0</span>] = <span style="color: #B452CD">1.0</span>

<span style="color: #8B008B; font-weight: bold">for</span> polydegree <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #B452CD">1</span>, Maxpolydegree):
    <span style="color: #8B008B; font-weight: bold">for</span> degree <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(polydegree):
        X[:,degree] = x**degree


<span style="color: #228B22"># We split the data in test and training data</span>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span style="color: #B452CD">0.2</span>)

<span style="color: #228B22"># matrix inversion to find beta</span>
OLSbeta = np.linalg.pinv(X_train.T @ X_train) @ X_train.T @ y_train
<span style="color: #658b00">print</span>(OLSbeta)
<span style="color: #228B22"># and then make the prediction</span>
ytildeOLS = X_train @ OLSbeta
<span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;Training MSE for OLS&quot;</span>)
<span style="color: #658b00">print</span>(MSE(y_train,ytildeOLS))
ypredictOLS = X_test @ OLSbeta
<span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;Test MSE OLS&quot;</span>)
<span style="color: #658b00">print</span>(MSE(y_test,ypredictOLS))

p = <span style="color: #658b00">len</span>(OLSbeta)
I = np.eye(p,p)
<span style="color: #228B22"># Decide which values of lambda to use</span>
nlambdas = <span style="color: #B452CD">4</span>
MSEOwnRidgePredict = np.zeros(nlambdas)
MSEOwnRidgeTrain = np.zeros(nlambdas)
MSERidgePredict = np.zeros(nlambdas)
MSERidgeTrain = np.zeros(nlambdas)

lambdas = np.logspace(-<span style="color: #B452CD">4</span>, <span style="color: #B452CD">4</span>, nlambdas)
<span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(nlambdas):
    lmb = lambdas[i]
    OwnRidgeBeta = np.linalg.pinv(X_train.T @ X_train+lmb*I) @ X_train.T @ y_train
    <span style="color: #228B22"># include lasso using Scikit-Learn</span>
    <span style="color: #228B22"># Note: we include the intercept</span>
    RegRidge = linear_model.Ridge(lmb,fit_intercept=<span style="color: #8B008B; font-weight: bold">False</span>)
    RegRidge.fit(X_train,y_train)
    <span style="color: #228B22"># and then make the prediction</span>
    ytildeOwnRidge = X_train @ OwnRidgeBeta
    ypredictOwnRidge = X_test @ OwnRidgeBeta
    ytildeRidge = RegRidge.predict(X_train)
    ypredictRidge = RegRidge.predict(X_test)
    MSEOwnRidgePredict[i] = MSE(y_test,ypredictOwnRidge)
    MSEOwnRidgeTrain[i] = MSE(y_train,ytildeOwnRidge)
    MSERidgePredict[i] = MSE(y_test,ypredictRidge)
    MSERidgeTrain[i] = MSE(y_train,ytildeRidge)
    <span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;Beta values for own Ridge implementation&quot;</span>)
    <span style="color: #658b00">print</span>(OwnRidgeBeta)
    <span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;Beta values for Scikit-Learn Ridge implementation&quot;</span>)
    <span style="color: #658b00">print</span>(RegRidge.coef_)
<span style="color: #228B22"># Now plot the results</span>
plt.figure()
plt.plot(np.log10(lambdas), MSEOwnRidgeTrain, <span style="color: #CD5555">&#39;b&#39;</span>, label = <span style="color: #CD5555">&#39;MSE Ridge train&#39;</span>)
plt.plot(np.log10(lambdas), MSEOwnRidgePredict, <span style="color: #CD5555">&#39;r&#39;</span>, label = <span style="color: #CD5555">&#39;MSE Ridge Test&#39;</span>)
plt.plot(np.log10(lambdas), MSERidgeTrain, <span style="color: #CD5555">&#39;y&#39;</span>, label = <span style="color: #CD5555">&#39;MSE Ridge train&#39;</span>)
plt.plot(np.log10(lambdas), MSERidgePredict, <span style="color: #CD5555">&#39;g&#39;</span>, label = <span style="color: #CD5555">&#39;MSE Ridge Test&#39;</span>)

plt.xlabel(<span style="color: #CD5555">&#39;log10(lambda)&#39;</span>)
plt.ylabel(<span style="color: #CD5555">&#39;MSE&#39;</span>)
plt.legend()
plt.show()
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">pandas</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">pd</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.model_selection</span> <span style="color: #8B008B; font-weight: bold">import</span> train_test_split
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn</span> <span style="color: #8B008B; font-weight: bold">import</span> linear_model
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.preprocessing</span> <span style="color: #8B008B; font-weight: bold">import</span> StandardScaler

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">R2</span>(y_data, y_model):
    <span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #B452CD">1</span> - np.sum((y_data - y_model) ** <span style="color: #B452CD">2</span>) / np.sum((y_data - np.mean(y_data)) ** <span style="color: #B452CD">2</span>)
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">MSE</span>(y_data,y_model):
    n = np.size(y_model)
    <span style="color: #8B008B; font-weight: bold">return</span> np.sum((y_data-y_model)**<span style="color: #B452CD">2</span>)/n


<span style="color: #228B22"># A seed just to ensure that the random numbers are the same for every run.</span>
<span style="color: #228B22"># Useful for eventual debugging.</span>
np.random.seed(<span style="color: #B452CD">315</span>)

n = <span style="color: #B452CD">100</span>
x = np.random.rand(n)
y = np.exp(-x**<span style="color: #B452CD">2</span>) + <span style="color: #B452CD">1.5</span> * np.exp(-(x-<span style="color: #B452CD">2</span>)**<span style="color: #B452CD">2</span>)

Maxpolydegree = <span style="color: #B452CD">5</span>
X = np.zeros((n,Maxpolydegree-<span style="color: #B452CD">1</span>))

<span style="color: #8B008B; font-weight: bold">for</span> degree <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #B452CD">1</span>,Maxpolydegree): <span style="color: #228B22">#No intercept column</span>
    X[:,degree-<span style="color: #B452CD">1</span>] = x**(degree)




<span style="color: #228B22"># We split the data in test and training data</span>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span style="color: #B452CD">0.2</span>)





<span style="color: #228B22">#For our own implementation, we will need to deal with the intercept by centering the design matrix and the target variable</span>
X_train_mean = np.mean(X_train,axis=<span style="color: #B452CD">0</span>)
X_train_scaled = X_train - X_train_mean <span style="color: #228B22">#Center by removing mean from each feature</span>
X_test_scaled = X_test - X_train_mean

y_scaler = np.mean(y_train)           <span style="color: #228B22">#The model intercept (called y_scaler) is given by the mean of target variable (IF X is centered)</span>
y_train_scaled = y_train - y_scaler   <span style="color: #228B22">#Remove the intercept from the training data.</span>


p = Maxpolydegree-<span style="color: #B452CD">1</span>
I = np.eye(p,p)
<span style="color: #228B22"># Decide which values of lambda to use</span>
nlambdas = <span style="color: #B452CD">4</span>
MSEOwnRidgePredict = np.zeros(nlambdas)
MSERidgePredict = np.zeros(nlambdas)

lambdas = np.logspace(-<span style="color: #B452CD">4</span>, <span style="color: #B452CD">1</span>, nlambdas)
<span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(nlambdas):
    lmb = lambdas[i]
    OwnRidgeBeta = np.linalg.pinv(X_train_scaled.T @ X_train_scaled+lmb*I) @ X_train_scaled.T @ (y_train_scaled)
    intercept_ = y_scaler - X_train_mean<span style="color: #707a7c">@OwnRidgeBeta</span> <span style="color: #228B22">#The intercept can be shifted so the model can predict on uncentered data</span>
    
    ypredictOwnRidge = X_test @ OwnRidgeBeta + intercept_ <span style="color: #228B22">#Add intercept to prediction</span>
    <span style="color: #228B22">#EQUIVALENT PREDICTION:</span>
    ypredictOwnRidge = X_test_scaled @ OwnRidgeBeta + y_scaler <span style="color: #228B22">#Add intercept to prediction</span>
    <span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;Values for own Ridge prediction&quot;</span>)
    <span style="color: #658b00">print</span>(ypredictOwnRidge)

    

    RegRidge = linear_model.Ridge(lmb)
    RegRidge.fit(X_train,y_train)
    ypredictRidge = RegRidge.predict(X_test)
    <span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;Values for SL Ridge prediction&quot;</span>)
    <span style="color: #658b00">print</span>(ypredictRidge)


    MSEOwnRidgePredict[i] = MSE(y_test,ypredictOwnRidge)
    MSERidgePredict[i] = MSE(y_test,ypredictRidge)

    <span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;Beta values for own Ridge implementation&quot;</span>)
    <span style="color: #658b00">print</span>(OwnRidgeBeta) <span style="color: #228B22">#Intercept is given by mean of target variable</span>
    <span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;Beta values for Scikit-Learn Ridge implementation&quot;</span>)
    <span style="color: #658b00">print</span>(RegRidge.coef_)
    <span style="color: #658b00">print</span>(<span style="color: #CD5555">&#39;Intercept from own implementation:&#39;</span>)
    <span style="color: #658b00">print</span>(intercept_)
    <span style="color: #658b00">print</span>(<span style="color: #CD5555">&#39;Intercept from Scikit-Learn Ridge implementation&#39;</span>)
    <span style="color: #658b00">print</span>(RegRidge.intercept_)



<span style="color: #228B22"># Now plot the results</span>

plt.figure()
plt.plot(np.log10(lambdas), MSEOwnRidgePredict, <span style="color: #CD5555">&#39;b--&#39;</span>, label = <span style="color: #CD5555">&#39;MSE own Ridge Test&#39;</span>)
plt.plot(np.log10(lambdas), MSERidgePredict, <span style="color: #CD5555">&#39;g--&#39;</span>, label = <span style="color: #CD5555">&#39;MSE SL Ridge Test&#39;</span>)

plt.xlabel(<span style="color: #CD5555">&#39;log10(lambda)&#39;</span>)
plt.ylabel(<span style="color: #CD5555">&#39;MSE&#39;</span>)
plt.legend()
plt.show()
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>

<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.linear_model</span> <span style="color: #8B008B; font-weight: bold">import</span> LinearRegression


np.random.seed(<span style="color: #B452CD">2021</span>)


<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">fit_beta</span>(X, y):
    <span style="color: #8B008B; font-weight: bold">return</span> np.linalg.pinv(X.T @ X) @ X.T @ y


true_beta = [<span style="color: #B452CD">2</span>, <span style="color: #B452CD">0.5</span>, <span style="color: #B452CD">3.7</span>]

x = np.linspace(<span style="color: #B452CD">0</span>, <span style="color: #B452CD">1</span>, <span style="color: #B452CD">11</span>)
y = np.sum(
    np.asarray([x ** p * b <span style="color: #8B008B; font-weight: bold">for</span> p, b <span style="color: #8B008B">in</span> <span style="color: #658b00">enumerate</span>(true_beta)]), axis=<span style="color: #B452CD">0</span>
) + <span style="color: #B452CD">0.1</span> * np.random.normal(size=<span style="color: #658b00">len</span>(x))

degree = <span style="color: #B452CD">3</span>
X = np.zeros((<span style="color: #658b00">len</span>(x), degree))

<span style="color: #228B22"># Include the intercept in the design matrix</span>
<span style="color: #8B008B; font-weight: bold">for</span> p <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(degree):
    X[:, p] = x ** p

beta = fit_beta(X, y)

<span style="color: #228B22"># Intercept is included in the design matrix</span>
clf = LinearRegression(fit_intercept=<span style="color: #8B008B; font-weight: bold">False</span>).fit(X, y)

<span style="color: #658b00">print</span>(<span style="color: #CD5555">f&quot;True beta: {</span>true_beta<span style="color: #CD5555">}&quot;</span>)
<span style="color: #658b00">print</span>(<span style="color: #CD5555">f&quot;Fitted beta: {</span>beta<span style="color: #CD5555">}&quot;</span>)
<span style="color: #658b00">print</span>(<span style="color: #CD5555">f&quot;Sklearn fitted beta: {</span>clf.coef_<span style="color: #CD5555">}&quot;</span>)


plt.figure()
plt.scatter(x, y, label=<span style="color: #CD5555">&quot;Data&quot;</span>)
plt.plot(x, X @ beta, label=<span style="color: #CD5555">&quot;Fit&quot;</span>)
plt.plot(x, clf.predict(X), label=<span style="color: #CD5555">&quot;Sklearn (fit_intercept=False)&quot;</span>)


<span style="color: #228B22"># Do not include the intercept in the design matrix</span>
X = np.zeros((<span style="color: #658b00">len</span>(x), degree - <span style="color: #B452CD">1</span>))

<span style="color: #8B008B; font-weight: bold">for</span> p <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(degree - <span style="color: #B452CD">1</span>):
    X[:, p] = x ** (p + <span style="color: #B452CD">1</span>)

<span style="color: #228B22"># Intercept is not included in the design matrix</span>
clf = LinearRegression(fit_intercept=<span style="color: #8B008B; font-weight: bold">True</span>).fit(X, y)

<span style="color: #228B22"># Use centered values for X and y when computing coefficients</span>
y_offset = np.average(y, axis=<span style="color: #B452CD">0</span>)
X_offset = np.average(X, axis=<span style="color: #B452CD">0</span>)

beta = fit_beta(X - X_offset, y - y_offset)
intercept = np.mean(y_offset - X_offset @ beta)

<span style="color: #658b00">print</span>(<span style="color: #CD5555">f&quot;Manual intercept: {</span>intercept<span style="color: #CD5555">}&quot;</span>)
<span style="color: #658b00">print</span>(<span style="color: #CD5555">f&quot;Fitted beta (sans intercept): {</span>beta<span style="color: #CD5555">}&quot;</span>)
<span style="color: #658b00">print</span>(<span style="color: #CD5555">f&quot;Sklearn intercept: {</span>clf.intercept_<span style="color: #CD5555">}&quot;</span>)
<span style="color: #658b00">print</span>(<span style="color: #CD5555">f&quot;Sklearn fitted beta (sans intercept): {</span>clf.coef_<span style="color: #CD5555">}&quot;</span>)

plt.plot(x, X @ beta + intercept, <span style="color: #CD5555">&quot;--&quot;</span>, label=<span style="color: #CD5555">&quot;Fit (manual intercept)&quot;</span>)
plt.plot(x, clf.predict(X), <span style="color: #CD5555">&quot;--&quot;</span>, label=<span style="color: #CD5555">&quot;Sklearn (fit_intercept=True)&quot;</span>)
plt.grid()
plt.legend()

plt.show()
</pre></div>
</section>


<section>
<h2 id="more-complicated-example-the-ising-model">More complicated Example: The Ising model </h2>

<p>
The one-dimensional Ising model with nearest neighbor interaction, no
external field and a constant coupling constant \( J \) is given by

<p>&nbsp;<br>
$$
\begin{align}
    H = -J \sum_{k}^L s_k s_{k + 1},
\tag{1}
\end{align}
$$
<p>&nbsp;<br>

<p>
where \( s_i \in \{-1, 1\} \) and \( s_{N + 1} = s_1 \). The number of spins
in the system is determined by \( L \). For the one-dimensional system
there is no phase transition.

<p>
We will look at a system of \( L = 40 \) spins with a coupling constant of
\( J = 1 \). To get enough training data we will generate 10000 states
with their respective energies.

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">mpl_toolkits.axes_grid1</span> <span style="color: #8B008B; font-weight: bold">import</span> make_axes_locatable
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">seaborn</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">sns</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">scipy.linalg</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">scl</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.model_selection</span> <span style="color: #8B008B; font-weight: bold">import</span> train_test_split
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">tqdm</span>
sns.set(color_codes=<span style="color: #8B008B; font-weight: bold">True</span>)
cmap_args=<span style="color: #658b00">dict</span>(vmin=-<span style="color: #B452CD">1.</span>, vmax=<span style="color: #B452CD">1.</span>, cmap=<span style="color: #CD5555">&#39;seismic&#39;</span>)

L = <span style="color: #B452CD">40</span>
n = <span style="color: #658b00">int</span>(<span style="color: #B452CD">1e4</span>)

spins = np.random.choice([-<span style="color: #B452CD">1</span>, <span style="color: #B452CD">1</span>], size=(n, L))
J = <span style="color: #B452CD">1.0</span>

energies = np.zeros(n)

<span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(n):
    energies[i] = - J * np.dot(spins[i], np.roll(spins[i], <span style="color: #B452CD">1</span>))
</pre></div>
<p>
Here we use ordinary least squares
regression to predict the energy for the nearest neighbor
one-dimensional Ising model on a ring, i.e., the endpoints wrap
around. We will use linear regression to fit a value for
the coupling constant to achieve this.
</section>


<section>
<h2 id="reformulating-the-problem-to-suit-regression">Reformulating the problem to suit regression </h2>

<p>
A more general form for the one-dimensional Ising model is

<p>&nbsp;<br>
$$
\begin{align}
    H = - \sum_j^L \sum_k^L s_j s_k J_{jk}.
\tag{2}
\end{align}
$$
<p>&nbsp;<br>

<p>
Here we allow for interactions beyond the nearest neighbors and a state dependent
coupling constant. This latter expression can be formulated as
a matrix-product
<p>&nbsp;<br>
$$
\begin{align}
    \boldsymbol{H} = \boldsymbol{X} J,
\tag{3}
\end{align}
$$
<p>&nbsp;<br>

<p>
where \( X_{jk} = s_j s_k \) and \( J \) is a matrix which consists of the
elements \( -J_{jk} \). This form of writing the energy fits perfectly
with the form utilized in linear regression, that is

<p>&nbsp;<br>
$$
\begin{align}
    \boldsymbol{y} = \boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\epsilon},
\tag{4}
\end{align}
$$
<p>&nbsp;<br>

<p>
We split the data in training and test data as discussed in the previous example

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span>X = np.zeros((n, L ** <span style="color: #B452CD">2</span>))
<span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(n):
    X[i] = np.outer(spins[i], spins[i]).ravel()
y = energies
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span style="color: #B452CD">0.2</span>)
</pre></div>
</section>


<section>
<h2 id="linear-regression">Linear regression </h2>

<p>
In the ordinary least squares method we choose the cost function

<p>&nbsp;<br>
$$
\begin{align}
    C(\boldsymbol{X}, \boldsymbol{\beta})= \frac{1}{n}\left\{(\boldsymbol{X}\boldsymbol{\beta} - \boldsymbol{y})^T(\boldsymbol{X}\boldsymbol{\beta} - \boldsymbol{y})\right\}.
\tag{5}
\end{align}
$$
<p>&nbsp;<br>

<p>
We then find the extremal point of \( C \) by taking the derivative with respect to \( \boldsymbol{\beta} \) as discussed above.
This yields the expression for \( \boldsymbol{\beta} \) to be

<p>&nbsp;<br>
$$
    \boldsymbol{\beta} = \frac{\boldsymbol{X}^T \boldsymbol{y}}{\boldsymbol{X}^T \boldsymbol{X}},
$$
<p>&nbsp;<br>

<p>
which immediately imposes some requirements on \( \boldsymbol{X} \) as there must exist
an inverse of \( \boldsymbol{X}^T \boldsymbol{X} \). If the expression we are modeling contains an
intercept, i.e., a constant term, we must make sure that the
first column of \( \boldsymbol{X} \) consists of \( 1 \). We do this here

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span>X_train_own = np.concatenate(
    (np.ones(<span style="color: #658b00">len</span>(X_train))[:, np.newaxis], X_train),
    axis=<span style="color: #B452CD">1</span>
)
X_test_own = np.concatenate(
    (np.ones(<span style="color: #658b00">len</span>(X_test))[:, np.newaxis], X_test),
    axis=<span style="color: #B452CD">1</span>
)
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span><span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">ols_inv</span>(x: np.ndarray, y: np.ndarray) -&gt; np.ndarray:
    <span style="color: #8B008B; font-weight: bold">return</span> scl.inv(x.T @ x) @ (x.T @ y)
beta = ols_inv(X_train_own, y_train)
</pre></div>
</section>


<section>
<h2 id="singular-value-decomposition">Singular Value decomposition </h2>

<p>
Doing the inversion directly turns out to be a bad idea since the matrix
\( \boldsymbol{X}^T\boldsymbol{X} \) is singular. An alternative approach is to use the <b>singular
value decomposition</b>. Using the definition of the Moore-Penrose
pseudoinverse we can write the equation for \( \boldsymbol{\beta} \) as

<p>&nbsp;<br>
$$
    \boldsymbol{\beta} = \boldsymbol{X}^{+}\boldsymbol{y},
$$
<p>&nbsp;<br>

<p>
where the pseudoinverse of \( \boldsymbol{X} \) is given by

<p>&nbsp;<br>
$$
    \boldsymbol{X}^{+} = \frac{\boldsymbol{X}^T}{\boldsymbol{X}^T\boldsymbol{X}}.
$$
<p>&nbsp;<br>

<p>
Using singular value decomposition we can decompose the matrix  \( \boldsymbol{X} = \boldsymbol{U}\boldsymbol{\Sigma} \boldsymbol{V}^T \),
where \( \boldsymbol{U} \) and \( \boldsymbol{V} \) are orthogonal(unitary) matrices and \( \boldsymbol{\Sigma} \) contains the singular values (more details below).
where \( X^{+} = V\Sigma^{+} U^T \). This reduces the equation for
\( \omega \) to
<p>&nbsp;<br>
$$
\begin{align}
    \boldsymbol{\beta} = \boldsymbol{V}\boldsymbol{\Sigma}^{+} \boldsymbol{U}^T \boldsymbol{y}.
\tag{6}
\end{align}
$$
<p>&nbsp;<br>

<p>
Note that solving this equation by actually doing the pseudoinverse
(which is what we will do) is not a good idea as this operation scales
as \( \mathcal{O}(n^3) \), where \( n \) is the number of elements in a
general matrix. Instead, doing \( QR \)-factorization and solving the
linear system as an equation would reduce this down to
\( \mathcal{O}(n^2) \) operations.

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span><span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">ols_svd</span>(x: np.ndarray, y: np.ndarray) -&gt; np.ndarray:
    u, s, v = scl.svd(x)
    <span style="color: #8B008B; font-weight: bold">return</span> v.T @ scl.pinv(scl.diagsvd(s, u.shape[<span style="color: #B452CD">0</span>], v.shape[<span style="color: #B452CD">0</span>])) @ u.T @ y
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span>beta = ols_svd(X_train_own,y_train)
</pre></div>
<p>
When extracting the \( J \)-matrix  we need to make sure that we remove the intercept, as is done here

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span>J = beta[<span style="color: #B452CD">1</span>:].reshape(L, L)
</pre></div>
<p>
A way of looking at the coefficients in \( J \) is to plot the matrices as images.

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span>fig = plt.figure(figsize=(<span style="color: #B452CD">20</span>, <span style="color: #B452CD">14</span>))
im = plt.imshow(J, **cmap_args)
plt.title(<span style="color: #CD5555">&quot;OLS&quot;</span>, fontsize=<span style="color: #B452CD">18</span>)
plt.xticks(fontsize=<span style="color: #B452CD">18</span>)
plt.yticks(fontsize=<span style="color: #B452CD">18</span>)
cb = fig.colorbar(im)
cb.ax.set_yticklabels(cb.ax.get_yticklabels(), fontsize=<span style="color: #B452CD">18</span>)
plt.show()
</pre></div>
<p>
It is interesting to note that OLS
considers both \( J_{j, j + 1} = -0.5 \) and \( J_{j, j - 1} = -0.5 \) as
valid matrix elements for \( J \).
In our discussion below on hyperparameters and Ridge and Lasso regression we will see that
this problem can be removed, partly and only with Lasso regression.

<p>
In this case our matrix inversion was actually possible. The obvious question now is what is the mathematics behind the SVD?
</section>


<section>
<h2 id="the-one-dimensional-ising-model">The one-dimensional Ising model </h2>

<p>
Let us bring back the Ising model again, but now with an additional
focus on Ridge and Lasso regression as well. We repeat some of the
basic parts of the Ising model and the setup of the training and test
data.  The one-dimensional Ising model with nearest neighbor
interaction, no external field and a constant coupling constant \( J \) is
given by

<p>&nbsp;<br>
$$
\begin{align}
    H = -J \sum_{k}^L s_k s_{k + 1},
\tag{7}
\end{align}
$$
<p>&nbsp;<br>

where \( s_i \in \{-1, 1\} \) and \( s_{N + 1} = s_1 \). The number of spins in the system is determined by \( L \). For the one-dimensional system there is no phase transition.

<p>
We will look at a system of \( L = 40 \) spins with a coupling constant of \( J = 1 \). To get enough training data we will generate 10000 states with their respective energies.

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">mpl_toolkits.axes_grid1</span> <span style="color: #8B008B; font-weight: bold">import</span> make_axes_locatable
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">seaborn</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">sns</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">scipy.linalg</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">scl</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.model_selection</span> <span style="color: #8B008B; font-weight: bold">import</span> train_test_split
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">sklearn.linear_model</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">skl</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">tqdm</span>
sns.set(color_codes=<span style="color: #8B008B; font-weight: bold">True</span>)
cmap_args=<span style="color: #658b00">dict</span>(vmin=-<span style="color: #B452CD">1.</span>, vmax=<span style="color: #B452CD">1.</span>, cmap=<span style="color: #CD5555">&#39;seismic&#39;</span>)

L = <span style="color: #B452CD">40</span>
n = <span style="color: #658b00">int</span>(<span style="color: #B452CD">1e4</span>)

spins = np.random.choice([-<span style="color: #B452CD">1</span>, <span style="color: #B452CD">1</span>], size=(n, L))
J = <span style="color: #B452CD">1.0</span>

energies = np.zeros(n)

<span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(n):
    energies[i] = - J * np.dot(spins[i], np.roll(spins[i], <span style="color: #B452CD">1</span>))
</pre></div>
<p>
A more general form for the one-dimensional Ising model is

<p>&nbsp;<br>
$$
\begin{align}
    H = - \sum_j^L \sum_k^L s_j s_k J_{jk}.
\tag{8}
\end{align}
$$
<p>&nbsp;<br>

<p>
Here we allow for interactions beyond the nearest neighbors and a more
adaptive coupling matrix. This latter expression can be formulated as
a matrix-product on the form
<p>&nbsp;<br>
$$
\begin{align}
    H = X J,
\tag{9}
\end{align}
$$
<p>&nbsp;<br>

<p>
where \( X_{jk} = s_j s_k \) and \( J \) is the matrix consisting of the
elements \( -J_{jk} \). This form of writing the energy fits perfectly
with the form utilized in linear regression, viz.
<p>&nbsp;<br>
$$
\begin{align}
    \boldsymbol{y} = \boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}.
\tag{10}
\end{align}
$$
<p>&nbsp;<br>

We organize the data as we did above
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span>X = np.zeros((n, L ** <span style="color: #B452CD">2</span>))
<span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(n):
    X[i] = np.outer(spins[i], spins[i]).ravel()
y = energies
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span style="color: #B452CD">0.96</span>)

X_train_own = np.concatenate(
    (np.ones(<span style="color: #658b00">len</span>(X_train))[:, np.newaxis], X_train),
    axis=<span style="color: #B452CD">1</span>
)

X_test_own = np.concatenate(
    (np.ones(<span style="color: #658b00">len</span>(X_test))[:, np.newaxis], X_test),
    axis=<span style="color: #B452CD">1</span>
)
</pre></div>
<p>
We will do all fitting with <b>Scikit-Learn</b>,

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span>clf = skl.LinearRegression().fit(X_train, y_train)
</pre></div>
<p>
When  extracting the \( J \)-matrix we make sure to remove the intercept
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span>J_sk = clf.coef_.reshape(L, L)
</pre></div>
<p>
And then we plot the results
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span>fig = plt.figure(figsize=(<span style="color: #B452CD">20</span>, <span style="color: #B452CD">14</span>))
im = plt.imshow(J_sk, **cmap_args)
plt.title(<span style="color: #CD5555">&quot;LinearRegression from Scikit-learn&quot;</span>, fontsize=<span style="color: #B452CD">18</span>)
plt.xticks(fontsize=<span style="color: #B452CD">18</span>)
plt.yticks(fontsize=<span style="color: #B452CD">18</span>)
cb = fig.colorbar(im)
cb.ax.set_yticklabels(cb.ax.get_yticklabels(), fontsize=<span style="color: #B452CD">18</span>)
plt.show()
</pre></div>
<p>
The results perfectly with our previous discussion where we used our own code.
</section>


<section>
<h2 id="ridge-regression">Ridge regression </h2>

<p>
Having explored the ordinary least squares we move on to ridge
regression. In ridge regression we include a <b>regularizer</b>. This
involves a new cost function which leads to a new estimate for the
weights \( \boldsymbol{\beta} \). This results in a penalized regression problem. The
cost function is given by

<p>&nbsp;<br>
$$
\begin{align}
    C(\boldsymbol{X}, \boldsymbol{\beta}; \lambda) = (\boldsymbol{X}\boldsymbol{\beta} - \boldsymbol{y})^T(\boldsymbol{X}\boldsymbol{\beta} - \boldsymbol{y}) + \lambda \boldsymbol{\beta}^T\boldsymbol{\beta}.
\tag{11}
\end{align}
$$
<p>&nbsp;<br>

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span>_lambda = <span style="color: #B452CD">0.1</span>
clf_ridge = skl.Ridge(alpha=_lambda).fit(X_train, y_train)
J_ridge_sk = clf_ridge.coef_.reshape(L, L)
fig = plt.figure(figsize=(<span style="color: #B452CD">20</span>, <span style="color: #B452CD">14</span>))
im = plt.imshow(J_ridge_sk, **cmap_args)
plt.title(<span style="color: #CD5555">&quot;Ridge from Scikit-learn&quot;</span>, fontsize=<span style="color: #B452CD">18</span>)
plt.xticks(fontsize=<span style="color: #B452CD">18</span>)
plt.yticks(fontsize=<span style="color: #B452CD">18</span>)
cb = fig.colorbar(im)
cb.ax.set_yticklabels(cb.ax.get_yticklabels(), fontsize=<span style="color: #B452CD">18</span>)

plt.show()
</pre></div>
</section>


<section>
<h2 id="lasso-regression">LASSO regression </h2>

<p>
In the <b>Least Absolute Shrinkage and Selection Operator</b> (LASSO)-method we get a third cost function.

<p>&nbsp;<br>
$$
\begin{align}
    C(\boldsymbol{X}, \boldsymbol{\beta}; \lambda) = (\boldsymbol{X}\boldsymbol{\beta} - \boldsymbol{y})^T(\boldsymbol{X}\boldsymbol{\beta} - \boldsymbol{y}) + \lambda \sqrt{\boldsymbol{\beta}^T\boldsymbol{\beta}}.
\tag{12}
\end{align}
$$
<p>&nbsp;<br>

<p>
Finding the extremal point of this cost function is not so straight-forward as in least squares and ridge. We will therefore rely solely on the function ``Lasso`` from <b>Scikit-Learn</b>.

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span>clf_lasso = skl.Lasso(alpha=_lambda).fit(X_train, y_train)
J_lasso_sk = clf_lasso.coef_.reshape(L, L)
fig = plt.figure(figsize=(<span style="color: #B452CD">20</span>, <span style="color: #B452CD">14</span>))
im = plt.imshow(J_lasso_sk, **cmap_args)
plt.title(<span style="color: #CD5555">&quot;Lasso from Scikit-learn&quot;</span>, fontsize=<span style="color: #B452CD">18</span>)
plt.xticks(fontsize=<span style="color: #B452CD">18</span>)
plt.yticks(fontsize=<span style="color: #B452CD">18</span>)
cb = fig.colorbar(im)
cb.ax.set_yticklabels(cb.ax.get_yticklabels(), fontsize=<span style="color: #B452CD">18</span>)

plt.show()
</pre></div>
<p>
It is quite striking how LASSO breaks the symmetry of the coupling
constant as opposed to ridge and OLS. We get a sparse solution with
\( J_{j, j + 1} = -1 \).
</section>


<section>
<h2 id="performance-as-function-of-the-regularization-parameter">Performance as  function of the regularization parameter </h2>

<p>
We see how the different models perform for a different set of values for \( \lambda \).

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span>lambdas = np.logspace(-<span style="color: #B452CD">4</span>, <span style="color: #B452CD">5</span>, <span style="color: #B452CD">10</span>)

train_errors = {
    <span style="color: #CD5555">&quot;ols_sk&quot;</span>: np.zeros(lambdas.size),
    <span style="color: #CD5555">&quot;ridge_sk&quot;</span>: np.zeros(lambdas.size),
    <span style="color: #CD5555">&quot;lasso_sk&quot;</span>: np.zeros(lambdas.size)
}

test_errors = {
    <span style="color: #CD5555">&quot;ols_sk&quot;</span>: np.zeros(lambdas.size),
    <span style="color: #CD5555">&quot;ridge_sk&quot;</span>: np.zeros(lambdas.size),
    <span style="color: #CD5555">&quot;lasso_sk&quot;</span>: np.zeros(lambdas.size)
}

plot_counter = <span style="color: #B452CD">1</span>

fig = plt.figure(figsize=(<span style="color: #B452CD">32</span>, <span style="color: #B452CD">54</span>))

<span style="color: #8B008B; font-weight: bold">for</span> i, _lambda <span style="color: #8B008B">in</span> <span style="color: #658b00">enumerate</span>(tqdm.tqdm(lambdas)):
    <span style="color: #8B008B; font-weight: bold">for</span> key, method <span style="color: #8B008B">in</span> <span style="color: #658b00">zip</span>(
        [<span style="color: #CD5555">&quot;ols_sk&quot;</span>, <span style="color: #CD5555">&quot;ridge_sk&quot;</span>, <span style="color: #CD5555">&quot;lasso_sk&quot;</span>],
        [skl.LinearRegression(), skl.Ridge(alpha=_lambda), skl.Lasso(alpha=_lambda)]
    ):
        method = method.fit(X_train, y_train)

        train_errors[key][i] = method.score(X_train, y_train)
        test_errors[key][i] = method.score(X_test, y_test)

        omega = method.coef_.reshape(L, L)

        plt.subplot(<span style="color: #B452CD">10</span>, <span style="color: #B452CD">5</span>, plot_counter)
        plt.imshow(omega, **cmap_args)
        plt.title(<span style="color: #CD5555">r&quot;%s, $\lambda = %.4f$&quot;</span> % (key, _lambda))
        plot_counter += <span style="color: #B452CD">1</span>

plt.show()
</pre></div>
<p>
We see that LASSO reaches a good solution for low
values of \( \lambda \), but will "wither" when we increase \( \lambda \) too
much. Ridge is more stable over a larger range of values for
\( \lambda \), but eventually also fades away.
</section>


<section>
<h2 id="finding-the-optimal-value-of-lambda">Finding the optimal value of \( \lambda \) </h2>

<p>
To determine which value of \( \lambda \) is best we plot the accuracy of
the models when predicting the training and the testing set. We expect
the accuracy of the training set to be quite good, but if the accuracy
of the testing set is much lower this tells us that we might be
subject to an overfit model. The ideal scenario is an accuracy on the
testing set that is close to the accuracy of the training set.

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span>fig = plt.figure(figsize=(<span style="color: #B452CD">20</span>, <span style="color: #B452CD">14</span>))

colors = {
    <span style="color: #CD5555">&quot;ols_sk&quot;</span>: <span style="color: #CD5555">&quot;r&quot;</span>,
    <span style="color: #CD5555">&quot;ridge_sk&quot;</span>: <span style="color: #CD5555">&quot;y&quot;</span>,
    <span style="color: #CD5555">&quot;lasso_sk&quot;</span>: <span style="color: #CD5555">&quot;c&quot;</span>
}

<span style="color: #8B008B; font-weight: bold">for</span> key <span style="color: #8B008B">in</span> train_errors:
    plt.semilogx(
        lambdas,
        train_errors[key],
        colors[key],
        label=<span style="color: #CD5555">&quot;Train {0}&quot;</span>.format(key),
        linewidth=<span style="color: #B452CD">4.0</span>
    )

<span style="color: #8B008B; font-weight: bold">for</span> key <span style="color: #8B008B">in</span> test_errors:
    plt.semilogx(
        lambdas,
        test_errors[key],
        colors[key] + <span style="color: #CD5555">&quot;--&quot;</span>,
        label=<span style="color: #CD5555">&quot;Test {0}&quot;</span>.format(key),
        linewidth=<span style="color: #B452CD">4.0</span>
    )
plt.legend(loc=<span style="color: #CD5555">&quot;best&quot;</span>, fontsize=<span style="color: #B452CD">18</span>)
plt.xlabel(<span style="color: #CD5555">r&quot;$\lambda$&quot;</span>, fontsize=<span style="color: #B452CD">18</span>)
plt.ylabel(<span style="color: #CD5555">r&quot;$R^2$&quot;</span>, fontsize=<span style="color: #B452CD">18</span>)
plt.tick_params(labelsize=<span style="color: #B452CD">18</span>)
plt.show()
</pre></div>
<p>
From the above figure we can see that LASSO with \( \lambda = 10^{-2} \)
achieves a very good accuracy on the test set. This by far surpasses the
other models for all values of \( \lambda \).
</section>


<section>
<h2 id="logistic-regression">Logistic Regression </h2>

<p>
In linear regression our main interest was centered on learning the
coefficients of a functional fit (say a polynomial) in order to be
able to predict the response of a continuous variable on some unseen
data. The fit to the continuous variable \( y_i \) is based on some
independent variables \( \hat{x}_i \). Linear regression resulted in
analytical expressions for standard ordinary Least Squares or Ridge
regression (in terms of matrices to invert) for several quantities,
ranging from the variance and thereby the confidence intervals of the
parameters \( \hat{\beta} \) to the mean squared error. If we can invert
the product of the design matrices, linear regression gives then a
simple recipe for fitting our data.
</section>


<section>
<h2 id="classification-problems">Classification problems </h2>

<p>
Classification problems, however, are concerned with outcomes taking
the form of discrete variables (i.e. categories). We may for example,
on the basis of DNA sequencing for a number of patients, like to find
out which mutations are important for a certain disease; or based on
scans of various patients' brains, figure out if there is a tumor or
not; or given a specific physical system, we'd like to identify its
state, say whether it is an ordered or disordered system (typical
situation in solid state physics); or classify the status of a
patient, whether she/he has a stroke or not and many other similar
situations.

<p>
The most common situation we encounter when we apply logistic
regression is that of two possible outcomes, normally denoted as a
binary outcome, true or false, positive or negative, success or
failure etc.
</section>


<section>
<h2 id="optimization-and-deep-learning">Optimization and Deep learning </h2>

<p>
Logistic regression will also serve as our stepping stone towards
neural network algorithms and supervised deep learning. For logistic
learning, the minimization of the cost function leads to a non-linear
equation in the parameters \( \hat{\beta} \). The optimization of the
problem calls therefore for minimization algorithms. This forms the
bottle neck of all machine learning algorithms, namely how to find
reliable minima of a multi-variable function. This leads us to the
family of gradient descent methods. The latter are the working horses
of basically all modern machine learning algorithms.

<p>
We note also that many of the topics discussed here on logistic 
regression are also commonly used in modern supervised Deep Learning
models, as we will see later.
</section>


<section>
<h2 id="basics">Basics </h2>

<p>
We consider the case where the dependent variables, also called the
responses or the outcomes, \( y_i \) are discrete and only take values
from \( k=0,\dots,K-1 \) (i.e. \( K \) classes).

<p>
The goal is to predict the
output classes from the design matrix \( \hat{X}\in\mathbb{R}^{n\times p} \)
made of \( n \) samples, each of which carries \( p \) features or predictors. The
primary goal is to identify the classes to which new unseen samples
belong.

<p>
Let us specialize to the case of two classes only, with outputs
\( y_i=0 \) and \( y_i=1 \). Our outcomes could represent the status of a
credit card user that could default or not on her/his credit card
debt. That is

<p>&nbsp;<br>
$$
y_i = \begin{bmatrix} 0 & \mathrm{no}\\  1 & \mathrm{yes} \end{bmatrix}.
$$
<p>&nbsp;<br>
</section>


<section>
<h2 id="linear-classifier">Linear classifier </h2>

<p>
Before moving to the logistic model, let us try to use our linear
regression model to classify these two outcomes. We could for example
fit a linear model to the default case if \( y_i > 0.5 \) and the no
default case \( y_i \leq 0.5 \).

<p>
We would then have our 
weighted linear combination, namely 
<p>&nbsp;<br>
$$
\begin{equation}
\hat{y} = \hat{X}^T\hat{\beta} +  \hat{\epsilon},
\tag{13}
\end{equation}
$$
<p>&nbsp;<br>

where \( \hat{y} \) is a vector representing the possible outcomes, \( \hat{X} \) is our
\( n\times p \) design matrix and \( \hat{\beta} \) represents our estimators/predictors.
</section>


<section>
<h2 id="some-selected-properties">Some selected properties </h2>

<p>
The main problem with our function is that it takes values on the
entire real axis. In the case of logistic regression, however, the
labels \( y_i \) are discrete variables. A typical example is the credit
card data discussed below here, where we can set the state of
defaulting the debt to \( y_i=1 \) and not to \( y_i=0 \) for one the persons
in the data set (see the full example below).

<p>
One simple way to get a discrete output is to have sign
functions that map the output of a linear regressor to values \( \{0,1\} \),
\( f(s_i)=sign(s_i)=1 \) if \( s_i\ge 0 \) and 0 if otherwise. 
We will encounter this model in our first demonstration of neural networks. Historically it is called the ``perceptron" model in the machine learning
literature. This model is extremely simple. However, in many cases it is more
favorable to use a ``soft" classifier that outputs
the probability of a given category. This leads us to the logistic function.
</section>


<section>
<h2 id="simple-example">Simple example </h2>

<p>
The following example on data for coronary heart disease (CHD) as function of age may serve as an illustration. In the code here we read and plot whether a person has had CHD (output = 1) or not (output = 0). This ouput  is plotted the person's against age. Clearly, the figure shows that attempting to make a standard linear regression fit may not be very meaningful.

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span><span style="color: #228B22"># Common imports</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">os</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">pandas</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">pd</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.linear_model</span> <span style="color: #8B008B; font-weight: bold">import</span> LinearRegression, Ridge, Lasso
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.model_selection</span> <span style="color: #8B008B; font-weight: bold">import</span> train_test_split
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.utils</span> <span style="color: #8B008B; font-weight: bold">import</span> resample
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.metrics</span> <span style="color: #8B008B; font-weight: bold">import</span> mean_squared_error
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">IPython.display</span> <span style="color: #8B008B; font-weight: bold">import</span> display
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">pylab</span> <span style="color: #8B008B; font-weight: bold">import</span> plt, mpl
plt.style.use(<span style="color: #CD5555">&#39;seaborn&#39;</span>)
mpl.rcParams[<span style="color: #CD5555">&#39;font.family&#39;</span>] = <span style="color: #CD5555">&#39;serif&#39;</span>

<span style="color: #228B22"># Where to save the figures and data files</span>
PROJECT_ROOT_DIR = <span style="color: #CD5555">&quot;Results&quot;</span>
FIGURE_ID = <span style="color: #CD5555">&quot;Results/FigureFiles&quot;</span>
DATA_ID = <span style="color: #CD5555">&quot;DataFiles/&quot;</span>

<span style="color: #8B008B; font-weight: bold">if</span> <span style="color: #8B008B">not</span> os.path.exists(PROJECT_ROOT_DIR):
    os.mkdir(PROJECT_ROOT_DIR)

<span style="color: #8B008B; font-weight: bold">if</span> <span style="color: #8B008B">not</span> os.path.exists(FIGURE_ID):
    os.makedirs(FIGURE_ID)

<span style="color: #8B008B; font-weight: bold">if</span> <span style="color: #8B008B">not</span> os.path.exists(DATA_ID):
    os.makedirs(DATA_ID)

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">image_path</span>(fig_id):
    <span style="color: #8B008B; font-weight: bold">return</span> os.path.join(FIGURE_ID, fig_id)

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">data_path</span>(dat_id):
    <span style="color: #8B008B; font-weight: bold">return</span> os.path.join(DATA_ID, dat_id)

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">save_fig</span>(fig_id):
    plt.savefig(image_path(fig_id) + <span style="color: #CD5555">&quot;.png&quot;</span>, <span style="color: #658b00">format</span>=<span style="color: #CD5555">&#39;png&#39;</span>)

infile = <span style="color: #658b00">open</span>(data_path(<span style="color: #CD5555">&quot;chddata.csv&quot;</span>),<span style="color: #CD5555">&#39;r&#39;</span>)

<span style="color: #228B22"># Read the chd data as  csv file and organize the data into arrays with age group, age, and chd</span>
chd = pd.read_csv(infile, names=(<span style="color: #CD5555">&#39;ID&#39;</span>, <span style="color: #CD5555">&#39;Age&#39;</span>, <span style="color: #CD5555">&#39;Agegroup&#39;</span>, <span style="color: #CD5555">&#39;CHD&#39;</span>))
chd.columns = [<span style="color: #CD5555">&#39;ID&#39;</span>, <span style="color: #CD5555">&#39;Age&#39;</span>, <span style="color: #CD5555">&#39;Agegroup&#39;</span>, <span style="color: #CD5555">&#39;CHD&#39;</span>]
output = chd[<span style="color: #CD5555">&#39;CHD&#39;</span>]
age = chd[<span style="color: #CD5555">&#39;Age&#39;</span>]
agegroup = chd[<span style="color: #CD5555">&#39;Agegroup&#39;</span>]
numberID  = chd[<span style="color: #CD5555">&#39;ID&#39;</span>] 
display(chd)

plt.scatter(age, output, marker=<span style="color: #CD5555">&#39;o&#39;</span>)
plt.axis([<span style="color: #B452CD">18</span>,<span style="color: #B452CD">70.0</span>,-<span style="color: #B452CD">0.1</span>, <span style="color: #B452CD">1.2</span>])
plt.xlabel(<span style="color: #CD5555">r&#39;Age&#39;</span>)
plt.ylabel(<span style="color: #CD5555">r&#39;CHD&#39;</span>)
plt.title(<span style="color: #CD5555">r&#39;Age distribution and Coronary heart disease&#39;</span>)
plt.show()
</pre></div>
</section>


<section>
<h2 id="plotting-the-mean-value-for-each-group">Plotting the mean value for each group </h2>

<p>
What we could attempt however is to plot the mean value for each group.

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span>agegroupmean = np.array([<span style="color: #B452CD">0.1</span>, <span style="color: #B452CD">0.133</span>, <span style="color: #B452CD">0.250</span>, <span style="color: #B452CD">0.333</span>, <span style="color: #B452CD">0.462</span>, <span style="color: #B452CD">0.625</span>, <span style="color: #B452CD">0.765</span>, <span style="color: #B452CD">0.800</span>])
group = np.array([<span style="color: #B452CD">1</span>, <span style="color: #B452CD">2</span>, <span style="color: #B452CD">3</span>, <span style="color: #B452CD">4</span>, <span style="color: #B452CD">5</span>, <span style="color: #B452CD">6</span>, <span style="color: #B452CD">7</span>, <span style="color: #B452CD">8</span>])
plt.plot(group, agegroupmean, <span style="color: #CD5555">&quot;r-&quot;</span>)
plt.axis([<span style="color: #B452CD">0</span>,<span style="color: #B452CD">9</span>,<span style="color: #B452CD">0</span>, <span style="color: #B452CD">1.0</span>])
plt.xlabel(<span style="color: #CD5555">r&#39;Age group&#39;</span>)
plt.ylabel(<span style="color: #CD5555">r&#39;CHD mean values&#39;</span>)
plt.title(<span style="color: #CD5555">r&#39;Mean values for each age group&#39;</span>)
plt.show()
</pre></div>
<p>
We are now trying to find a function \( f(y\vert x) \), that is a function which gives us an expected value for the output \( y \) with a given input \( x \).
In standard linear regression with a linear dependence on \( x \), we would write this in terms of our model
<p>&nbsp;<br>
$$
f(y_i\vert x_i)=\beta_0+\beta_1 x_i.
$$
<p>&nbsp;<br>

<p>
This expression implies however that \( f(y_i\vert x_i) \) could take any
value from minus infinity to plus infinity. If we however let
\( f(y\vert y) \) be represented by the mean value, the above example
shows us that we can constrain the function to take values between
zero and one, that is we have \( 0 \le f(y_i\vert x_i) \le 1 \). Looking
at our last curve we see also that it has an S-shaped form. This leads
us to a very popular model for the function \( f \), namely the so-called
Sigmoid function or logistic model. We will consider this function as
representing the probability for finding a value of \( y_i \) with a given
\( x_i \).
</section>


<section>
<h2 id="the-logistic-function">The logistic function </h2>

<p>
Another widely studied model, is the so-called 
perceptron model, which is an example of a &quot;hard classification&quot; model. We
will encounter this model when we discuss neural networks as
well. Each datapoint is deterministically assigned to a category (i.e
\( y_i=0 \) or \( y_i=1 \)). In many cases, and the coronary heart disease data forms one of many such examples, it is favorable to have a &quot;soft&quot;
classifier that outputs the probability of a given category rather
than a single value. For example, given \( x_i \), the classifier
outputs the probability of being in a category \( k \).  Logistic regression
is the most common example of a so-called soft classifier. In logistic
regression, the probability that a data point \( x_i \)
belongs to a category \( y_i=\{0,1\} \) is given by the so-called logit function (or Sigmoid) which is meant to represent the likelihood for a given event, 
<p>&nbsp;<br>
$$
p(t) = \frac{1}{1+\mathrm \exp{-t}}=\frac{\exp{t}}{1+\mathrm \exp{t}}.
$$
<p>&nbsp;<br>

Note that \( 1-p(t)= p(-t) \).
</section>


<section>
<h2 id="examples-of-likelihood-functions-used-in-logistic-regression-and-nueral-networks">Examples of likelihood functions used in logistic regression and nueral networks </h2>

<p>
The following code plots the logistic function, the step function and other functions we will encounter from here and on.

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span><span style="color: #CD5555">&quot;&quot;&quot;The sigmoid function (or the logistic curve) is a</span>
<span style="color: #CD5555">function that takes any real number, z, and outputs a number (0,1).</span>
<span style="color: #CD5555">It is useful in neural networks for assigning weights on a relative scale.</span>
<span style="color: #CD5555">The value z is the weighted sum of parameters involved in the learning algorithm.&quot;&quot;&quot;</span>

<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">math</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">mt</span>

z = numpy.arange(-<span style="color: #B452CD">5</span>, <span style="color: #B452CD">5</span>, .<span style="color: #B452CD">1</span>)
sigma_fn = numpy.vectorize(<span style="color: #8B008B; font-weight: bold">lambda</span> z: <span style="color: #B452CD">1</span>/(<span style="color: #B452CD">1</span>+numpy.exp(-z)))
sigma = sigma_fn(z)

fig = plt.figure()
ax = fig.add_subplot(<span style="color: #B452CD">111</span>)
ax.plot(z, sigma)
ax.set_ylim([-<span style="color: #B452CD">0.1</span>, <span style="color: #B452CD">1.1</span>])
ax.set_xlim([-<span style="color: #B452CD">5</span>,<span style="color: #B452CD">5</span>])
ax.grid(<span style="color: #8B008B; font-weight: bold">True</span>)
ax.set_xlabel(<span style="color: #CD5555">&#39;z&#39;</span>)
ax.set_title(<span style="color: #CD5555">&#39;sigmoid function&#39;</span>)

plt.show()

<span style="color: #CD5555">&quot;&quot;&quot;Step Function&quot;&quot;&quot;</span>
z = numpy.arange(-<span style="color: #B452CD">5</span>, <span style="color: #B452CD">5</span>, .<span style="color: #B452CD">02</span>)
step_fn = numpy.vectorize(<span style="color: #8B008B; font-weight: bold">lambda</span> z: <span style="color: #B452CD">1.0</span> <span style="color: #8B008B; font-weight: bold">if</span> z &gt;= <span style="color: #B452CD">0.0</span> <span style="color: #8B008B; font-weight: bold">else</span> <span style="color: #B452CD">0.0</span>)
step = step_fn(z)

fig = plt.figure()
ax = fig.add_subplot(<span style="color: #B452CD">111</span>)
ax.plot(z, step)
ax.set_ylim([-<span style="color: #B452CD">0.5</span>, <span style="color: #B452CD">1.5</span>])
ax.set_xlim([-<span style="color: #B452CD">5</span>,<span style="color: #B452CD">5</span>])
ax.grid(<span style="color: #8B008B; font-weight: bold">True</span>)
ax.set_xlabel(<span style="color: #CD5555">&#39;z&#39;</span>)
ax.set_title(<span style="color: #CD5555">&#39;step function&#39;</span>)

plt.show()

<span style="color: #CD5555">&quot;&quot;&quot;tanh Function&quot;&quot;&quot;</span>
z = numpy.arange(-<span style="color: #B452CD">2</span>*mt.pi, <span style="color: #B452CD">2</span>*mt.pi, <span style="color: #B452CD">0.1</span>)
t = numpy.tanh(z)

fig = plt.figure()
ax = fig.add_subplot(<span style="color: #B452CD">111</span>)
ax.plot(z, t)
ax.set_ylim([-<span style="color: #B452CD">1.0</span>, <span style="color: #B452CD">1.0</span>])
ax.set_xlim([-<span style="color: #B452CD">2</span>*mt.pi,<span style="color: #B452CD">2</span>*mt.pi])
ax.grid(<span style="color: #8B008B; font-weight: bold">True</span>)
ax.set_xlabel(<span style="color: #CD5555">&#39;z&#39;</span>)
ax.set_title(<span style="color: #CD5555">&#39;tanh function&#39;</span>)

plt.show()
</pre></div>
</section>


<section>
<h2 id="two-parameters">Two parameters </h2>

<p>
We assume now that we have two classes with \( y_i \) either \( 0 \) or \( 1 \). Furthermore we assume also that we have only two parameters \( \beta \) in our fitting of the Sigmoid function, that is we define probabilities 
<p>&nbsp;<br>
$$
\begin{align*}
p(y_i=1|x_i,\hat{\beta}) &= \frac{\exp{(\beta_0+\beta_1x_i)}}{1+\exp{(\beta_0+\beta_1x_i)}},\nonumber\\
p(y_i=0|x_i,\hat{\beta}) &= 1 - p(y_i=1|x_i,\hat{\beta}),
\end{align*}
$$
<p>&nbsp;<br>

where \( \hat{\beta} \) are the weights we wish to extract from data, in our case \( \beta_0 \) and \( \beta_1 \).

<p>
Note that we used
<p>&nbsp;<br>
$$
p(y_i=0\vert x_i, \hat{\beta}) = 1-p(y_i=1\vert x_i, \hat{\beta}).
$$
<p>&nbsp;<br>
</section>


<section>
<h2 id="maximum-likelihood">Maximum likelihood </h2>

<p>
In order to define the total likelihood for all possible outcomes from a  
dataset \( \mathcal{D}=\{(y_i,x_i)\} \), with the binary labels
\( y_i\in\{0,1\} \) and where the data points are drawn independently, we use the so-called <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation" target="_blank">Maximum Likelihood Estimation</a> (MLE) principle. 
We aim thus at maximizing 
the probability of seeing the observed data. We can then approximate the 
likelihood in terms of the product of the individual probabilities of a specific outcome \( y_i \), that is 
<p>&nbsp;<br>
$$
\begin{align*}
P(\mathcal{D}|\hat{\beta})& = \prod_{i=1}^n \left[p(y_i=1|x_i,\hat{\beta})\right]^{y_i}\left[1-p(y_i=1|x_i,\hat{\beta}))\right]^{1-y_i}\nonumber \\
\end{align*}
$$
<p>&nbsp;<br>

from which we obtain the log-likelihood and our <b>cost/loss</b> function
<p>&nbsp;<br>
$$
\mathcal{C}(\hat{\beta}) = \sum_{i=1}^n \left( y_i\log{p(y_i=1|x_i,\hat{\beta})} + (1-y_i)\log\left[1-p(y_i=1|x_i,\hat{\beta}))\right]\right).
$$
<p>&nbsp;<br>
</section>


<section>
<h2 id="the-cost-function-rewritten">The cost function rewritten </h2>

<p>
Reordering the logarithms, we can rewrite the <b>cost/loss</b> function as
<p>&nbsp;<br>
$$
\mathcal{C}(\hat{\beta}) = \sum_{i=1}^n  \left(y_i(\beta_0+\beta_1x_i) -\log{(1+\exp{(\beta_0+\beta_1x_i)})}\right).
$$
<p>&nbsp;<br>

<p>
The maximum likelihood estimator is defined as the set of parameters that maximize the log-likelihood where we maximize with respect to \( \beta \).
Since the cost (error) function is just the negative log-likelihood, for logistic regression we have that
<p>&nbsp;<br>
$$
\mathcal{C}(\hat{\beta})=-\sum_{i=1}^n  \left(y_i(\beta_0+\beta_1x_i) -\log{(1+\exp{(\beta_0+\beta_1x_i)})}\right).
$$
<p>&nbsp;<br>

This equation is known in statistics as the <b>cross entropy</b>. Finally, we note that just as in linear regression, 
in practice we often supplement the cross-entropy with additional regularization terms, usually \( L_1 \) and \( L_2 \) regularization as we did for Ridge and Lasso regression.
</section>


<section>
<h2 id="minimizing-the-cross-entropy">Minimizing the cross entropy </h2>

<p>
The cross entropy is a convex function of the weights \( \hat{\beta} \) and,
therefore, any local minimizer is a global minimizer.

<p>
Minimizing this
cost function with respect to the two parameters \( \beta_0 \) and \( \beta_1 \) we obtain

<p>&nbsp;<br>
$$
\frac{\partial \mathcal{C}(\hat{\beta})}{\partial \beta_0} = -\sum_{i=1}^n  \left(y_i -\frac{\exp{(\beta_0+\beta_1x_i)}}{1+\exp{(\beta_0+\beta_1x_i)}}\right),
$$
<p>&nbsp;<br>

and 
<p>&nbsp;<br>
$$
\frac{\partial \mathcal{C}(\hat{\beta})}{\partial \beta_1} = -\sum_{i=1}^n  \left(y_ix_i -x_i\frac{\exp{(\beta_0+\beta_1x_i)}}{1+\exp{(\beta_0+\beta_1x_i)}}\right).
$$
<p>&nbsp;<br>
</section>


<section>
<h2 id="a-more-compact-expression">A more compact expression </h2>

<p>
Let us now define a vector \( \hat{y} \) with \( n \) elements \( y_i \), an
\( n\times p \) matrix \( \hat{X} \) which contains the \( x_i \) values and a
vector \( \hat{p} \) of fitted probabilities \( p(y_i\vert x_i,\hat{\beta}) \). We can rewrite in a more compact form the first
derivative of cost function as

<p>&nbsp;<br>
$$
\frac{\partial \mathcal{C}(\hat{\beta})}{\partial \hat{\beta}} = -\hat{X}^T\left(\hat{y}-\hat{p}\right). 
$$
<p>&nbsp;<br>

<p>
If we in addition define a diagonal matrix \( \hat{W} \) with elements 
\( p(y_i\vert x_i,\hat{\beta})(1-p(y_i\vert x_i,\hat{\beta}) \), we can obtain a compact expression of the second derivative as

<p>&nbsp;<br>
$$
\frac{\partial^2 \mathcal{C}(\hat{\beta})}{\partial \hat{\beta}\partial \hat{\beta}^T} = \hat{X}^T\hat{W}\hat{X}. 
$$
<p>&nbsp;<br>
</section>


<section>
<h2 id="extending-to-more-predictors">Extending to more predictors </h2>

<p>
Within a binary classification problem, we can easily expand our model to include multiple predictors. Our ratio between likelihoods is then with \( p \) predictors
<p>&nbsp;<br>
$$
\log{ \frac{p(\hat{\beta}\hat{x})}{1-p(\hat{\beta}\hat{x})}} = \beta_0+\beta_1x_1+\beta_2x_2+\dots+\beta_px_p.
$$
<p>&nbsp;<br>

Here we defined \( \hat{x}=[1,x_1,x_2,\dots,x_p] \) and \( \hat{\beta}=[\beta_0, \beta_1, \dots, \beta_p] \) leading to
<p>&nbsp;<br>
$$
p(\hat{\beta}\hat{x})=\frac{ \exp{(\beta_0+\beta_1x_1+\beta_2x_2+\dots+\beta_px_p)}}{1+\exp{(\beta_0+\beta_1x_1+\beta_2x_2+\dots+\beta_px_p)}}.
$$
<p>&nbsp;<br>
</section>


<section>
<h2 id="including-more-classes">Including more classes </h2>

<p>
Till now we have mainly focused on two classes, the so-called binary
system. Suppose we wish to extend to \( K \) classes.  Let us for the sake
of simplicity assume we have only two predictors. We have then following model

<p>&nbsp;<br>
$$
\log{\frac{p(C=1\vert x)}{p(K\vert x)}} = \beta_{10}+\beta_{11}x_1,
$$
<p>&nbsp;<br>

and 
<p>&nbsp;<br>
$$
\log{\frac{p(C=2\vert x)}{p(K\vert x)}} = \beta_{20}+\beta_{21}x_1,
$$
<p>&nbsp;<br>

and so on till the class \( C=K-1 \) class
<p>&nbsp;<br>
$$
\log{\frac{p(C=K-1\vert x)}{p(K\vert x)}} = \beta_{(K-1)0}+\beta_{(K-1)1}x_1,
$$
<p>&nbsp;<br>

<p>
and the model is specified in term of \( K-1 \) so-called log-odds or
<b>logit</b> transformations.
</section>


<section>
<h2 id="more-classes">More classes </h2>

<p>
In our discussion of neural networks we will encounter the above again
in terms of a slightly modified function, the so-called <b>Softmax</b> function.

<p>
The softmax function is used in various multiclass classification
methods, such as multinomial logistic regression (also known as
softmax regression), multiclass linear discriminant analysis, naive
Bayes classifiers, and artificial neural networks.  Specifically, in
multinomial logistic regression and linear discriminant analysis, the
input to the function is the result of \( K \) distinct linear functions,
and the predicted probability for the \( k \)-th class given a sample
vector \( \hat{x} \) and a weighting vector \( \hat{\beta} \) is (with two
predictors):

<p>&nbsp;<br>
$$
p(C=k\vert \mathbf {x} )=\frac{\exp{(\beta_{k0}+\beta_{k1}x_1)}}{1+\sum_{l=1}^{K-1}\exp{(\beta_{l0}+\beta_{l1}x_1)}}.
$$
<p>&nbsp;<br>

It is easy to extend to more predictors. The final class is 
<p>&nbsp;<br>
$$
p(C=K\vert \mathbf {x} )=\frac{1}{1+\sum_{l=1}^{K-1}\exp{(\beta_{l0}+\beta_{l1}x_1)}},
$$
<p>&nbsp;<br>

<p>
and they sum to one. Our earlier discussions were all specialized to
the case with two classes only. It is easy to see from the above that
what we derived earlier is compatible with these equations.

<p>
To find the optimal parameters we would typically use a gradient
descent method.  Newton's method and gradient descent methods are
discussed in the material on <a href="https://compphysics.github.io/MachineLearning/doc/pub/Splines/html/Splines-bs.html" target="_blank">optimization
methods</a>.

<p>
This will be discussed next week. Before we develop our own codes for logistic regression, we end this lecture by studying the functionality that <b>Scikit-learn</b> offers.
</section>


<section>
<h2 id="wisconsin-cancer-data">Wisconsin Cancer Data  </h2>

<p>
We show here how we can use a simple regression case on the breast
cancer data using Logistic regression as our algorithm for
classification.

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.model_selection</span> <span style="color: #8B008B; font-weight: bold">import</span>  train_test_split 
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.datasets</span> <span style="color: #8B008B; font-weight: bold">import</span> load_breast_cancer
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.linear_model</span> <span style="color: #8B008B; font-weight: bold">import</span> LogisticRegression

<span style="color: #228B22"># Load the data</span>
cancer = load_breast_cancer()

X_train, X_test, y_train, y_test = train_test_split(cancer.data,cancer.target,random_state=<span style="color: #B452CD">0</span>)
<span style="color: #658b00">print</span>(X_train.shape)
<span style="color: #658b00">print</span>(X_test.shape)
<span style="color: #228B22"># Logistic Regression</span>
logreg = LogisticRegression(solver=<span style="color: #CD5555">&#39;lbfgs&#39;</span>)
logreg.fit(X_train, y_train)
<span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;Test set accuracy with Logistic Regression: {:.2f}&quot;</span>.format(logreg.score(X_test,y_test)))
<span style="color: #228B22">#now scale the data</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.preprocessing</span> <span style="color: #8B008B; font-weight: bold">import</span> StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
<span style="color: #228B22"># Logistic Regression</span>
logreg.fit(X_train_scaled, y_train)
<span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;Test set accuracy Logistic Regression with scaled data: {:.2f}&quot;</span>.format(logreg.score(X_test_scaled,y_test)))
</pre></div>
</section>


<section>
<h2 id="using-the-correlation-matrix">Using the correlation matrix </h2>

<p>
In addition to the above scores, we could also study the covariance (and the correlation matrix).
We use <b>Pandas</b> to compute the correlation matrix.
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.model_selection</span> <span style="color: #8B008B; font-weight: bold">import</span>  train_test_split 
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.datasets</span> <span style="color: #8B008B; font-weight: bold">import</span> load_breast_cancer
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.linear_model</span> <span style="color: #8B008B; font-weight: bold">import</span> LogisticRegression
cancer = load_breast_cancer()
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">pandas</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">pd</span>
<span style="color: #228B22"># Making a data frame</span>
cancerpd = pd.DataFrame(cancer.data, columns=cancer.feature_names)

fig, axes = plt.subplots(<span style="color: #B452CD">15</span>,<span style="color: #B452CD">2</span>,figsize=(<span style="color: #B452CD">10</span>,<span style="color: #B452CD">20</span>))
malignant = cancer.data[cancer.target == <span style="color: #B452CD">0</span>]
benign = cancer.data[cancer.target == <span style="color: #B452CD">1</span>]
ax = axes.ravel()

<span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #B452CD">30</span>):
    _, bins = np.histogram(cancer.data[:,i], bins =<span style="color: #B452CD">50</span>)
    ax[i].hist(malignant[:,i], bins = bins, alpha = <span style="color: #B452CD">0.5</span>)
    ax[i].hist(benign[:,i], bins = bins, alpha = <span style="color: #B452CD">0.5</span>)
    ax[i].set_title(cancer.feature_names[i])
    ax[i].set_yticks(())
ax[<span style="color: #B452CD">0</span>].set_xlabel(<span style="color: #CD5555">&quot;Feature magnitude&quot;</span>)
ax[<span style="color: #B452CD">0</span>].set_ylabel(<span style="color: #CD5555">&quot;Frequency&quot;</span>)
ax[<span style="color: #B452CD">0</span>].legend([<span style="color: #CD5555">&quot;Malignant&quot;</span>, <span style="color: #CD5555">&quot;Benign&quot;</span>], loc =<span style="color: #CD5555">&quot;best&quot;</span>)
fig.tight_layout()
plt.show()

<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">seaborn</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">sns</span>
correlation_matrix = cancerpd.corr().round(<span style="color: #B452CD">1</span>)
<span style="color: #228B22"># use the heatmap function from seaborn to plot the correlation matrix</span>
<span style="color: #228B22"># annot = True to print the values inside the square</span>
plt.figure(figsize=(<span style="color: #B452CD">15</span>,<span style="color: #B452CD">8</span>))
sns.heatmap(data=correlation_matrix, annot=<span style="color: #8B008B; font-weight: bold">True</span>)
plt.show()
</pre></div>
</section>


<section>
<h2 id="discussing-the-correlation-data">Discussing the correlation data </h2>

<p>
In the above example we note two things. In the first plot we display
the overlap of benign and malignant tumors as functions of the various
features in the Wisconsing breast cancer data set. We see that for
some of the features we can distinguish clearly the benign and
malignant cases while for other features we cannot. This can point to
us which features may be of greater interest when we wish to classify
a benign or not benign tumour.

<p>
In the second figure we have computed the so-called correlation
matrix, which in our case with thirty features becomes a \( 30\times 30 \)
matrix.

<p>
We constructed this matrix using <b>pandas</b> via the statements
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span>cancerpd = pd.DataFrame(cancer.data, columns=cancer.feature_names)
</pre></div>
<p>
and then
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span>correlation_matrix = cancerpd.corr().round(<span style="color: #B452CD">1</span>)
</pre></div>
<p>
Diagonalizing this matrix we can in turn say something about which
features are of relevance and which are not. This leads  us to
the classical Principal Component Analysis (PCA) theorem with
applications. This will be discussed later this semester (<a href="https://compphysics.github.io/MachineLearning/doc/pub/week43/html/week43-bs.html" target="_blank">week 43</a>).
</section>


<section>
<h2 id="other-measures-in-classification-studies-cancer-data-again">Other measures in classification studies: Cancer Data  again </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%;"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.model_selection</span> <span style="color: #8B008B; font-weight: bold">import</span>  train_test_split 
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.datasets</span> <span style="color: #8B008B; font-weight: bold">import</span> load_breast_cancer
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.linear_model</span> <span style="color: #8B008B; font-weight: bold">import</span> LogisticRegression

<span style="color: #228B22"># Load the data</span>
cancer = load_breast_cancer()

X_train, X_test, y_train, y_test = train_test_split(cancer.data,cancer.target,random_state=<span style="color: #B452CD">0</span>)
<span style="color: #658b00">print</span>(X_train.shape)
<span style="color: #658b00">print</span>(X_test.shape)
<span style="color: #228B22"># Logistic Regression</span>
logreg = LogisticRegression(solver=<span style="color: #CD5555">&#39;lbfgs&#39;</span>)
logreg.fit(X_train, y_train)
<span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;Test set accuracy with Logistic Regression: {:.2f}&quot;</span>.format(logreg.score(X_test,y_test)))
<span style="color: #228B22">#now scale the data</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.preprocessing</span> <span style="color: #8B008B; font-weight: bold">import</span> StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
<span style="color: #228B22"># Logistic Regression</span>
logreg.fit(X_train_scaled, y_train)
<span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;Test set accuracy Logistic Regression with scaled data: {:.2f}&quot;</span>.format(logreg.score(X_test_scaled,y_test)))


<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.preprocessing</span> <span style="color: #8B008B; font-weight: bold">import</span> LabelEncoder
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.model_selection</span> <span style="color: #8B008B; font-weight: bold">import</span> cross_validate
<span style="color: #228B22">#Cross validation</span>
accuracy = cross_validate(logreg,X_test_scaled,y_test,cv=<span style="color: #B452CD">10</span>)[<span style="color: #CD5555">&#39;test_score&#39;</span>]
<span style="color: #658b00">print</span>(accuracy)
<span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;Test set accuracy with Logistic Regression  and scaled data: {:.2f}&quot;</span>.format(logreg.score(X_test_scaled,y_test)))


<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">scikitplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">skplt</span>
y_pred = logreg.predict(X_test_scaled)
skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize=<span style="color: #8B008B; font-weight: bold">True</span>)
plt.show()
y_probas = logreg.predict_proba(X_test_scaled)
skplt.metrics.plot_roc(y_test, y_probas)
plt.show()
skplt.metrics.plot_cumulative_gain(y_test, y_probas)
plt.show()
</pre></div>
</section>



</div> <!-- class="slides" -->
</div> <!-- class="reveal" -->

<script src="reveal.js/lib/js/head.min.js"></script>
<script src="reveal.js/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

    // Display navigation controls in the bottom right corner
    controls: true,

    // Display progress bar (below the horiz. slider)
    progress: true,

    // Display the page number of the current slide
    slideNumber: true,

    // Push each slide change to the browser history
    history: false,

    // Enable keyboard shortcuts for navigation
    keyboard: true,

    // Enable the slide overview mode
    overview: true,

    // Vertical centering of slides
    //center: true,
    center: false,

    // Enables touch navigation on devices with touch input
    touch: true,

    // Loop the presentation
    loop: false,

    // Change the presentation direction to be RTL
    rtl: false,

    // Turns fragments on and off globally
    fragments: true,

    // Flags if the presentation is running in an embedded mode,
    // i.e. contained within a limited portion of the screen
    embedded: false,

    // Number of milliseconds between automatically proceeding to the
    // next slide, disabled when set to 0, this value can be overwritten
    // by using a data-autoslide attribute on your slides
    autoSlide: 0,

    // Stop auto-sliding after user input
    autoSlideStoppable: true,

    // Enable slide navigation via mouse wheel
    mouseWheel: false,

    // Hides the address bar on mobile devices
    hideAddressBar: true,

    // Opens links in an iframe preview overlay
    previewLinks: false,

    // Transition style
    transition: 'default', // default/cube/page/concave/zoom/linear/fade/none

    // Transition speed
    transitionSpeed: 'default', // default/fast/slow

    // Transition style for full page slide backgrounds
    backgroundTransition: 'default', // default/none/slide/concave/convex/zoom

    // Number of slides away from the current that are visible
    viewDistance: 3,

    // Parallax background image
    //parallaxBackgroundImage: '', // e.g. "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'"

    // Parallax background size
    //parallaxBackgroundSize: '' // CSS syntax, e.g. "2100px 900px"

    theme: Reveal.getQueryHash().theme, // available themes are in reveal.js/css/theme
    transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/none

});

Reveal.initialize({
    dependencies: [
        // Cross-browser shim that fully implements classList - https://github.com/eligrey/classList.js/
        { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },

        // Interpret Markdown in <section> elements
        { src: 'reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
        { src: 'reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },

        // Syntax highlight for <code> elements
        { src: 'reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },

        // Zoom in and out with Alt+click
        { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },

        // Speaker notes
        { src: 'reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },

        // Remote control your reveal.js presentation using a touch device
        //{ src: 'reveal.js/plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } },

        // MathJax
        //{ src: 'reveal.js/plugin/math/math.js', async: true }
    ]
});

Reveal.initialize({

    // The "normal" size of the presentation, aspect ratio will be preserved
    // when the presentation is scaled to fit different resolutions. Can be
    // specified using percentage units.
    width: 1170,  // original: 960,
    height: 700,

    // Factor of the display size that should remain empty around the content
    margin: 0.1,

    // Bounds for smallest/largest possible scale to apply to content
    minScale: 0.2,
    maxScale: 1.0

});
</script>

<!-- begin footer logo
<div style="position: absolute; bottom: 0px; left: 0; margin-left: 0px">
<img src="somelogo.png">
</div>
     end footer logo -->



</body>
</html>

<!--
HTML file automatically generated from DocOnce source
(https://github.com/doconce/doconce/)
doconce format html week36.do.txt --html_style=bootstrap --pygments_html_style=default --html_admon=bootstrap_panel --html_output=week36-bs --no_mako
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Week 36: Linear Regression and Gradient descent">
<title>Week 36: Linear Regression and Gradient descent</title>
<!-- Bootstrap style: bootstrap -->
<!-- doconce format html week36.do.txt --html_style=bootstrap --pygments_html_style=default --html_admon=bootstrap_panel --html_output=week36-bs --no_mako -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->
<style type="text/css">
/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}
/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>
</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Plans for week 36', 2, None, 'plans-for-week-36'),
              ('Material for lecture Monday September 2',
               2,
               None,
               'material-for-lecture-monday-september-2'),
              ('Mathematical Interpretation of Ordinary Least Squares',
               2,
               None,
               'mathematical-interpretation-of-ordinary-least-squares'),
              ('Residual Error', 2, None, 'residual-error'),
              ('Simple case', 2, None, 'simple-case'),
              ('The singular value decomposition',
               2,
               None,
               'the-singular-value-decomposition'),
              ('Linear Regression Problems',
               2,
               None,
               'linear-regression-problems'),
              ('Fixing the singularity', 2, None, 'fixing-the-singularity'),
              ('Ridge and LASSO Regression',
               2,
               None,
               'ridge-and-lasso-regression'),
              ('Deriving the  Ridge Regression Equations',
               2,
               None,
               'deriving-the-ridge-regression-equations'),
              ('Basic math of the SVD', 2, None, 'basic-math-of-the-svd'),
              ('The SVD, a Fantastic Algorithm',
               2,
               None,
               'the-svd-a-fantastic-algorithm'),
              ('Economy-size SVD', 2, None, 'economy-size-svd'),
              ('Codes for the SVD', 2, None, 'codes-for-the-svd'),
              ('Note about SVD Calculations',
               2,
               None,
               'note-about-svd-calculations'),
              ('Mathematics of the SVD and implications',
               2,
               None,
               'mathematics-of-the-svd-and-implications'),
              ('Example Matrix', 2, None, 'example-matrix'),
              ('Setting up the Matrix to be inverted',
               2,
               None,
               'setting-up-the-matrix-to-be-inverted'),
              ('Further properties (important for our analyses later)',
               2,
               None,
               'further-properties-important-for-our-analyses-later'),
              ('Back to Ridge and LASSO Regression',
               2,
               None,
               'back-to-ridge-and-lasso-regression'),
              ('Interpreting the Ridge results',
               2,
               None,
               'interpreting-the-ridge-results'),
              ('More interpretations', 2, None, 'more-interpretations'),
              ('Deriving the  Lasso Regression Equations',
               2,
               None,
               'deriving-the-lasso-regression-equations'),
              ('Material for lab sessions  sessions Tuesday and Wednesday',
               2,
               None,
               'material-for-lab-sessions-sessions-tuesday-and-wednesday'),
              ('Linear Regression and  the SVD',
               2,
               None,
               'linear-regression-and-the-svd'),
              ('What does it mean?', 2, None, 'what-does-it-mean'),
              ('Ridge and LASSO Regression',
               2,
               None,
               'ridge-and-lasso-regression'),
              ('From OLS to Ridge and Lasso',
               2,
               None,
               'from-ols-to-ridge-and-lasso'),
              ('Deriving the  Ridge Regression Equations',
               2,
               None,
               'deriving-the-ridge-regression-equations'),
              ('Note on Scikit-Learn', 2, None, 'note-on-scikit-learn'),
              ('Comparison with OLS', 2, None, 'comparison-with-ols'),
              ('SVD analysis', 2, None, 'svd-analysis'),
              ('Interpreting the Ridge results',
               2,
               None,
               'interpreting-the-ridge-results'),
              ('More interpretations', 2, None, 'more-interpretations'),
              ('Deriving the  Lasso Regression Equations',
               2,
               None,
               'deriving-the-lasso-regression-equations'),
              ('Simple example to illustrate Ordinary Least Squares, Ridge and '
               'Lasso Regression',
               2,
               None,
               'simple-example-to-illustrate-ordinary-least-squares-ridge-and-lasso-regression'),
              ('Ridge Regression', 2, None, 'ridge-regression'),
              ('Lasso Regression', 2, None, 'lasso-regression'),
              ('Yet another Example', 2, None, 'yet-another-example'),
              ('The OLS case', 2, None, 'the-ols-case'),
              ('The Ridge case', 2, None, 'the-ridge-case'),
              ('Writing the Cost Function',
               2,
               None,
               'writing-the-cost-function'),
              ('Lasso case', 2, None, 'lasso-case'),
              ('The first Case', 2, None, 'the-first-case'),
              ('Simple code for solving the above problem',
               2,
               None,
               'simple-code-for-solving-the-above-problem'),
              ('With Lasso Regression', 2, None, 'with-lasso-regression'),
              ('Another Example, now with a polynomial fit',
               2,
               None,
               'another-example-now-with-a-polynomial-fit')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="week36-bs.html">Week 36: Linear Regression and Gradient descent</a>
  </div>
  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="._week36-bs001.html#plans-for-week-36" style="font-size: 80%;">Plans for week 36</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs002.html#material-for-lecture-monday-september-2" style="font-size: 80%;">Material for lecture Monday September 2</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs003.html#mathematical-interpretation-of-ordinary-least-squares" style="font-size: 80%;">Mathematical Interpretation of Ordinary Least Squares</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs004.html#residual-error" style="font-size: 80%;">Residual Error</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs005.html#simple-case" style="font-size: 80%;">Simple case</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs006.html#the-singular-value-decomposition" style="font-size: 80%;">The singular value decomposition</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs007.html#linear-regression-problems" style="font-size: 80%;">Linear Regression Problems</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs008.html#fixing-the-singularity" style="font-size: 80%;">Fixing the singularity</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs027.html#ridge-and-lasso-regression" style="font-size: 80%;">Ridge and LASSO Regression</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs029.html#deriving-the-ridge-regression-equations" style="font-size: 80%;">Deriving the  Ridge Regression Equations</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs011.html#basic-math-of-the-svd" style="font-size: 80%;">Basic math of the SVD</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs012.html#the-svd-a-fantastic-algorithm" style="font-size: 80%;">The SVD, a Fantastic Algorithm</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs013.html#economy-size-svd" style="font-size: 80%;">Economy-size SVD</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs014.html#codes-for-the-svd" style="font-size: 80%;">Codes for the SVD</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs015.html#note-about-svd-calculations" style="font-size: 80%;">Note about SVD Calculations</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs016.html#mathematics-of-the-svd-and-implications" style="font-size: 80%;">Mathematics of the SVD and implications</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs017.html#example-matrix" style="font-size: 80%;">Example Matrix</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs018.html#setting-up-the-matrix-to-be-inverted" style="font-size: 80%;">Setting up the Matrix to be inverted</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs019.html#further-properties-important-for-our-analyses-later" style="font-size: 80%;">Further properties (important for our analyses later)</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs020.html#back-to-ridge-and-lasso-regression" style="font-size: 80%;">Back to Ridge and LASSO Regression</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs033.html#interpreting-the-ridge-results" style="font-size: 80%;">Interpreting the Ridge results</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs034.html#more-interpretations" style="font-size: 80%;">More interpretations</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs035.html#deriving-the-lasso-regression-equations" style="font-size: 80%;">Deriving the  Lasso Regression Equations</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs024.html#material-for-lab-sessions-sessions-tuesday-and-wednesday" style="font-size: 80%;">Material for lab sessions  sessions Tuesday and Wednesday</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs025.html#linear-regression-and-the-svd" style="font-size: 80%;">Linear Regression and  the SVD</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs026.html#what-does-it-mean" style="font-size: 80%;">What does it mean?</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs027.html#ridge-and-lasso-regression" style="font-size: 80%;">Ridge and LASSO Regression</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs028.html#from-ols-to-ridge-and-lasso" style="font-size: 80%;">From OLS to Ridge and Lasso</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs029.html#deriving-the-ridge-regression-equations" style="font-size: 80%;">Deriving the  Ridge Regression Equations</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs030.html#note-on-scikit-learn" style="font-size: 80%;">Note on Scikit-Learn</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs031.html#comparison-with-ols" style="font-size: 80%;">Comparison with OLS</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs032.html#svd-analysis" style="font-size: 80%;">SVD analysis</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs033.html#interpreting-the-ridge-results" style="font-size: 80%;">Interpreting the Ridge results</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs034.html#more-interpretations" style="font-size: 80%;">More interpretations</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs035.html#deriving-the-lasso-regression-equations" style="font-size: 80%;">Deriving the  Lasso Regression Equations</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs036.html#simple-example-to-illustrate-ordinary-least-squares-ridge-and-lasso-regression" style="font-size: 80%;">Simple example to illustrate Ordinary Least Squares, Ridge and Lasso Regression</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs037.html#ridge-regression" style="font-size: 80%;">Ridge Regression</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs038.html#lasso-regression" style="font-size: 80%;">Lasso Regression</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs039.html#yet-another-example" style="font-size: 80%;">Yet another Example</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs040.html#the-ols-case" style="font-size: 80%;">The OLS case</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs041.html#the-ridge-case" style="font-size: 80%;">The Ridge case</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs042.html#writing-the-cost-function" style="font-size: 80%;">Writing the Cost Function</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs043.html#lasso-case" style="font-size: 80%;">Lasso case</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs044.html#the-first-case" style="font-size: 80%;">The first Case</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs045.html#simple-code-for-solving-the-above-problem" style="font-size: 80%;">Simple code for solving the above problem</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs046.html#with-lasso-regression" style="font-size: 80%;">With Lasso Regression</a></li>
     <!-- navigation toc: --> <li><a href="._week36-bs047.html#another-example-now-with-a-polynomial-fit" style="font-size: 80%;">Another Example, now with a polynomial fit</a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->
<div class="container">
<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->
<a name="part0010"></a>
<!-- !split -->
<h2 id="deriving-the-ridge-regression-equations" class="anchor">Deriving the  Ridge Regression Equations </h2>

<p>Using the matrix-vector expression for Ridge regression and dropping the parameter \( 1/n \) in front of the standard means squared error equation, we have</p>

$$
C(\boldsymbol{X},\boldsymbol{\theta})=\left\{(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta})^T(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta})\right\}+\lambda\boldsymbol{\theta}^T\boldsymbol{\theta},
$$

<p>and 
taking the derivatives with respect to \( \boldsymbol{\theta} \) we obtain then
a slightly modified matrix inversion problem which for finite values
of \( \lambda \) does not suffer from singularity problems. We obtain
the optimal parameters
</p>
$$
\hat{\boldsymbol{\theta}}_{\mathrm{Ridge}} = \left(\boldsymbol{X}^T\boldsymbol{X}+\lambda\boldsymbol{I}\right)^{-1}\boldsymbol{X}^T\boldsymbol{y},
$$

<p>with \( \boldsymbol{I} \) being a \( p\times p \) identity matrix with the constraint that</p>

$$
\sum_{i=0}^{p-1} \theta_i^2 \leq t,
$$

<p>with \( t \) a finite positive number. </p>

<p>If we keep the \( 1/n \) factor, the equation for the optimal \( \theta \) changes to</p>
$$
\hat{\boldsymbol{\theta}}_{\mathrm{Ridge}} = \left(\boldsymbol{X}^T\boldsymbol{X}+n\lambda\boldsymbol{I}\right)^{-1}\boldsymbol{X}^T\boldsymbol{y}.
$$

<p>In many textbooks the \( 1/n \) term is often omitted. Note that a library like <b>Scikit-Learn</b> does not include the \( 1/n \) factor in the setup of the cost function.</p>

<p>When we compare this with the ordinary least squares result we have</p>
$$
\hat{\boldsymbol{\theta}}_{\mathrm{OLS}} = \left(\boldsymbol{X}^T\boldsymbol{X}\right)^{-1}\boldsymbol{X}^T\boldsymbol{y},
$$

<p>which can lead to singular matrices. However, with the SVD, we can always compute the inverse of the matrix \( \boldsymbol{X}^T\boldsymbol{X} \).</p>

<p>We see that Ridge regression is nothing but the standard OLS with a
modified diagonal term added to \( \boldsymbol{X}^T\boldsymbol{X} \). The consequences, in
particular for our discussion of the bias-variance tradeoff are rather
interesting. We will see that for specific values of \( \lambda \), we may
even reduce the variance of the optimal parameters \( \boldsymbol{\theta} \). These topics and other related ones, will be discussed after the more linear algebra oriented analysis here.
</p>

<p>When we have discussed the singular value decomposition of the design
matrix \( \boldsymbol{X} \), we will in turn perform a more rigorous mathematical
discussion of Ridge regression.
</p>

<p>The code here is a simple demonstration of how to implement Ridge regression with our own code and compare this with scikit-learn.</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">pandas</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">pd</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> train_test_split
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn</span> <span style="color: #008000; font-weight: bold">import</span> linear_model

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">MSE</span>(y_data,y_model):
    n <span style="color: #666666">=</span> np<span style="color: #666666">.</span>size(y_model)
    <span style="color: #008000; font-weight: bold">return</span> np<span style="color: #666666">.</span>sum((y_data<span style="color: #666666">-</span>y_model)<span style="color: #666666">**2</span>)<span style="color: #666666">/</span>n


<span style="color: #408080; font-style: italic"># A seed just to ensure that the random numbers are the same for every run.</span>
<span style="color: #408080; font-style: italic"># Useful for eventual debugging.</span>
np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>seed(<span style="color: #666666">3155</span>)

n <span style="color: #666666">=</span> <span style="color: #666666">100</span>
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(n)
y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>x<span style="color: #666666">**2</span>) <span style="color: #666666">+</span> <span style="color: #666666">1.5</span> <span style="color: #666666">*</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(x<span style="color: #666666">-2</span>)<span style="color: #666666">**2</span>)

Maxpolydegree <span style="color: #666666">=</span> <span style="color: #666666">20</span>
X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros((n,Maxpolydegree))
<span style="color: #408080; font-style: italic">#We include explicitely the intercept column</span>
<span style="color: #008000; font-weight: bold">for</span> degree <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(Maxpolydegree):
    X[:,degree] <span style="color: #666666">=</span> x<span style="color: #666666">**</span>degree
<span style="color: #408080; font-style: italic"># We split the data in test and training data</span>
X_train, X_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(X, y, test_size<span style="color: #666666">=0.2</span>)

p <span style="color: #666666">=</span> Maxpolydegree
I <span style="color: #666666">=</span> np<span style="color: #666666">.</span>eye(p,p)
<span style="color: #408080; font-style: italic"># Decide which values of lambda to use</span>
nlambdas <span style="color: #666666">=</span> <span style="color: #666666">6</span>
MSEOwnRidgePredict <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(nlambdas)
MSERidgePredict <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(nlambdas)
lambdas <span style="color: #666666">=</span> np<span style="color: #666666">.</span>logspace(<span style="color: #666666">-4</span>, <span style="color: #666666">2</span>, nlambdas)
<span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(nlambdas):
    lmb <span style="color: #666666">=</span> lambdas[i]
    OwnRidgeTheta <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linalg<span style="color: #666666">.</span>pinv(X_train<span style="color: #666666">.</span>T <span style="color: #666666">@</span> X_train<span style="color: #666666">+</span>lmb<span style="color: #666666">*</span>I) <span style="color: #666666">@</span> X_train<span style="color: #666666">.</span>T <span style="color: #666666">@</span> y_train
    <span style="color: #408080; font-style: italic"># Note: we include the intercept column and no scaling</span>
    RegRidge <span style="color: #666666">=</span> linear_model<span style="color: #666666">.</span>Ridge(lmb,fit_intercept<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>)
    RegRidge<span style="color: #666666">.</span>fit(X_train,y_train)
    <span style="color: #408080; font-style: italic"># and then make the prediction</span>
    ytildeOwnRidge <span style="color: #666666">=</span> X_train <span style="color: #666666">@</span> OwnRidgeTheta
    ypredictOwnRidge <span style="color: #666666">=</span> X_test <span style="color: #666666">@</span> OwnRidgeTheta
    ytildeRidge <span style="color: #666666">=</span> RegRidge<span style="color: #666666">.</span>predict(X_train)
    ypredictRidge <span style="color: #666666">=</span> RegRidge<span style="color: #666666">.</span>predict(X_test)
    MSEOwnRidgePredict[i] <span style="color: #666666">=</span> MSE(y_test,ypredictOwnRidge)
    MSERidgePredict[i] <span style="color: #666666">=</span> MSE(y_test,ypredictRidge)
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Theta values for own Ridge implementation&quot;</span>)
    <span style="color: #008000">print</span>(OwnRidgeTheta)
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Theta values for Scikit-Learn Ridge implementation&quot;</span>)
    <span style="color: #008000">print</span>(RegRidge<span style="color: #666666">.</span>coef_)
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;MSE values for own Ridge implementation&quot;</span>)
    <span style="color: #008000">print</span>(MSEOwnRidgePredict[i])
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;MSE values for Scikit-Learn Ridge implementation&quot;</span>)
    <span style="color: #008000">print</span>(MSERidgePredict[i])

<span style="color: #408080; font-style: italic"># Now plot the results</span>
plt<span style="color: #666666">.</span>figure()
plt<span style="color: #666666">.</span>plot(np<span style="color: #666666">.</span>log10(lambdas), MSEOwnRidgePredict, <span style="color: #BA2121">&#39;r&#39;</span>, label <span style="color: #666666">=</span> <span style="color: #BA2121">&#39;MSE own Ridge Test&#39;</span>)
plt<span style="color: #666666">.</span>plot(np<span style="color: #666666">.</span>log10(lambdas), MSERidgePredict, <span style="color: #BA2121">&#39;g&#39;</span>, label <span style="color: #666666">=</span> <span style="color: #BA2121">&#39;MSE Ridge Test&#39;</span>)

plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&#39;log10(lambda)&#39;</span>)
plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&#39;MSE&#39;</span>)
plt<span style="color: #666666">.</span>legend()
plt<span style="color: #666666">.</span>show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>The results here agree when we force <b>Scikit-Learn</b>'s Ridge function to include the first column in our design matrix.
We see that the results agree very well. Here we have thus explicitely included the intercept column in the design matrix.
What happens if we do not include the intercept in our fit? We will discuss this in more detail next week.
</p>

<p>
<!-- navigation buttons at the bottom of the page -->
<ul class="pagination">
<li><a href="._week36-bs009.html">&laquo;</a></li>
  <li><a href="._week36-bs000.html">1</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week36-bs002.html">3</a></li>
  <li><a href="._week36-bs003.html">4</a></li>
  <li><a href="._week36-bs004.html">5</a></li>
  <li><a href="._week36-bs005.html">6</a></li>
  <li><a href="._week36-bs006.html">7</a></li>
  <li><a href="._week36-bs007.html">8</a></li>
  <li><a href="._week36-bs008.html">9</a></li>
  <li><a href="._week36-bs009.html">10</a></li>
  <li class="active"><a href="._week36-bs010.html">11</a></li>
  <li><a href="._week36-bs011.html">12</a></li>
  <li><a href="._week36-bs012.html">13</a></li>
  <li><a href="._week36-bs013.html">14</a></li>
  <li><a href="._week36-bs014.html">15</a></li>
  <li><a href="._week36-bs015.html">16</a></li>
  <li><a href="._week36-bs016.html">17</a></li>
  <li><a href="._week36-bs017.html">18</a></li>
  <li><a href="._week36-bs018.html">19</a></li>
  <li><a href="._week36-bs019.html">20</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week36-bs047.html">48</a></li>
  <li><a href="._week36-bs011.html">&raquo;</a></li>
</ul>
<!-- ------------------- end of main content --------------- -->
</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>
<!-- Bootstrap footer
<footer>
<a href="https://..."><img width="250" align=right src="https://..."></a>
</footer>
-->
<center style="font-size:80%">
<!-- copyright only on the titlepage -->
</center>
</body>
</html>


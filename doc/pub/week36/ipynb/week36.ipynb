{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "654ab862",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html week36.do.txt --no_mako -->\n",
    "<!-- dom:TITLE: Week 36: Linear Regression and Gradient descent -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bc19f6",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Week 36: Linear Regression and Gradient descent\n",
    "**Morten Hjorth-Jensen**, Department of Physics, University of Oslo, Norway\n",
    "\n",
    "Date: **September 1-5, 2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9af352",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Plans for week 36\n",
    "\n",
    "* Material for the lecture on Monday September 2\n",
    "\n",
    "a. Linear Regression, ordinary least squares (OLS), Ridge and Lasso and mathematical analysis\n",
    "\n",
    "b. Derivation of Gradient descent and discussion of implementations for\n",
    "<!-- * [Video of lecture](https://youtu.be/oHjqjUB36KE) -->\n",
    "<!-- * [Whiteboard notes](https://github.com/CompPhysics/MachineLearning/blob/master/doc/HandWrittenNotes/2024/NotesSeptember2.pdf) -->\n",
    "\n",
    "* Material for the active learning sessions on Tuesday and Wednesday (see at the end of this slides)\n",
    "\n",
    "  * Technicalities concerning Ridge and Lasso linear regression.\n",
    "\n",
    "  * Presentation and discussion of the first project\n",
    "<!-- * [Video of lab session](https://youtu.be/ZrIdZdZtHe0) -->\n",
    "\n",
    "* Reading suggestion:\n",
    "\n",
    "a. Goodfellow et al, Deep Learning, introduction to gradient descent, see chapter 4.3 at <https://www.deeplearningbook.org/contents/numerical.html>\n",
    "\n",
    "b. Rashcka et al, pages 37-44 and pages 278-283 with focus on linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd2183c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Material for lecture Monday September 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513f1331",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Material for lab sessions  sessions Tuesday and Wednesday\n",
    "\n",
    "The material here contains a summary of the lecture on Monday and discussion of SVD, Ridge and Lasso regression with examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad197236",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Linear Regression and  the SVD\n",
    "\n",
    "We used the SVD to analyse the matrix to invert in ordinary lineat regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b9ddee",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X}^T\\boldsymbol{X}=\\boldsymbol{V}\\boldsymbol{\\Sigma}^T\\boldsymbol{U}^T\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^T=\\boldsymbol{V}\\boldsymbol{\\Sigma}^T\\boldsymbol{\\Sigma}\\boldsymbol{V}^T.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68bd4a2",
   "metadata": {
    "editable": true
   },
   "source": [
    "Since the matrices here have dimension $p\\times p$, with $p$ corresponding to the singular values, we defined last week the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90436ad2",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{\\Sigma}^T\\boldsymbol{\\Sigma} = \\begin{bmatrix} \\tilde{\\boldsymbol{\\Sigma}} & \\boldsymbol{0}\\\\ \\end{bmatrix}\\begin{bmatrix} \\tilde{\\boldsymbol{\\Sigma}} \\\\ \\boldsymbol{0}\\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f67366e",
   "metadata": {
    "editable": true
   },
   "source": [
    "where the tilde-matrix $\\tilde{\\boldsymbol{\\Sigma}}$ is a matrix of dimension $p\\times p$ containing only the singular values $\\sigma_i$, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1866df",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\tilde{\\boldsymbol{\\Sigma}}=\\begin{bmatrix} \\sigma_0 & 0 & 0 & \\dots & 0 & 0 \\\\\n",
    "                                    0 & \\sigma_1 & 0 & \\dots & 0 & 0 \\\\\n",
    "\t\t\t\t    0 & 0 & \\sigma_2 & \\dots & 0 & 0 \\\\\n",
    "\t\t\t\t    0 & 0 & 0 & \\dots & \\sigma_{p-2} & 0 \\\\\n",
    "\t\t\t\t    0 & 0 & 0 & \\dots & 0 & \\sigma_{p-1} \\\\\n",
    "\\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4b618c",
   "metadata": {
    "editable": true
   },
   "source": [
    "meaning we can write"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b26f26",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X}^T\\boldsymbol{X}=\\boldsymbol{V}\\tilde{\\boldsymbol{\\Sigma}}^2\\boldsymbol{V}^T.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd9e09f",
   "metadata": {
    "editable": true
   },
   "source": [
    "Multiplying from the right with $\\boldsymbol{V}$ (using the orthogonality of $\\boldsymbol{V}$) we get"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d875472",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\left(\\boldsymbol{X}^T\\boldsymbol{X}\\right)\\boldsymbol{V}=\\boldsymbol{V}\\tilde{\\boldsymbol{\\Sigma}}^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dd73e0",
   "metadata": {
    "editable": true
   },
   "source": [
    "## What does it mean?\n",
    "\n",
    "This means the vectors $\\boldsymbol{v}_i$ of the orthogonal matrix $\\boldsymbol{V}$\n",
    "are the eigenvectors of the matrix $\\boldsymbol{X}^T\\boldsymbol{X}$ with eigenvalues\n",
    "given by the singular values squared, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bfb958",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\left(\\boldsymbol{X}^T\\boldsymbol{X}\\right)\\boldsymbol{v}_i=\\boldsymbol{v}_i\\sigma_i^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f471a5",
   "metadata": {
    "editable": true
   },
   "source": [
    "In other words, each non-zero singular value of $\\boldsymbol{X}$ is a positive\n",
    "square root of an eigenvalue of $\\boldsymbol{X}^T\\boldsymbol{X}$.  It means also that\n",
    "the columns of $\\boldsymbol{V}$ are the eigenvectors of\n",
    "$\\boldsymbol{X}^T\\boldsymbol{X}$. Since we have ordered the singular values of\n",
    "$\\boldsymbol{X}$ in a descending order, it means that the column vectors\n",
    "$\\boldsymbol{v}_i$ are hierarchically ordered by how much correlation they\n",
    "encode from the columns of $\\boldsymbol{X}$. \n",
    "\n",
    "Note that these are also the eigenvectors and eigenvalues of the\n",
    "Hessian matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83252299",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Ridge and LASSO Regression\n",
    "\n",
    "Let us remind ourselves about the expression for the standard Mean Squared Error (MSE) which we used to define our cost function and the equations for the ordinary least squares (OLS) method, that is \n",
    "our optimization problem is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8521ba",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "{\\displaystyle \\min_{\\boldsymbol{\\beta}\\in {\\mathbb{R}}^{p}}}\\frac{1}{n}\\left\\{\\left(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta}\\right)^T\\left(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta}\\right)\\right\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33246dbe",
   "metadata": {
    "editable": true
   },
   "source": [
    "or we can state it as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e05b84b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "{\\displaystyle \\min_{\\boldsymbol{\\beta}\\in\n",
    "{\\mathbb{R}}^{p}}}\\frac{1}{n}\\sum_{i=0}^{n-1}\\left(y_i-\\tilde{y}_i\\right)^2=\\frac{1}{n}\\vert\\vert \\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta}\\vert\\vert_2^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386f7592",
   "metadata": {
    "editable": true
   },
   "source": [
    "where we have used the definition of  a norm-2 vector, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305c74c8",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\vert\\vert \\boldsymbol{x}\\vert\\vert_2 = \\sqrt{\\sum_i x_i^2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3eed08",
   "metadata": {
    "editable": true
   },
   "source": [
    "## From OLS to Ridge and Lasso\n",
    "\n",
    "By minimizing the above equation with respect to the parameters\n",
    "$\\boldsymbol{\\beta}$ we could then obtain an analytical expression for the\n",
    "parameters $\\boldsymbol{\\beta}$.  We can add a regularization parameter $\\lambda$ by\n",
    "defining a new cost function to be optimized, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61c3bb7",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "{\\displaystyle \\min_{\\boldsymbol{\\beta}\\in\n",
    "{\\mathbb{R}}^{p}}}\\frac{1}{n}\\vert\\vert \\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta}\\vert\\vert_2^2+\\lambda\\vert\\vert \\boldsymbol{\\beta}\\vert\\vert_2^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0e2714",
   "metadata": {
    "editable": true
   },
   "source": [
    "which leads to the Ridge regression minimization problem where we\n",
    "require that $\\vert\\vert \\boldsymbol{\\beta}\\vert\\vert_2^2\\le t$, where $t$ is\n",
    "a finite number larger than zero. We do not include such a constraints in the discussions here.\n",
    "\n",
    "By defining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3987610",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\boldsymbol{X},\\boldsymbol{\\beta})=\\frac{1}{n}\\vert\\vert \\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta}\\vert\\vert_2^2+\\lambda\\vert\\vert \\boldsymbol{\\beta}\\vert\\vert_1,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1957b4b",
   "metadata": {
    "editable": true
   },
   "source": [
    "we have a new optimization equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb514910",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "{\\displaystyle \\min_{\\boldsymbol{\\beta}\\in\n",
    "{\\mathbb{R}}^{p}}}\\frac{1}{n}\\vert\\vert \\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta}\\vert\\vert_2^2+\\lambda\\vert\\vert \\boldsymbol{\\beta}\\vert\\vert_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2630434",
   "metadata": {
    "editable": true
   },
   "source": [
    "which leads to Lasso regression. Lasso stands for least absolute shrinkage and selection operator. \n",
    "\n",
    "Here we have defined the norm-1 as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83f49fb",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\vert\\vert \\boldsymbol{x}\\vert\\vert_1 = \\sum_i \\vert x_i\\vert.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fbce65",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Deriving the  Ridge Regression Equations\n",
    "\n",
    "Using the matrix-vector expression for Ridge regression and dropping the parameter $1/n$ in front of the standard means squared error equation, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab835024",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\boldsymbol{X},\\boldsymbol{\\beta})=\\left\\{(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta})^T(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta})\\right\\}+\\lambda\\boldsymbol{\\beta}^T\\boldsymbol{\\beta},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f9c302",
   "metadata": {
    "editable": true
   },
   "source": [
    "and \n",
    "taking the derivatives with respect to $\\boldsymbol{\\beta}$ we obtain then\n",
    "a slightly modified matrix inversion problem which for finite values\n",
    "of $\\lambda$ does not suffer from singularity problems. We obtain\n",
    "the optimal parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ebd5bc",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}}_{\\mathrm{Ridge}} = \\left(\\boldsymbol{X}^T\\boldsymbol{X}+\\lambda\\boldsymbol{I}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362ea5fa",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $\\boldsymbol{I}$ being a $p\\times p$ identity matrix with the constraint that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d34847",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\sum_{i=0}^{p-1} \\beta_i^2 \\leq t,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25b4c25",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $t$ a finite positive number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0a0404",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Note on Scikit-Learn\n",
    "\n",
    "Note well that a library like **Scikit-Learn** does not include the $1/n$ factor in the expression for the mean-squared error. If you include it, the optimal parameter $\\beta$ becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7712fb05",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}}_{\\mathrm{Ridge}} = \\left(\\boldsymbol{X}^T\\boldsymbol{X}+n\\lambda\\boldsymbol{I}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064a101a",
   "metadata": {
    "editable": true
   },
   "source": [
    "In our codes where we compare our own codes with **Scikit-Learn**, we do thus not include the $1/n$ factor in the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd71d29",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Comparison with OLS\n",
    "When we compare this with the ordinary least squares result we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e51db6",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}}_{\\mathrm{OLS}} = \\left(\\boldsymbol{X}^T\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68683057",
   "metadata": {
    "editable": true
   },
   "source": [
    "which can lead to singular matrices. However, with the SVD, we can always compute the inverse of the matrix $\\boldsymbol{X}^T\\boldsymbol{X}$.\n",
    "\n",
    "We see that Ridge regression is nothing but the standard OLS with a\n",
    "modified diagonal term added to $\\boldsymbol{X}^T\\boldsymbol{X}$. The consequences, in\n",
    "particular for our discussion of the bias-variance tradeoff are rather\n",
    "interesting. We will see that for specific values of $\\lambda$, we may\n",
    "even reduce the variance of the optimal parameters $\\boldsymbol{\\beta}$. These topics and other related ones, will be discussed after the more linear algebra oriented analysis here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18240326",
   "metadata": {
    "editable": true
   },
   "source": [
    "## SVD analysis\n",
    "\n",
    "Using our insights about the SVD of the design matrix $\\boldsymbol{X}$ \n",
    "We have already analyzed the OLS solutions in terms of the eigenvectors (the columns) of the right singular value matrix $\\boldsymbol{U}$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ab6fb0",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\tilde{\\boldsymbol{y}}_{\\mathrm{OLS}}=\\boldsymbol{X}\\boldsymbol{\\beta}  =\\boldsymbol{U}\\boldsymbol{U}^T\\boldsymbol{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a960dd",
   "metadata": {
    "editable": true
   },
   "source": [
    "For Ridge regression this becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f6a309",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\tilde{\\boldsymbol{y}}_{\\mathrm{Ridge}}=\\boldsymbol{X}\\boldsymbol{\\beta}_{\\mathrm{Ridge}} = \\boldsymbol{U\\Sigma V^T}\\left(\\boldsymbol{V}\\boldsymbol{\\Sigma}^2\\boldsymbol{V}^T+\\lambda\\boldsymbol{I} \\right)^{-1}(\\boldsymbol{U\\Sigma V^T})^T\\boldsymbol{y}=\\sum_{j=0}^{p-1}\\boldsymbol{u}_j\\boldsymbol{u}_j^T\\frac{\\sigma_j^2}{\\sigma_j^2+\\lambda}\\boldsymbol{y},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940a96a0",
   "metadata": {
    "editable": true
   },
   "source": [
    "with the vectors $\\boldsymbol{u}_j$ being the columns of $\\boldsymbol{U}$ from the SVD of the matrix $\\boldsymbol{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9d0724",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Interpreting the Ridge results\n",
    "\n",
    "Since $\\lambda \\geq 0$, it means that compared to OLS, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a5c44c",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\sigma_j^2}{\\sigma_j^2+\\lambda} \\leq 1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9894a9f",
   "metadata": {
    "editable": true
   },
   "source": [
    "Ridge regression finds the coordinates of $\\boldsymbol{y}$ with respect to the\n",
    "orthonormal basis $\\boldsymbol{U}$, it then shrinks the coordinates by\n",
    "$\\frac{\\sigma_j^2}{\\sigma_j^2+\\lambda}$. Recall that the SVD has\n",
    "eigenvalues ordered in a descending way, that is $\\sigma_i \\geq\n",
    "\\sigma_{i+1}$.\n",
    "\n",
    "For small eigenvalues $\\sigma_i$ it means that their contributions become less important, a fact which can be used to reduce the number of degrees of freedom. More about this when we have covered the material on a statistical interpretation of various linear regression methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbffece4",
   "metadata": {
    "editable": true
   },
   "source": [
    "## More interpretations\n",
    "\n",
    "For the sake of simplicity, let us assume that the design matrix is orthonormal, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381fe4a6",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X}^T\\boldsymbol{X}=(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} =\\boldsymbol{I}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6786abf3",
   "metadata": {
    "editable": true
   },
   "source": [
    "In this case the standard OLS results in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8f3546",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{\\beta}^{\\mathrm{OLS}} = \\boldsymbol{X}^T\\boldsymbol{y}=\\sum_{i=0}^{n-1}\\boldsymbol{u}_i\\boldsymbol{u}_i^T\\boldsymbol{y},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2507d9e7",
   "metadata": {
    "editable": true
   },
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945284de",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{\\beta}^{\\mathrm{Ridge}} = \\left(\\boldsymbol{I}+\\lambda\\boldsymbol{I}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y}=\\left(1+\\lambda\\right)^{-1}\\boldsymbol{\\beta}^{\\mathrm{OLS}},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61991d06",
   "metadata": {
    "editable": true
   },
   "source": [
    "that is the Ridge estimator scales the OLS estimator by the inverse of a factor $1+\\lambda$, and\n",
    "the Ridge estimator converges to zero when the hyperparameter goes to\n",
    "infinity.\n",
    "\n",
    "We will come back to more interpreations after we have gone through some of the statistical analysis part. \n",
    "\n",
    "For more discussions of Ridge and Lasso regression, [Wessel van Wieringen's](https://arxiv.org/abs/1509.09169) article is highly recommended.\n",
    "Similarly, [Mehta et al's article](https://arxiv.org/abs/1803.08823) is also recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5730190",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Deriving the  Lasso Regression Equations\n",
    "\n",
    "Using the matrix-vector expression for Lasso regression, we have the following **cost** function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9baf1a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\boldsymbol{X},\\boldsymbol{\\beta})=\\frac{1}{n}\\left\\{(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta})^T(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta})\\right\\}+\\lambda\\vert\\vert\\boldsymbol{\\beta}\\vert\\vert_1,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865388f2",
   "metadata": {
    "editable": true
   },
   "source": [
    "Taking the derivative with respect to $\\boldsymbol{\\beta}$ and recalling that the derivative of the absolute value is (we drop the boldfaced vector symbol for simplicity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db158717",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{d \\vert \\beta\\vert}{d \\beta}=\\mathrm{sgn}(\\beta)=\\left\\{\\begin{array}{cc} 1 & \\beta > 0 \\\\-1 & \\beta < 0, \\end{array}\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26695ac",
   "metadata": {
    "editable": true
   },
   "source": [
    "we have that the derivative of the cost function is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a5ca74",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial C(\\boldsymbol{X},\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}}=-\\frac{2}{n}\\boldsymbol{X}^T(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta})+\\lambda sgn(\\boldsymbol{\\beta})=0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bed7a2b",
   "metadata": {
    "editable": true
   },
   "source": [
    "and reordering we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdbbaca",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{\\beta}+\\lambda sgn(\\boldsymbol{\\beta})=\\boldsymbol{X}^T\\boldsymbol{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c12039f",
   "metadata": {
    "editable": true
   },
   "source": [
    "This equation does not lead to a nice analytical equation as in Ridge regression or ordinary least squares. We have absorbed the factor $2/n$ in a redefinition of the parameter $\\lambda$. We will solve this type of problems using libraries like **scikit-learn** and using our own gradient descent code in project 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef20a9b4",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Simple example to illustrate Ordinary Least Squares, Ridge and Lasso Regression\n",
    "\n",
    "Let us assume that our design matrix is given by unit (identity) matrix, that is a square diagonal matrix with ones only along the\n",
    "diagonal. In this case we have an equal number of rows and columns $n=p$.\n",
    "\n",
    "Our model approximation is just $\\tilde{\\boldsymbol{y}}=\\boldsymbol{\\beta}$ and the mean squared error and thereby the cost function for ordinary least sqquares (OLS) is then (we drop the term $1/n$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff638cc0",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\boldsymbol{\\beta})=\\sum_{i=0}^{p-1}(y_i-\\beta_i)^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3311b61b",
   "metadata": {
    "editable": true
   },
   "source": [
    "and minimizing we have that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e40ccd2",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\beta}_i^{\\mathrm{OLS}} = y_i.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac9dff2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Ridge Regression\n",
    "\n",
    "For Ridge regression our cost function is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5134e6ba",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\boldsymbol{\\beta})=\\sum_{i=0}^{p-1}(y_i-\\beta_i)^2+\\lambda\\sum_{i=0}^{p-1}\\beta_i^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6907400",
   "metadata": {
    "editable": true
   },
   "source": [
    "and minimizing we have that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f47d69d",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\beta}_i^{\\mathrm{Ridge}} = \\frac{y_i}{1+\\lambda}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fe94eb",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Lasso Regression\n",
    "\n",
    "For Lasso regression our cost function is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e383b22",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\boldsymbol{\\beta})=\\sum_{i=0}^{p-1}(y_i-\\beta_i)^2+\\lambda\\sum_{i=0}^{p-1}\\vert\\beta_i\\vert=\\sum_{i=0}^{p-1}(y_i-\\beta_i)^2+\\lambda\\sum_{i=0}^{p-1}\\sqrt{\\beta_i^2},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217a5190",
   "metadata": {
    "editable": true
   },
   "source": [
    "and minimizing we have that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af831ebf",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "-2\\sum_{i=0}^{p-1}(y_i-\\beta_i)+\\lambda \\sum_{i=0}^{p-1}\\frac{(\\beta_i)}{\\vert\\beta_i\\vert}=0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63c47e3",
   "metadata": {
    "editable": true
   },
   "source": [
    "which leads to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8432613",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}}_i^{\\mathrm{Lasso}} = \\left\\{\\begin{array}{ccc}y_i-\\frac{\\lambda}{2} &\\mathrm{if} & y_i> \\frac{\\lambda}{2}\\\\\n",
    "                                                          y_i+\\frac{\\lambda}{2} &\\mathrm{if} & y_i< -\\frac{\\lambda}{2}\\\\\n",
    "\t\t\t\t\t\t\t  0 &\\mathrm{if} & \\vert y_i\\vert\\le  \\frac{\\lambda}{2}\\end{array}\\right.\\\\.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40be7059",
   "metadata": {
    "editable": true
   },
   "source": [
    "Plotting these results shows clearly that Lasso regression suppresses (sets to zero) values of $\\beta_i$ for specific values of $\\lambda$. Ridge regression reduces on the other hand the values of $\\beta_i$ as function of $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae35aa35",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Yet another Example\n",
    "\n",
    "Let us assume we have a data set with outputs/targets given by the vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e6d414",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{y}=\\begin{bmatrix}4 \\\\ 2 \\\\3\\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8488396a",
   "metadata": {
    "editable": true
   },
   "source": [
    "and our inputs as a $3\\times 2$ design matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd26957",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X}=\\begin{bmatrix}2 & 0\\\\ 0 & 1 \\\\ 0 & 0\\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f3283b",
   "metadata": {
    "editable": true
   },
   "source": [
    "meaning that we have two features and two unknown parameters $\\beta_0$ and $\\beta_1$ to be determined either by ordinary least squares, Ridge or Lasso regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e9eeb6",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The OLS case\n",
    "\n",
    "For ordinary least squares (OLS) we know that the optimal solution is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6082802f",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}}^{\\mathrm{OLS}}=\\left( \\boldsymbol{X}^T\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ecd677",
   "metadata": {
    "editable": true
   },
   "source": [
    "Inserting the above values we obtain that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be994264",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}}^{\\mathrm{OLS}}=\\begin{bmatrix}2 \\\\ 2\\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe94745b",
   "metadata": {
    "editable": true
   },
   "source": [
    "The code which implements this simpler case is presented after the discussion of Ridge and Lasso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b49cedd",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The Ridge case\n",
    "\n",
    "For Ridge regression we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a806f154",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}}^{\\mathrm{Ridge}}=\\left( \\boldsymbol{X}^T\\boldsymbol{X}+\\lambda\\boldsymbol{I}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefa84ec",
   "metadata": {
    "editable": true
   },
   "source": [
    "Inserting the above values we obtain that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01d6040",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}}^{\\mathrm{Ridge}}=\\begin{bmatrix}\\frac{8}{4+\\lambda} \\\\ \\frac{2}{1+\\lambda}\\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ae26b8",
   "metadata": {
    "editable": true
   },
   "source": [
    "There is normally a constraint on the value of $\\vert\\vert \\boldsymbol{\\beta}\\vert\\vert_2$ via the parameter $\\lambda$.\n",
    "Let us for simplicity assume that $\\beta_0^2+\\beta_1^2=1$ as constraint. This will allow us to find an expression for the optimal values of $\\beta$ and $\\lambda$.\n",
    "\n",
    "To see this, let us write the cost function for Ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db0770b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Writing the Cost Function\n",
    "\n",
    "We define the MSE without the $1/n$ factor and have then, using that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4296d66b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X}\\boldsymbol{\\beta}=\\begin{bmatrix} 2\\beta_0 \\\\ \\beta_1 \\\\0 \\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167a1b00",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\boldsymbol{\\beta})=(4-2\\beta_0)^2+(2-\\beta_1)^2+\\lambda(\\beta_0^2+\\beta_1^2),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae480e8f",
   "metadata": {
    "editable": true
   },
   "source": [
    "and taking the derivative with respect to $\\beta_0$ we get"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d787a1dc",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\beta_0=\\frac{8}{4+\\lambda},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cd33a8",
   "metadata": {
    "editable": true
   },
   "source": [
    "and for $\\beta_1$ we obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd43b981",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\beta_1=\\frac{2}{1+\\lambda},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f0108c",
   "metadata": {
    "editable": true
   },
   "source": [
    "Using the constraint for $\\beta_0^2+\\beta_1^2=1$ we can constrain $\\lambda$ by solving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74d5dc6",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\left(\\frac{8}{4+\\lambda}\\right)^2+\\left(\\frac{2}{1+\\lambda}\\right)^2=1,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cdb6be",
   "metadata": {
    "editable": true
   },
   "source": [
    "which gives $\\lambda=4.571$ and $\\beta_0=0.933$ and $\\beta_1=0.359$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7d1f9f",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Lasso case\n",
    "\n",
    "For Lasso we need now, keeping a  constraint on $\\vert\\beta_0\\vert+\\vert\\beta_1\\vert=1$,  to take the derivative of the absolute values of $\\beta_0$\n",
    "and $\\beta_1$. This gives us the following derivatives of the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33af366",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\boldsymbol{\\beta})=(4-2\\beta_0)^2+(2-\\beta_1)^2+\\lambda(\\vert\\beta_0\\vert+\\vert\\beta_1\\vert),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510d9216",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial C(\\boldsymbol{\\beta})}{\\partial \\beta_0}=-4(4-2\\beta_0)+\\lambda\\mathrm{sgn}(\\beta_0)=0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6cf088",
   "metadata": {
    "editable": true
   },
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af73ab41",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial C(\\boldsymbol{\\beta})}{\\partial \\beta_1}=-2(2-\\beta_1)+\\lambda\\mathrm{sgn}(\\beta_1)=0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0616ec01",
   "metadata": {
    "editable": true
   },
   "source": [
    "We have now four cases to solve besides the trivial cases $\\beta_0$ and/or $\\beta_1$ are zero, namely\n",
    "1. $\\beta_0 > 0$ and $\\beta_1 > 0$,\n",
    "\n",
    "2. $\\beta_0 > 0$ and $\\beta_1 < 0$,\n",
    "\n",
    "3. $\\beta_0 < 0$ and $\\beta_1 > 0$,\n",
    "\n",
    "4. $\\beta_0 < 0$ and $\\beta_1 < 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7330fcf",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The first Case\n",
    "\n",
    "If we consider the first case, we have then"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d7b150",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "-4(4-2\\beta_0)+\\lambda=0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0803ea8a",
   "metadata": {
    "editable": true
   },
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac7d432",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "-2(2-\\beta_1)+\\lambda=0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7d543c",
   "metadata": {
    "editable": true
   },
   "source": [
    "which yields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a73761e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\beta_0=\\frac{16+\\lambda}{8},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff21f10e",
   "metadata": {
    "editable": true
   },
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1b8d20",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\beta_1=\\frac{4+\\lambda}{2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c449fb5e",
   "metadata": {
    "editable": true
   },
   "source": [
    "Using the constraint on $\\beta_0$ and $\\beta_1$ we can then find the optimal value of $\\lambda$ for the different cases. We leave this as an exercise to you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96af3eda",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Simple code for solving the above problem\n",
    "\n",
    "Here we set up the OLS, Ridge and Lasso functionality in order to study the above example. Note that here we have opted for a set of values of $\\lambda$, meaning that we need to perform a search in order to find the optimal values.\n",
    "\n",
    "First we study and compare the OLS and Ridge results.  The next code compares all three methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8a3087a",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def R2(y_data, y_model):\n",
    "    return 1 - np.sum((y_data - y_model) ** 2) / np.sum((y_data - np.mean(y_data)) ** 2)\n",
    "def MSE(y_data,y_model):\n",
    "    n = np.size(y_model)\n",
    "    return np.sum((y_data-y_model)**2)/n\n",
    "\n",
    "\n",
    "# A seed just to ensure that the random numbers are the same for every run.\n",
    "# Useful for eventual debugging.\n",
    "\n",
    "X = np.array( [ [ 2, 0], [0, 1], [0,0]])\n",
    "y = np.array( [4, 2, 3])\n",
    "\n",
    "\n",
    "# matrix inversion to find beta\n",
    "OLSbeta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "print(OLSbeta)\n",
    "# and then make the prediction\n",
    "ytildeOLS = X @ OLSbeta\n",
    "print(\"Training MSE for OLS\")\n",
    "print(MSE(y,ytildeOLS))\n",
    "ypredictOLS = X @ OLSbeta\n",
    "\n",
    "# Repeat now for Ridge regression and various values of the regularization parameter\n",
    "I = np.eye(2,2)\n",
    "# Decide which values of lambda to use\n",
    "nlambdas = 100\n",
    "MSEPredict = np.zeros(nlambdas)\n",
    "lambdas = np.logspace(-4, 4, nlambdas)\n",
    "for i in range(nlambdas):\n",
    "    lmb = lambdas[i]\n",
    "    Ridgebeta = np.linalg.inv(X.T @ X+lmb*I) @ X.T @ y\n",
    "#    print(Ridgebeta)\n",
    "    # and then make the prediction\n",
    "    ypredictRidge = X @ Ridgebeta\n",
    "    MSEPredict[i] = MSE(y,ypredictRidge)\n",
    "#    print(MSEPredict[i])\n",
    "    # Now plot the results\n",
    "plt.figure()\n",
    "plt.plot(np.log10(lambdas), MSEPredict, 'r--', label = 'MSE Ridge Train')\n",
    "plt.xlabel('log10(lambda)')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5510a2c",
   "metadata": {
    "editable": true
   },
   "source": [
    "We see here that we reach a plateau. What is actually happening?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d959d30",
   "metadata": {
    "editable": true
   },
   "source": [
    "## With Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d1473e5",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "\n",
    "def R2(y_data, y_model):\n",
    "    return 1 - np.sum((y_data - y_model) ** 2) / np.sum((y_data - np.mean(y_data)) ** 2)\n",
    "def MSE(y_data,y_model):\n",
    "    n = np.size(y_model)\n",
    "    return np.sum((y_data-y_model)**2)/n\n",
    "\n",
    "\n",
    "# A seed just to ensure that the random numbers are the same for every run.\n",
    "# Useful for eventual debugging.\n",
    "\n",
    "X = np.array( [ [ 2, 0], [0, 1], [0,0]])\n",
    "y = np.array( [4, 2, 3])\n",
    "\n",
    "\n",
    "# matrix inversion to find beta\n",
    "OLSbeta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "print(OLSbeta)\n",
    "# and then make the prediction\n",
    "ytildeOLS = X @ OLSbeta\n",
    "print(\"Training MSE for OLS\")\n",
    "print(MSE(y,ytildeOLS))\n",
    "ypredictOLS = X @ OLSbeta\n",
    "\n",
    "# Repeat now for Ridge regression and various values of the regularization parameter\n",
    "I = np.eye(2,2)\n",
    "# Decide which values of lambda to use\n",
    "nlambdas = 100\n",
    "MSERidgePredict = np.zeros(nlambdas)\n",
    "MSELassoPredict = np.zeros(nlambdas)\n",
    "lambdas = np.logspace(-4, 4, nlambdas)\n",
    "for i in range(nlambdas):\n",
    "    lmb = lambdas[i]\n",
    "    Ridgebeta = np.linalg.inv(X.T @ X+lmb*I) @ X.T @ y\n",
    "    print(Ridgebeta)\n",
    "    # and then make the prediction\n",
    "    ypredictRidge = X @ Ridgebeta\n",
    "    MSERidgePredict[i] = MSE(y,ypredictRidge)\n",
    "    RegLasso = linear_model.Lasso(lmb,fit_intercept=False)\n",
    "    RegLasso.fit(X,y)\n",
    "    ypredictLasso = RegLasso.predict(X)\n",
    "    print(RegLasso.coef_)\n",
    "    MSELassoPredict[i] = MSE(y,ypredictLasso)\n",
    "# Now plot the results\n",
    "plt.figure()\n",
    "plt.plot(np.log10(lambdas), MSERidgePredict, 'r--', label = 'MSE Ridge Train')\n",
    "plt.plot(np.log10(lambdas), MSELassoPredict, 'r--', label = 'MSE Lasso Train')\n",
    "plt.xlabel('log10(lambda)')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9861ae2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Another Example, now with a polynomial fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cc52080",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "\n",
    "def R2(y_data, y_model):\n",
    "    return 1 - np.sum((y_data - y_model) ** 2) / np.sum((y_data - np.mean(y_data)) ** 2)\n",
    "def MSE(y_data,y_model):\n",
    "    n = np.size(y_model)\n",
    "    return np.sum((y_data-y_model)**2)/n\n",
    "\n",
    "\n",
    "# A seed just to ensure that the random numbers are the same for every run.\n",
    "# Useful for eventual debugging.\n",
    "np.random.seed(3155)\n",
    "\n",
    "x = np.random.rand(100)\n",
    "y = 2.0+5*x*x+0.1*np.random.randn(100)\n",
    "\n",
    "# number of features p (here degree of polynomial\n",
    "p = 3\n",
    "#  The design matrix now as function of a given polynomial\n",
    "X = np.zeros((len(x),p))\n",
    "X[:,0] = 1.0\n",
    "X[:,1] = x\n",
    "X[:,2] = x*x\n",
    "# We split the data in test and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# matrix inversion to find beta\n",
    "OLSbeta = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ y_train\n",
    "print(OLSbeta)\n",
    "# and then make the prediction\n",
    "ytildeOLS = X_train @ OLSbeta\n",
    "print(\"Training MSE for OLS\")\n",
    "print(MSE(y_train,ytildeOLS))\n",
    "ypredictOLS = X_test @ OLSbeta\n",
    "print(\"Test MSE OLS\")\n",
    "print(MSE(y_test,ypredictOLS))\n",
    "\n",
    "# Repeat now for Lasso and Ridge regression and various values of the regularization parameter\n",
    "I = np.eye(p,p)\n",
    "# Decide which values of lambda to use\n",
    "nlambdas = 100\n",
    "MSEPredict = np.zeros(nlambdas)\n",
    "MSETrain = np.zeros(nlambdas)\n",
    "MSELassoPredict = np.zeros(nlambdas)\n",
    "MSELassoTrain = np.zeros(nlambdas)\n",
    "lambdas = np.logspace(-4, 4, nlambdas)\n",
    "for i in range(nlambdas):\n",
    "    lmb = lambdas[i]\n",
    "    Ridgebeta = np.linalg.inv(X_train.T @ X_train+lmb*I) @ X_train.T @ y_train\n",
    "    # include lasso using Scikit-Learn\n",
    "    RegLasso = linear_model.Lasso(lmb,fit_intercept=False)\n",
    "    RegLasso.fit(X_train,y_train)\n",
    "    # and then make the prediction\n",
    "    ytildeRidge = X_train @ Ridgebeta\n",
    "    ypredictRidge = X_test @ Ridgebeta\n",
    "    ytildeLasso = RegLasso.predict(X_train)\n",
    "    ypredictLasso = RegLasso.predict(X_test)\n",
    "    MSEPredict[i] = MSE(y_test,ypredictRidge)\n",
    "    MSETrain[i] = MSE(y_train,ytildeRidge)\n",
    "    MSELassoPredict[i] = MSE(y_test,ypredictLasso)\n",
    "    MSELassoTrain[i] = MSE(y_train,ytildeLasso)\n",
    "\n",
    "# Now plot the results\n",
    "plt.figure()\n",
    "plt.plot(np.log10(lambdas), MSETrain, label = 'MSE Ridge train')\n",
    "plt.plot(np.log10(lambdas), MSEPredict, 'r--', label = 'MSE Ridge Test')\n",
    "plt.plot(np.log10(lambdas), MSELassoTrain, label = 'MSE Lasso train')\n",
    "plt.plot(np.log10(lambdas), MSELassoPredict, 'r--', label = 'MSE Lasso Test')\n",
    "\n",
    "plt.xlabel('log10(lambda)')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

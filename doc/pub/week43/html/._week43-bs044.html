<!--
Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Week 43: Solving Differential Equations with Deep Learning and Dimensionality Reduction methods">

<title>Week 43: Solving Differential Equations with Deep Learning and Dimensionality Reduction methods</title>

<!-- Bootstrap style: bootstrap -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->

<style type="text/css">

/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}

/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>


</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Plans for week 43', 2, None, '___sec0'),
              ('Recurrent Neural Networks', 2, None, '___sec1'),
              ('Solving ODEs with Deep Learning', 2, None, '___sec2'),
              ('Ordinary Differential Equations', 2, None, '___sec3'),
              ('The trial solution', 2, None, '___sec4'),
              ('Minimization process', 2, None, '___sec5'),
              ('Minimizing the cost function using gradient descent and '
               'automatic differentiation',
               2,
               None,
               '___sec6'),
              ('Example: Exponential decay', 2, None, '___sec7'),
              ('The function to solve for', 2, None, '___sec8'),
              ('The trial solution', 2, None, '___sec9'),
              ('Setup of Network', 2, None, '___sec10'),
              ('Reformulating the problem', 2, None, '___sec11'),
              ('More technicalities', 2, None, '___sec12'),
              ('More details', 2, None, '___sec13'),
              ('A possible implementation of a neural network',
               2,
               None,
               '___sec14'),
              ('Technicalities', 2, None, '___sec15'),
              ('Final technicalities I', 2, None, '___sec16'),
              ('Final technicalities II', 2, None, '___sec17'),
              ('Final technicalities III', 2, None, '___sec18'),
              ('Final technicalities IV', 2, None, '___sec19'),
              ('Back propagation', 2, None, '___sec20'),
              ('Gradient descent', 2, None, '___sec21'),
              ('The code for solving the ODE', 2, None, '___sec22'),
              ('The network with one input layer, specified number of hidden '
               'layers, and one output layer',
               2,
               None,
               '___sec23'),
              ('Example: Population growth', 2, None, '___sec24'),
              ('Setting up the problem', 2, None, '___sec25'),
              ('The trial solution', 2, None, '___sec26'),
              ('The program using Autograd', 2, None, '___sec27'),
              ('Using forward Euler to solve the ODE', 2, None, '___sec28'),
              ('Example: Solving the one dimensional Poisson equation',
               2,
               None,
               '___sec29'),
              ('The specific equation to solve for', 2, None, '___sec30'),
              ('Solving the equation using Autograd', 2, None, '___sec31'),
              ('Comparing with a numerical scheme', 2, None, '___sec32'),
              ('Setting up the code', 2, None, '___sec33'),
              ('Partial Differential Equations', 2, None, '___sec34'),
              ('Type of problem', 2, None, '___sec35'),
              ('Network requirements', 2, None, '___sec36'),
              ('More details', 2, None, '___sec37'),
              ('Example: The diffusion equation', 2, None, '___sec38'),
              ('Defining the problem', 2, None, '___sec39'),
              ('Setting up the network using Autograd', 2, None, '___sec40'),
              ('Setting up the network using Autograd; The trial solution',
               2,
               None,
               '___sec41'),
              ('Why the jacobian?', 2, None, '___sec42'),
              ('Setting up the network using Autograd; The full program',
               2,
               None,
               '___sec43'),
              ('Example: Solving the wave equation with Neural Networks',
               2,
               None,
               '___sec44'),
              ('The problem to solve for', 2, None, '___sec45'),
              ('The trial solution', 2, None, '___sec46'),
              ('The analytical solution', 2, None, '___sec47'),
              ('Solving the wave equation - the full program using Autograd',
               2,
               None,
               '___sec48'),
              ('Resources on differential equations and deep learning',
               2,
               None,
               '___sec49'),
              ('Friday, Principal Component Analysis', 2, None, '___sec50'),
              ('Basic ideas of the Principal Component Analysis (PCA)',
               2,
               None,
               '___sec51'),
              ('Introducing the Covariance and Correlation functions',
               2,
               None,
               '___sec52'),
              ('Correlation Function and Design/Feature Matrix',
               2,
               None,
               '___sec53'),
              ('Covariance Matrix Examples', 2, None, '___sec54'),
              ('Correlation Matrix', 2, None, '___sec55'),
              ('Correlation Matrix with Pandas', 2, None, '___sec56'),
              ('Correlation Matrix with Pandas and the Franke function',
               2,
               None,
               '___sec57'),
              ('Rewriting the Covariance and/or Correlation Matrix',
               2,
               None,
               '___sec58'),
              ('Towards the PCA theorem', 2, None, '___sec59'),
              ('The Algorithm before the Theorem', 2, None, '___sec60'),
              ('Writing our own PCA code', 2, None, '___sec61'),
              ('Compute the sample mean and center the data',
               3,
               None,
               '___sec62'),
              ('Compute the sample covariance', 3, None, '___sec63'),
              ('Diagonalize the sample covariance matrix to obtain the '
               'principal components',
               3,
               None,
               '___sec64'),
              ('Classical PCA Theorem', 2, None, '___sec65'),
              ('Proof of the PCA Theorem', 2, None, '___sec66'),
              ('PCA Proof continued', 2, None, '___sec67'),
              ('The final step', 2, None, '___sec68'),
              ('Geometric Interpretation and link with Singular Value '
               'Decomposition',
               2,
               None,
               '___sec69'),
              ('Principal Component Analysis', 2, None, '___sec70'),
              ('PCA and scikit-learn', 2, None, '___sec71'),
              ('Back to the Cancer Data', 2, None, '___sec72'),
              ('More on the PCA', 2, None, '___sec73'),
              ('Incremental PCA', 2, None, '___sec74'),
              ('Randomized PCA', 2, None, '___sec75'),
              ('Kernel PCA', 2, None, '___sec76'),
              ('LLE', 2, None, '___sec77'),
              ('Other techniques', 2, None, '___sec78')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="week43-bs.html">Week 43: Solving Differential Equations with Deep Learning and Dimensionality Reduction methods</a>
  </div>

  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="._week43-bs001.html#___sec0" style="font-size: 80%;"><b>Plans for week 43</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs002.html#___sec1" style="font-size: 80%;"><b>Recurrent Neural Networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs003.html#___sec2" style="font-size: 80%;"><b>Solving ODEs with Deep Learning</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs004.html#___sec3" style="font-size: 80%;"><b>Ordinary Differential Equations</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs005.html#___sec4" style="font-size: 80%;"><b>The trial solution</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs006.html#___sec5" style="font-size: 80%;"><b>Minimization process</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs007.html#___sec6" style="font-size: 80%;"><b>Minimizing the cost function using gradient descent and automatic differentiation</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs008.html#___sec7" style="font-size: 80%;"><b>Example: Exponential decay</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs009.html#___sec8" style="font-size: 80%;"><b>The function to solve for</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs010.html#___sec9" style="font-size: 80%;"><b>The trial solution</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs011.html#___sec10" style="font-size: 80%;"><b>Setup of Network</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs012.html#___sec11" style="font-size: 80%;"><b>Reformulating the problem</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs013.html#___sec12" style="font-size: 80%;"><b>More technicalities</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs014.html#___sec13" style="font-size: 80%;"><b>More details</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs015.html#___sec14" style="font-size: 80%;"><b>A possible implementation of a neural network</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs016.html#___sec15" style="font-size: 80%;"><b>Technicalities</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs017.html#___sec16" style="font-size: 80%;"><b>Final technicalities I</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs018.html#___sec17" style="font-size: 80%;"><b>Final technicalities II</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs019.html#___sec18" style="font-size: 80%;"><b>Final technicalities III</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs020.html#___sec19" style="font-size: 80%;"><b>Final technicalities IV</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs021.html#___sec20" style="font-size: 80%;"><b>Back propagation</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs022.html#___sec21" style="font-size: 80%;"><b>Gradient descent</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs023.html#___sec22" style="font-size: 80%;"><b>The code for solving the ODE</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs024.html#___sec23" style="font-size: 80%;"><b>The network with one input layer, specified number of hidden layers, and one output layer</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs025.html#___sec24" style="font-size: 80%;"><b>Example: Population growth</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs026.html#___sec25" style="font-size: 80%;"><b>Setting up the problem</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs027.html#___sec26" style="font-size: 80%;"><b>The trial solution</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs028.html#___sec27" style="font-size: 80%;"><b>The program using Autograd</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs029.html#___sec28" style="font-size: 80%;"><b>Using forward Euler to solve the ODE</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs030.html#___sec29" style="font-size: 80%;"><b>Example: Solving the one dimensional Poisson equation</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs031.html#___sec30" style="font-size: 80%;"><b>The specific equation to solve for</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs032.html#___sec31" style="font-size: 80%;"><b>Solving the equation using Autograd</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs033.html#___sec32" style="font-size: 80%;"><b>Comparing with a numerical scheme</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs034.html#___sec33" style="font-size: 80%;"><b>Setting up the code</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs035.html#___sec34" style="font-size: 80%;"><b>Partial Differential Equations</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs036.html#___sec35" style="font-size: 80%;"><b>Type of problem</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs037.html#___sec36" style="font-size: 80%;"><b>Network requirements</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs038.html#___sec37" style="font-size: 80%;"><b>More details</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs039.html#___sec38" style="font-size: 80%;"><b>Example: The diffusion equation</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs040.html#___sec39" style="font-size: 80%;"><b>Defining the problem</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs041.html#___sec40" style="font-size: 80%;"><b>Setting up the network using Autograd</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs042.html#___sec41" style="font-size: 80%;"><b>Setting up the network using Autograd; The trial solution</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs043.html#___sec42" style="font-size: 80%;"><b>Why the jacobian?</b></a></li>
     <!-- navigation toc: --> <li><a href="#___sec43" style="font-size: 80%;"><b>Setting up the network using Autograd; The full program</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs045.html#___sec44" style="font-size: 80%;"><b>Example: Solving the wave equation with Neural Networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs046.html#___sec45" style="font-size: 80%;"><b>The problem to solve for</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs047.html#___sec46" style="font-size: 80%;"><b>The trial solution</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs048.html#___sec47" style="font-size: 80%;"><b>The analytical solution</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs049.html#___sec48" style="font-size: 80%;"><b>Solving the wave equation - the full program using Autograd</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs050.html#___sec49" style="font-size: 80%;"><b>Resources on differential equations and deep learning</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs051.html#___sec50" style="font-size: 80%;"><b>Friday, Principal Component Analysis</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs052.html#___sec51" style="font-size: 80%;"><b>Basic ideas of the Principal Component Analysis (PCA)</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs053.html#___sec52" style="font-size: 80%;"><b>Introducing the Covariance and Correlation functions</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs054.html#___sec53" style="font-size: 80%;"><b>Correlation Function and Design/Feature Matrix</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs055.html#___sec54" style="font-size: 80%;"><b>Covariance Matrix Examples</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs056.html#___sec55" style="font-size: 80%;"><b>Correlation Matrix</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs057.html#___sec56" style="font-size: 80%;"><b>Correlation Matrix with Pandas</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs058.html#___sec57" style="font-size: 80%;"><b>Correlation Matrix with Pandas and the Franke function</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs059.html#___sec58" style="font-size: 80%;"><b>Rewriting the Covariance and/or Correlation Matrix</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs060.html#___sec59" style="font-size: 80%;"><b>Towards the PCA theorem</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs061.html#___sec60" style="font-size: 80%;"><b>The Algorithm before the Theorem</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs062.html#___sec61" style="font-size: 80%;"><b>Writing our own PCA code</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs062.html#___sec62" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Compute the sample mean and center the data</a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs062.html#___sec63" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Compute the sample covariance</a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs062.html#___sec64" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Diagonalize the sample covariance matrix to obtain the principal components</a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs063.html#___sec65" style="font-size: 80%;"><b>Classical PCA Theorem</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs064.html#___sec66" style="font-size: 80%;"><b>Proof of the PCA Theorem</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs065.html#___sec67" style="font-size: 80%;"><b>PCA Proof continued</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs066.html#___sec68" style="font-size: 80%;"><b>The final step</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs067.html#___sec69" style="font-size: 80%;"><b>Geometric Interpretation and link with Singular Value Decomposition</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs068.html#___sec70" style="font-size: 80%;"><b>Principal Component Analysis</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs069.html#___sec71" style="font-size: 80%;"><b>PCA and scikit-learn</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs070.html#___sec72" style="font-size: 80%;"><b>Back to the Cancer Data</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs071.html#___sec73" style="font-size: 80%;"><b>More on the PCA</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs072.html#___sec74" style="font-size: 80%;"><b>Incremental PCA</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs073.html#___sec75" style="font-size: 80%;"><b>Randomized PCA</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs074.html#___sec76" style="font-size: 80%;"><b>Kernel PCA</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs075.html#___sec77" style="font-size: 80%;"><b>LLE</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs076.html#___sec78" style="font-size: 80%;"><b>Other techniques</b></a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->

<div class="container">

<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->

<a name="part0044"></a>
<!-- !split -->

<h2 id="___sec43" class="anchor">Setting up the network using Autograd; The full program </h2>

<p>
Having set up the network, along with the trial solution and cost function, we can now see how the deep neural network performs by comparing the results to the analytical solution.

<p>
The analytical solution of our problem is

$$
g(x,t) = \exp(-\pi^2 t)\sin(\pi x)
$$

<p>
A possible way to implement a neural network solving the PDE, is given below.
Be aware, though, that it is fairly slow for the parameters used.
A better result is possible, but requires more iterations, and thus longer time to complete.

<p>
Indeed, the program below is not optimal in its implementation, but rather serves as an example on how to implement and use a neural network to solve a PDE.
Using TensorFlow results in a much better execution time. Try it!

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">autograd.numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">autograd</span> <span style="color: #008000; font-weight: bold">import</span> jacobian,hessian,grad
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">autograd.numpy.random</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">npr</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">matplotlib</span> <span style="color: #008000; font-weight: bold">import</span> cm
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">matplotlib</span> <span style="color: #008000; font-weight: bold">import</span> pyplot <span style="color: #008000; font-weight: bold">as</span> plt
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">mpl_toolkits.mplot3d</span> <span style="color: #008000; font-weight: bold">import</span> axes3d

<span style="color: #408080; font-style: italic">## Set up the network</span>

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">sigmoid</span>(z):
    <span style="color: #008000; font-weight: bold">return</span> <span style="color: #666666">1/</span>(<span style="color: #666666">1</span> <span style="color: #666666">+</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>z))

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">deep_neural_network</span>(deep_params, x):
    <span style="color: #408080; font-style: italic"># x is now a point and a 1D numpy array; make it a column vector</span>
    num_coordinates <span style="color: #666666">=</span> np<span style="color: #666666">.</span>size(x,<span style="color: #666666">0</span>)
    x <span style="color: #666666">=</span> x<span style="color: #666666">.</span>reshape(num_coordinates,<span style="color: #666666">-1</span>)

    num_points <span style="color: #666666">=</span> np<span style="color: #666666">.</span>size(x,<span style="color: #666666">1</span>)

    <span style="color: #408080; font-style: italic"># N_hidden is the number of hidden layers</span>
    N_hidden <span style="color: #666666">=</span> np<span style="color: #666666">.</span>size(deep_params) <span style="color: #666666">-</span> <span style="color: #666666">1</span> <span style="color: #408080; font-style: italic"># -1 since params consist of parameters to all the hidden layers AND the output layer</span>

    <span style="color: #408080; font-style: italic"># Assume that the input layer does nothing to the input x</span>
    x_input <span style="color: #666666">=</span> x
    x_prev <span style="color: #666666">=</span> x_input

    <span style="color: #408080; font-style: italic">## Hidden layers:</span>

    <span style="color: #008000; font-weight: bold">for</span> l <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(N_hidden):
        <span style="color: #408080; font-style: italic"># From the list of parameters P; find the correct weigths and bias for this layer</span>
        w_hidden <span style="color: #666666">=</span> deep_params[l]

        <span style="color: #408080; font-style: italic"># Add a row of ones to include bias</span>
        x_prev <span style="color: #666666">=</span> np<span style="color: #666666">.</span>concatenate((np<span style="color: #666666">.</span>ones((<span style="color: #666666">1</span>,num_points)), x_prev ), axis <span style="color: #666666">=</span> <span style="color: #666666">0</span>)

        z_hidden <span style="color: #666666">=</span> np<span style="color: #666666">.</span>matmul(w_hidden, x_prev)
        x_hidden <span style="color: #666666">=</span> sigmoid(z_hidden)

        <span style="color: #408080; font-style: italic"># Update x_prev such that next layer can use the output from this layer</span>
        x_prev <span style="color: #666666">=</span> x_hidden

    <span style="color: #408080; font-style: italic">## Output layer:</span>

    <span style="color: #408080; font-style: italic"># Get the weights and bias for this layer</span>
    w_output <span style="color: #666666">=</span> deep_params[<span style="color: #666666">-1</span>]

    <span style="color: #408080; font-style: italic"># Include bias:</span>
    x_prev <span style="color: #666666">=</span> np<span style="color: #666666">.</span>concatenate((np<span style="color: #666666">.</span>ones((<span style="color: #666666">1</span>,num_points)), x_prev), axis <span style="color: #666666">=</span> <span style="color: #666666">0</span>)

    z_output <span style="color: #666666">=</span> np<span style="color: #666666">.</span>matmul(w_output, x_prev)
    x_output <span style="color: #666666">=</span> z_output

    <span style="color: #008000; font-weight: bold">return</span> x_output[<span style="color: #666666">0</span>][<span style="color: #666666">0</span>]

<span style="color: #408080; font-style: italic">## Define the trial solution and cost function</span>
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">u</span>(x):
    <span style="color: #008000; font-weight: bold">return</span> np<span style="color: #666666">.</span>sin(np<span style="color: #666666">.</span>pi<span style="color: #666666">*</span>x)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">g_trial</span>(point,P):
    x,t <span style="color: #666666">=</span> point
    <span style="color: #008000; font-weight: bold">return</span> (<span style="color: #666666">1-</span>t)<span style="color: #666666">*</span>u(x) <span style="color: #666666">+</span> x<span style="color: #666666">*</span>(<span style="color: #666666">1-</span>x)<span style="color: #666666">*</span>t<span style="color: #666666">*</span>deep_neural_network(P,point)

<span style="color: #408080; font-style: italic"># The right side of the ODE:</span>
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">f</span>(point):
    <span style="color: #008000; font-weight: bold">return</span> <span style="color: #666666">0.</span>

<span style="color: #408080; font-style: italic"># The cost function:</span>
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">cost_function</span>(P, x, t):
    cost_sum <span style="color: #666666">=</span> <span style="color: #666666">0</span>

    g_t_jacobian_func <span style="color: #666666">=</span> jacobian(g_trial)
    g_t_hessian_func <span style="color: #666666">=</span> hessian(g_trial)

    <span style="color: #008000; font-weight: bold">for</span> x_ <span style="color: #AA22FF; font-weight: bold">in</span> x:
        <span style="color: #008000; font-weight: bold">for</span> t_ <span style="color: #AA22FF; font-weight: bold">in</span> t:
            point <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array([x_,t_])

            g_t <span style="color: #666666">=</span> g_trial(point,P)
            g_t_jacobian <span style="color: #666666">=</span> g_t_jacobian_func(point,P)
            g_t_hessian <span style="color: #666666">=</span> g_t_hessian_func(point,P)

            g_t_dt <span style="color: #666666">=</span> g_t_jacobian[<span style="color: #666666">1</span>]
            g_t_d2x <span style="color: #666666">=</span> g_t_hessian[<span style="color: #666666">0</span>][<span style="color: #666666">0</span>]

            func <span style="color: #666666">=</span> f(point)

            err_sqr <span style="color: #666666">=</span> ( (g_t_dt <span style="color: #666666">-</span> g_t_d2x) <span style="color: #666666">-</span> func)<span style="color: #666666">**2</span>
            cost_sum <span style="color: #666666">+=</span> err_sqr

    <span style="color: #008000; font-weight: bold">return</span> cost_sum <span style="color: #666666">/</span>( np<span style="color: #666666">.</span>size(x)<span style="color: #666666">*</span>np<span style="color: #666666">.</span>size(t) )

<span style="color: #408080; font-style: italic">## For comparison, define the analytical solution</span>
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">g_analytic</span>(point):
    x,t <span style="color: #666666">=</span> point
    <span style="color: #008000; font-weight: bold">return</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>np<span style="color: #666666">.</span>pi<span style="color: #666666">**2*</span>t)<span style="color: #666666">*</span>np<span style="color: #666666">.</span>sin(np<span style="color: #666666">.</span>pi<span style="color: #666666">*</span>x)

<span style="color: #408080; font-style: italic">## Set up a function for training the network to solve for the equation</span>
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">solve_pde_deep_neural_network</span>(x,t, num_neurons, num_iter, lmb):
    <span style="color: #408080; font-style: italic">## Set up initial weigths and biases</span>
    N_hidden <span style="color: #666666">=</span> np<span style="color: #666666">.</span>size(num_neurons)

    <span style="color: #408080; font-style: italic">## Set up initial weigths and biases</span>

    <span style="color: #408080; font-style: italic"># Initialize the list of parameters:</span>
    P <span style="color: #666666">=</span> [<span style="color: #008000; font-weight: bold">None</span>]<span style="color: #666666">*</span>(N_hidden <span style="color: #666666">+</span> <span style="color: #666666">1</span>) <span style="color: #408080; font-style: italic"># + 1 to include the output layer</span>

    P[<span style="color: #666666">0</span>] <span style="color: #666666">=</span> npr<span style="color: #666666">.</span>randn(num_neurons[<span style="color: #666666">0</span>], <span style="color: #666666">2</span> <span style="color: #666666">+</span> <span style="color: #666666">1</span> ) <span style="color: #408080; font-style: italic"># 2 since we have two points, +1 to include bias</span>
    <span style="color: #008000; font-weight: bold">for</span> l <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #666666">1</span>,N_hidden):
        P[l] <span style="color: #666666">=</span> npr<span style="color: #666666">.</span>randn(num_neurons[l], num_neurons[l<span style="color: #666666">-1</span>] <span style="color: #666666">+</span> <span style="color: #666666">1</span>) <span style="color: #408080; font-style: italic"># +1 to include bias</span>

    <span style="color: #408080; font-style: italic"># For the output layer</span>
    P[<span style="color: #666666">-1</span>] <span style="color: #666666">=</span> npr<span style="color: #666666">.</span>randn(<span style="color: #666666">1</span>, num_neurons[<span style="color: #666666">-1</span>] <span style="color: #666666">+</span> <span style="color: #666666">1</span> ) <span style="color: #408080; font-style: italic"># +1 since bias is included</span>

    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Initial cost: &#39;</span>,cost_function(P, x, t))

    cost_function_grad <span style="color: #666666">=</span> grad(cost_function,<span style="color: #666666">0</span>)

    <span style="color: #408080; font-style: italic"># Let the update be done num_iter times</span>
    <span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(num_iter):
        cost_grad <span style="color: #666666">=</span>  cost_function_grad(P, x , t)

        <span style="color: #008000; font-weight: bold">for</span> l <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(N_hidden<span style="color: #666666">+1</span>):
            P[l] <span style="color: #666666">=</span> P[l] <span style="color: #666666">-</span> lmb <span style="color: #666666">*</span> cost_grad[l]

    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Final cost: &#39;</span>,cost_function(P, x, t))

    <span style="color: #008000; font-weight: bold">return</span> P

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #19177C">__name__</span> <span style="color: #666666">==</span> <span style="color: #BA2121">&#39;__main__&#39;</span>:
    <span style="color: #408080; font-style: italic">### Use the neural network:</span>
    npr<span style="color: #666666">.</span>seed(<span style="color: #666666">15</span>)

    <span style="color: #408080; font-style: italic">## Decide the vales of arguments to the function to solve</span>
    Nx <span style="color: #666666">=</span> <span style="color: #666666">10</span>; Nt <span style="color: #666666">=</span> <span style="color: #666666">10</span>
    x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(<span style="color: #666666">0</span>, <span style="color: #666666">1</span>, Nx)
    t <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(<span style="color: #666666">0</span>,<span style="color: #666666">1</span>,Nt)

    <span style="color: #408080; font-style: italic">## Set up the parameters for the network</span>
    num_hidden_neurons <span style="color: #666666">=</span> [<span style="color: #666666">100</span>, <span style="color: #666666">25</span>]
    num_iter <span style="color: #666666">=</span> <span style="color: #666666">250</span>
    lmb <span style="color: #666666">=</span> <span style="color: #666666">0.01</span>

    P <span style="color: #666666">=</span> solve_pde_deep_neural_network(x,t, num_hidden_neurons, num_iter, lmb)

    <span style="color: #408080; font-style: italic">## Store the results</span>
    g_dnn_ag <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros((Nx, Nt))
    G_analytical <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros((Nx, Nt))
    <span style="color: #008000; font-weight: bold">for</span> i,x_ <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">enumerate</span>(x):
        <span style="color: #008000; font-weight: bold">for</span> j, t_ <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">enumerate</span>(t):
            point <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array([x_, t_])
            g_dnn_ag[i,j] <span style="color: #666666">=</span> g_trial(point,P)

            G_analytical[i,j] <span style="color: #666666">=</span> g_analytic(point)

    <span style="color: #408080; font-style: italic"># Find the map difference between the analytical and the computed solution</span>
    diff_ag <span style="color: #666666">=</span> np<span style="color: #666666">.</span>abs(g_dnn_ag <span style="color: #666666">-</span> G_analytical)
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Max absolute difference between the analytical solution and the network: </span><span style="color: #BB6688; font-weight: bold">%g</span><span style="color: #BA2121">&#39;</span><span style="color: #666666">%</span>np<span style="color: #666666">.</span>max(diff_ag))

    <span style="color: #408080; font-style: italic">## Plot the solutions in two dimensions, that being in position and time</span>

    T,X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>meshgrid(t,x)

    fig <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(<span style="color: #666666">10</span>,<span style="color: #666666">10</span>))
    ax <span style="color: #666666">=</span> fig<span style="color: #666666">.</span>gca(projection<span style="color: #666666">=</span><span style="color: #BA2121">&#39;3d&#39;</span>)
    ax<span style="color: #666666">.</span>set_title(<span style="color: #BA2121">&#39;Solution from the deep neural network w/ </span><span style="color: #BB6688; font-weight: bold">%d</span><span style="color: #BA2121"> layer&#39;</span><span style="color: #666666">%</span><span style="color: #008000">len</span>(num_hidden_neurons))
    s <span style="color: #666666">=</span> ax<span style="color: #666666">.</span>plot_surface(T,X,g_dnn_ag,linewidth<span style="color: #666666">=0</span>,antialiased<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>,cmap<span style="color: #666666">=</span>cm<span style="color: #666666">.</span>viridis)
    ax<span style="color: #666666">.</span>set_xlabel(<span style="color: #BA2121">&#39;Time $t$&#39;</span>)
    ax<span style="color: #666666">.</span>set_ylabel(<span style="color: #BA2121">&#39;Position $x$&#39;</span>);


    fig <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(<span style="color: #666666">10</span>,<span style="color: #666666">10</span>))
    ax <span style="color: #666666">=</span> fig<span style="color: #666666">.</span>gca(projection<span style="color: #666666">=</span><span style="color: #BA2121">&#39;3d&#39;</span>)
    ax<span style="color: #666666">.</span>set_title(<span style="color: #BA2121">&#39;Analytical solution&#39;</span>)
    s <span style="color: #666666">=</span> ax<span style="color: #666666">.</span>plot_surface(T,X,G_analytical,linewidth<span style="color: #666666">=0</span>,antialiased<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>,cmap<span style="color: #666666">=</span>cm<span style="color: #666666">.</span>viridis)
    ax<span style="color: #666666">.</span>set_xlabel(<span style="color: #BA2121">&#39;Time $t$&#39;</span>)
    ax<span style="color: #666666">.</span>set_ylabel(<span style="color: #BA2121">&#39;Position $x$&#39;</span>);

    fig <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(<span style="color: #666666">10</span>,<span style="color: #666666">10</span>))
    ax <span style="color: #666666">=</span> fig<span style="color: #666666">.</span>gca(projection<span style="color: #666666">=</span><span style="color: #BA2121">&#39;3d&#39;</span>)
    ax<span style="color: #666666">.</span>set_title(<span style="color: #BA2121">&#39;Difference&#39;</span>)
    s <span style="color: #666666">=</span> ax<span style="color: #666666">.</span>plot_surface(T,X,diff_ag,linewidth<span style="color: #666666">=0</span>,antialiased<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>,cmap<span style="color: #666666">=</span>cm<span style="color: #666666">.</span>viridis)
    ax<span style="color: #666666">.</span>set_xlabel(<span style="color: #BA2121">&#39;Time $t$&#39;</span>)
    ax<span style="color: #666666">.</span>set_ylabel(<span style="color: #BA2121">&#39;Position $x$&#39;</span>);

    <span style="color: #408080; font-style: italic">## Take some slices of the 3D plots just to see the solutions at particular times</span>
    indx1 <span style="color: #666666">=</span> <span style="color: #666666">0</span>
    indx2 <span style="color: #666666">=</span> <span style="color: #008000">int</span>(Nt<span style="color: #666666">/2</span>)
    indx3 <span style="color: #666666">=</span> Nt<span style="color: #666666">-1</span>

    t1 <span style="color: #666666">=</span> t[indx1]
    t2 <span style="color: #666666">=</span> t[indx2]
    t3 <span style="color: #666666">=</span> t[indx3]

    <span style="color: #408080; font-style: italic"># Slice the results from the DNN</span>
    res1 <span style="color: #666666">=</span> g_dnn_ag[:,indx1]
    res2 <span style="color: #666666">=</span> g_dnn_ag[:,indx2]
    res3 <span style="color: #666666">=</span> g_dnn_ag[:,indx3]

    <span style="color: #408080; font-style: italic"># Slice the analytical results</span>
    res_analytical1 <span style="color: #666666">=</span> G_analytical[:,indx1]
    res_analytical2 <span style="color: #666666">=</span> G_analytical[:,indx2]
    res_analytical3 <span style="color: #666666">=</span> G_analytical[:,indx3]

    <span style="color: #408080; font-style: italic"># Plot the slices</span>
    plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(<span style="color: #666666">10</span>,<span style="color: #666666">10</span>))
    plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&quot;Computed solutions at time = </span><span style="color: #BB6688; font-weight: bold">%g</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">%</span>t1)
    plt<span style="color: #666666">.</span>plot(x, res1)
    plt<span style="color: #666666">.</span>plot(x,res_analytical1)
    plt<span style="color: #666666">.</span>legend([<span style="color: #BA2121">&#39;dnn&#39;</span>,<span style="color: #BA2121">&#39;analytical&#39;</span>])

    plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(<span style="color: #666666">10</span>,<span style="color: #666666">10</span>))
    plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&quot;Computed solutions at time = </span><span style="color: #BB6688; font-weight: bold">%g</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">%</span>t2)
    plt<span style="color: #666666">.</span>plot(x, res2)
    plt<span style="color: #666666">.</span>plot(x,res_analytical2)
    plt<span style="color: #666666">.</span>legend([<span style="color: #BA2121">&#39;dnn&#39;</span>,<span style="color: #BA2121">&#39;analytical&#39;</span>])

    plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(<span style="color: #666666">10</span>,<span style="color: #666666">10</span>))
    plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&quot;Computed solutions at time = </span><span style="color: #BB6688; font-weight: bold">%g</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">%</span>t3)
    plt<span style="color: #666666">.</span>plot(x, res3)
    plt<span style="color: #666666">.</span>plot(x,res_analytical3)
    plt<span style="color: #666666">.</span>legend([<span style="color: #BA2121">&#39;dnn&#39;</span>,<span style="color: #BA2121">&#39;analytical&#39;</span>])

    plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
<p>
<!-- navigation buttons at the bottom of the page -->
<ul class="pagination">
<li><a href="._week43-bs043.html">&laquo;</a></li>
  <li><a href="._week43-bs000.html">1</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week43-bs036.html">37</a></li>
  <li><a href="._week43-bs037.html">38</a></li>
  <li><a href="._week43-bs038.html">39</a></li>
  <li><a href="._week43-bs039.html">40</a></li>
  <li><a href="._week43-bs040.html">41</a></li>
  <li><a href="._week43-bs041.html">42</a></li>
  <li><a href="._week43-bs042.html">43</a></li>
  <li><a href="._week43-bs043.html">44</a></li>
  <li class="active"><a href="._week43-bs044.html">45</a></li>
  <li><a href="._week43-bs045.html">46</a></li>
  <li><a href="._week43-bs046.html">47</a></li>
  <li><a href="._week43-bs047.html">48</a></li>
  <li><a href="._week43-bs048.html">49</a></li>
  <li><a href="._week43-bs049.html">50</a></li>
  <li><a href="._week43-bs050.html">51</a></li>
  <li><a href="._week43-bs051.html">52</a></li>
  <li><a href="._week43-bs052.html">53</a></li>
  <li><a href="._week43-bs053.html">54</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week43-bs076.html">77</a></li>
  <li><a href="._week43-bs045.html">&raquo;</a></li>
</ul>
<!-- ------------------- end of main content --------------- -->

</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>

<!-- Bootstrap footer
<footer>
<a href="http://..."><img width="250" align=right src="http://..."></a>
</footer>
-->


<center style="font-size:80%">
<!-- copyright only on the titlepage -->
</center>


</body>
</html>
    


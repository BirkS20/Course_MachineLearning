<!--
Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Week 43: Solving Differential Equations with Deep Learning and Dimensionality Reduction methods">

<title>Week 43: Solving Differential Equations with Deep Learning and Dimensionality Reduction methods</title>

<!-- Bootstrap style: bootstrap -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->

<style type="text/css">

/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}

/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>


</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Recurrent Neural Networks', 2, None, '___sec0'),
              ('Solving ODEs with Deep Learning', 2, None, '___sec1'),
              ('Ordinary Differential Equations', 2, None, '___sec2'),
              ('The trial solution', 2, None, '___sec3'),
              ('Minimization process', 2, None, '___sec4'),
              ('Minimizing the cost function using gradient descent and '
               'automatic differentiation',
               2,
               None,
               '___sec5'),
              ('Example: Exponential decay', 2, None, '___sec6'),
              ('The function to solve for', 2, None, '___sec7'),
              ('The trial solution', 2, None, '___sec8'),
              ('Setup of Network', 2, None, '___sec9'),
              ('Reformulating the problem', 2, None, '___sec10'),
              ('More technicalities', 2, None, '___sec11'),
              ('More details', 2, None, '___sec12'),
              ('A possible implementation of a neural network',
               2,
               None,
               '___sec13'),
              ('Technicalities', 2, None, '___sec14'),
              ('Final technicalities I', 2, None, '___sec15'),
              ('Final technicalities II', 2, None, '___sec16'),
              ('Final technicalities III', 2, None, '___sec17'),
              ('Final technicalities IV', 2, None, '___sec18'),
              ('Back propagation', 2, None, '___sec19'),
              ('Gradient descent', 2, None, '___sec20'),
              ('The code for solving the ODE', 2, None, '___sec21'),
              ('The network with one input layer, specified number of hidden '
               'layers, and one output layer',
               2,
               None,
               '___sec22'),
              ('Example: Population growth', 2, None, '___sec23'),
              ('Setting up the problem', 2, None, '___sec24'),
              ('The trial solution', 2, None, '___sec25'),
              ('The program using Autograd', 2, None, '___sec26'),
              ('Using forward Euler to solve the ODE', 2, None, '___sec27'),
              ('Example: Solving the one dimensional Poisson equation',
               2,
               None,
               '___sec28'),
              ('The specific equation to solve for', 2, None, '___sec29'),
              ('Solving the equation using Autograd', 2, None, '___sec30'),
              ('Comparing with a numerical scheme', 2, None, '___sec31'),
              ('Setting up the code', 2, None, '___sec32'),
              ('Partial Differential Equations', 2, None, '___sec33'),
              ('Type of problem', 2, None, '___sec34'),
              ('Network requirements', 2, None, '___sec35'),
              ('More details', 2, None, '___sec36'),
              ('Example: The diffusion equation', 2, None, '___sec37'),
              ('Defining the problem', 2, None, '___sec38'),
              ('Setting up the network using Autograd', 2, None, '___sec39'),
              ('Setting up the network using Autograd; The trial solution',
               2,
               None,
               '___sec40'),
              ('Why the jacobian?', 2, None, '___sec41'),
              ('Setting up the network using Autograd; The full program',
               2,
               None,
               '___sec42'),
              ('Example: Solving the wave equation with Neural Networks',
               2,
               None,
               '___sec43'),
              ('The problem to solve for', 2, None, '___sec44'),
              ('The trial solution', 2, None, '___sec45'),
              ('The analytical solution', 2, None, '___sec46'),
              ('Solving the wave equation - the full program using Autograd',
               2,
               None,
               '___sec47'),
              ('Resources on differential equations and deep learning',
               2,
               None,
               '___sec48'),
              ('Friday, Principal Component Analysis', 2, None, '___sec49'),
              ('Basic ideas of the Principal Component Analysis (PCA)',
               2,
               None,
               '___sec50'),
              ('Introducing the Covariance and Correlation functions',
               2,
               None,
               '___sec51'),
              ('Correlation Function and Design/Feature Matrix',
               2,
               None,
               '___sec52'),
              ('Covariance Matrix Examples', 2, None, '___sec53'),
              ('Correlation Matrix', 2, None, '___sec54'),
              ('Correlation Matrix with Pandas', 2, None, '___sec55'),
              ('Correlation Matrix with Pandas and the Franke function',
               2,
               None,
               '___sec56'),
              ('Rewriting the Covariance and/or Correlation Matrix',
               2,
               None,
               '___sec57'),
              ('Towards the PCA theorem', 2, None, '___sec58'),
              ('The Algorithm before the Theorem', 2, None, '___sec59'),
              ('Writing our own PCA code', 2, None, '___sec60'),
              ('Compute the sample mean and center the data',
               3,
               None,
               '___sec61'),
              ('Compute the sample covariance', 3, None, '___sec62'),
              ('Diagonalize the sample covariance matrix to obtain the '
               'principal components',
               3,
               None,
               '___sec63'),
              ('Classical PCA Theorem', 2, None, '___sec64'),
              ('Proof of the PCA Theorem', 2, None, '___sec65'),
              ('PCA Proof continued', 2, None, '___sec66'),
              ('The final step', 2, None, '___sec67'),
              ('Geometric Interpretation and link with Singular Value '
               'Decomposition',
               2,
               None,
               '___sec68'),
              ('Principal Component Analysis', 2, None, '___sec69'),
              ('PCA and scikit-learn', 2, None, '___sec70'),
              ('Back to the Cancer Data', 2, None, '___sec71'),
              ('More on the PCA', 2, None, '___sec72'),
              ('Incremental PCA', 2, None, '___sec73'),
              ('Randomized PCA', 2, None, '___sec74'),
              ('Kernel PCA', 2, None, '___sec75'),
              ('LLE', 2, None, '___sec76'),
              ('Other techniques', 2, None, '___sec77')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="week43-bs.html">Week 43: Solving Differential Equations with Deep Learning and Dimensionality Reduction methods</a>
  </div>

  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="._week43-bs002.html#___sec0" style="font-size: 80%;"><b>Recurrent Neural Networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs003.html#___sec1" style="font-size: 80%;"><b>Solving ODEs with Deep Learning</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs004.html#___sec2" style="font-size: 80%;"><b>Ordinary Differential Equations</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs005.html#___sec3" style="font-size: 80%;"><b>The trial solution</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs006.html#___sec4" style="font-size: 80%;"><b>Minimization process</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs007.html#___sec5" style="font-size: 80%;"><b>Minimizing the cost function using gradient descent and automatic differentiation</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs008.html#___sec6" style="font-size: 80%;"><b>Example: Exponential decay</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs009.html#___sec7" style="font-size: 80%;"><b>The function to solve for</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs010.html#___sec8" style="font-size: 80%;"><b>The trial solution</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs011.html#___sec9" style="font-size: 80%;"><b>Setup of Network</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs012.html#___sec10" style="font-size: 80%;"><b>Reformulating the problem</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs013.html#___sec11" style="font-size: 80%;"><b>More technicalities</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs014.html#___sec12" style="font-size: 80%;"><b>More details</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs015.html#___sec13" style="font-size: 80%;"><b>A possible implementation of a neural network</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs016.html#___sec14" style="font-size: 80%;"><b>Technicalities</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs017.html#___sec15" style="font-size: 80%;"><b>Final technicalities I</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs018.html#___sec16" style="font-size: 80%;"><b>Final technicalities II</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs019.html#___sec17" style="font-size: 80%;"><b>Final technicalities III</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs020.html#___sec18" style="font-size: 80%;"><b>Final technicalities IV</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs021.html#___sec19" style="font-size: 80%;"><b>Back propagation</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs022.html#___sec20" style="font-size: 80%;"><b>Gradient descent</b></a></li>
     <!-- navigation toc: --> <li><a href="#___sec21" style="font-size: 80%;"><b>The code for solving the ODE</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs024.html#___sec22" style="font-size: 80%;"><b>The network with one input layer, specified number of hidden layers, and one output layer</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs025.html#___sec23" style="font-size: 80%;"><b>Example: Population growth</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs026.html#___sec24" style="font-size: 80%;"><b>Setting up the problem</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs027.html#___sec25" style="font-size: 80%;"><b>The trial solution</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs028.html#___sec26" style="font-size: 80%;"><b>The program using Autograd</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs029.html#___sec27" style="font-size: 80%;"><b>Using forward Euler to solve the ODE</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs030.html#___sec28" style="font-size: 80%;"><b>Example: Solving the one dimensional Poisson equation</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs031.html#___sec29" style="font-size: 80%;"><b>The specific equation to solve for</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs032.html#___sec30" style="font-size: 80%;"><b>Solving the equation using Autograd</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs033.html#___sec31" style="font-size: 80%;"><b>Comparing with a numerical scheme</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs034.html#___sec32" style="font-size: 80%;"><b>Setting up the code</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs035.html#___sec33" style="font-size: 80%;"><b>Partial Differential Equations</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs036.html#___sec34" style="font-size: 80%;"><b>Type of problem</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs037.html#___sec35" style="font-size: 80%;"><b>Network requirements</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs038.html#___sec36" style="font-size: 80%;"><b>More details</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs039.html#___sec37" style="font-size: 80%;"><b>Example: The diffusion equation</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs040.html#___sec38" style="font-size: 80%;"><b>Defining the problem</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs041.html#___sec39" style="font-size: 80%;"><b>Setting up the network using Autograd</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs042.html#___sec40" style="font-size: 80%;"><b>Setting up the network using Autograd; The trial solution</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs043.html#___sec41" style="font-size: 80%;"><b>Why the jacobian?</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs044.html#___sec42" style="font-size: 80%;"><b>Setting up the network using Autograd; The full program</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs045.html#___sec43" style="font-size: 80%;"><b>Example: Solving the wave equation with Neural Networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs046.html#___sec44" style="font-size: 80%;"><b>The problem to solve for</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs047.html#___sec45" style="font-size: 80%;"><b>The trial solution</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs048.html#___sec46" style="font-size: 80%;"><b>The analytical solution</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs049.html#___sec47" style="font-size: 80%;"><b>Solving the wave equation - the full program using Autograd</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs050.html#___sec48" style="font-size: 80%;"><b>Resources on differential equations and deep learning</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs051.html#___sec49" style="font-size: 80%;"><b>Friday, Principal Component Analysis</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs052.html#___sec50" style="font-size: 80%;"><b>Basic ideas of the Principal Component Analysis (PCA)</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs053.html#___sec51" style="font-size: 80%;"><b>Introducing the Covariance and Correlation functions</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs054.html#___sec52" style="font-size: 80%;"><b>Correlation Function and Design/Feature Matrix</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs055.html#___sec53" style="font-size: 80%;"><b>Covariance Matrix Examples</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs056.html#___sec54" style="font-size: 80%;"><b>Correlation Matrix</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs057.html#___sec55" style="font-size: 80%;"><b>Correlation Matrix with Pandas</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs058.html#___sec56" style="font-size: 80%;"><b>Correlation Matrix with Pandas and the Franke function</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs059.html#___sec57" style="font-size: 80%;"><b>Rewriting the Covariance and/or Correlation Matrix</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs060.html#___sec58" style="font-size: 80%;"><b>Towards the PCA theorem</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs061.html#___sec59" style="font-size: 80%;"><b>The Algorithm before the Theorem</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs062.html#___sec60" style="font-size: 80%;"><b>Writing our own PCA code</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs062.html#___sec61" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Compute the sample mean and center the data</a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs062.html#___sec62" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Compute the sample covariance</a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs062.html#___sec63" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Diagonalize the sample covariance matrix to obtain the principal components</a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs063.html#___sec64" style="font-size: 80%;"><b>Classical PCA Theorem</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs064.html#___sec65" style="font-size: 80%;"><b>Proof of the PCA Theorem</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs065.html#___sec66" style="font-size: 80%;"><b>PCA Proof continued</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs066.html#___sec67" style="font-size: 80%;"><b>The final step</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs067.html#___sec68" style="font-size: 80%;"><b>Geometric Interpretation and link with Singular Value Decomposition</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs068.html#___sec69" style="font-size: 80%;"><b>Principal Component Analysis</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs069.html#___sec70" style="font-size: 80%;"><b>PCA and scikit-learn</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs070.html#___sec71" style="font-size: 80%;"><b>Back to the Cancer Data</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs071.html#___sec72" style="font-size: 80%;"><b>More on the PCA</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs072.html#___sec73" style="font-size: 80%;"><b>Incremental PCA</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs073.html#___sec74" style="font-size: 80%;"><b>Randomized PCA</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs074.html#___sec75" style="font-size: 80%;"><b>Kernel PCA</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs075.html#___sec76" style="font-size: 80%;"><b>LLE</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs076.html#___sec77" style="font-size: 80%;"><b>Other techniques</b></a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->

<div class="container">

<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->

<a name="part0023"></a>
<!-- !split -->

<h2 id="___sec21" class="anchor">The code for solving the ODE </h2>

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">autograd.numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">autograd</span> <span style="color: #008000; font-weight: bold">import</span> grad, elementwise_grad
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">autograd.numpy.random</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">npr</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">matplotlib</span> <span style="color: #008000; font-weight: bold">import</span> pyplot <span style="color: #008000; font-weight: bold">as</span> plt

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">sigmoid</span>(z):
    <span style="color: #008000; font-weight: bold">return</span> <span style="color: #666666">1/</span>(<span style="color: #666666">1</span> <span style="color: #666666">+</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>z))

<span style="color: #408080; font-style: italic"># Assuming one input, hidden, and output layer</span>
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">neural_network</span>(params, x):

    <span style="color: #408080; font-style: italic"># Find the weights (including and biases) for the hidden and output layer.</span>
    <span style="color: #408080; font-style: italic"># Assume that params is a list of parameters for each layer.</span>
    <span style="color: #408080; font-style: italic"># The biases are the first element for each array in params,</span>
    <span style="color: #408080; font-style: italic"># and the weights are the remaning elements in each array in params.</span>

    w_hidden <span style="color: #666666">=</span> params[<span style="color: #666666">0</span>]
    w_output <span style="color: #666666">=</span> params[<span style="color: #666666">1</span>]

    <span style="color: #408080; font-style: italic"># Assumes input x being an one-dimensional array</span>
    num_values <span style="color: #666666">=</span> np<span style="color: #666666">.</span>size(x)
    x <span style="color: #666666">=</span> x<span style="color: #666666">.</span>reshape(<span style="color: #666666">-1</span>, num_values)

    <span style="color: #408080; font-style: italic"># Assume that the input layer does nothing to the input x</span>
    x_input <span style="color: #666666">=</span> x

    <span style="color: #408080; font-style: italic">## Hidden layer:</span>

    <span style="color: #408080; font-style: italic"># Add a row of ones to include bias</span>
    x_input <span style="color: #666666">=</span> np<span style="color: #666666">.</span>concatenate((np<span style="color: #666666">.</span>ones((<span style="color: #666666">1</span>,num_values)), x_input ), axis <span style="color: #666666">=</span> <span style="color: #666666">0</span>)

    z_hidden <span style="color: #666666">=</span> np<span style="color: #666666">.</span>matmul(w_hidden, x_input)
    x_hidden <span style="color: #666666">=</span> sigmoid(z_hidden)

    <span style="color: #408080; font-style: italic">## Output layer:</span>

    <span style="color: #408080; font-style: italic"># Include bias:</span>
    x_hidden <span style="color: #666666">=</span> np<span style="color: #666666">.</span>concatenate((np<span style="color: #666666">.</span>ones((<span style="color: #666666">1</span>,num_values)), x_hidden ), axis <span style="color: #666666">=</span> <span style="color: #666666">0</span>)

    z_output <span style="color: #666666">=</span> np<span style="color: #666666">.</span>matmul(w_output, x_hidden)
    x_output <span style="color: #666666">=</span> z_output

    <span style="color: #008000; font-weight: bold">return</span> x_output

<span style="color: #408080; font-style: italic"># The trial solution using the deep neural network:</span>
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">g_trial</span>(x,params, g0 <span style="color: #666666">=</span> <span style="color: #666666">10</span>):
    <span style="color: #008000; font-weight: bold">return</span> g0 <span style="color: #666666">+</span> x<span style="color: #666666">*</span>neural_network(params,x)

<span style="color: #408080; font-style: italic"># The right side of the ODE:</span>
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">g</span>(x, g_trial, gamma <span style="color: #666666">=</span> <span style="color: #666666">2</span>):
    <span style="color: #008000; font-weight: bold">return</span> <span style="color: #666666">-</span>gamma<span style="color: #666666">*</span>g_trial

<span style="color: #408080; font-style: italic"># The cost function:</span>
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">cost_function</span>(P, x):

    <span style="color: #408080; font-style: italic"># Evaluate the trial function with the current parameters P</span>
    g_t <span style="color: #666666">=</span> g_trial(x,P)

    <span style="color: #408080; font-style: italic"># Find the derivative w.r.t x of the neural network</span>
    d_net_out <span style="color: #666666">=</span> elementwise_grad(neural_network,<span style="color: #666666">1</span>)(P,x)

    <span style="color: #408080; font-style: italic"># Find the derivative w.r.t x of the trial function</span>
    d_g_t <span style="color: #666666">=</span> elementwise_grad(g_trial,<span style="color: #666666">0</span>)(x,P)

    <span style="color: #408080; font-style: italic"># The right side of the ODE</span>
    func <span style="color: #666666">=</span> g(x, g_t)

    err_sqr <span style="color: #666666">=</span> (d_g_t <span style="color: #666666">-</span> func)<span style="color: #666666">**2</span>
    cost_sum <span style="color: #666666">=</span> np<span style="color: #666666">.</span>sum(err_sqr)

    <span style="color: #008000; font-weight: bold">return</span> cost_sum <span style="color: #666666">/</span> np<span style="color: #666666">.</span>size(err_sqr)

<span style="color: #408080; font-style: italic"># Solve the exponential decay ODE using neural network with one input, hidden, and output layer</span>
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">solve_ode_neural_network</span>(x, num_neurons_hidden, num_iter, lmb):
    <span style="color: #408080; font-style: italic">## Set up initial weights and biases</span>

    <span style="color: #408080; font-style: italic"># For the hidden layer</span>
    p0 <span style="color: #666666">=</span> npr<span style="color: #666666">.</span>randn(num_neurons_hidden, <span style="color: #666666">2</span> )

    <span style="color: #408080; font-style: italic"># For the output layer</span>
    p1 <span style="color: #666666">=</span> npr<span style="color: #666666">.</span>randn(<span style="color: #666666">1</span>, num_neurons_hidden <span style="color: #666666">+</span> <span style="color: #666666">1</span> ) <span style="color: #408080; font-style: italic"># +1 since bias is included</span>

    P <span style="color: #666666">=</span> [p0, p1]

    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Initial cost: </span><span style="color: #BB6688; font-weight: bold">%g</span><span style="color: #BA2121">&#39;</span><span style="color: #666666">%</span>cost_function(P, x))

    <span style="color: #408080; font-style: italic">## Start finding the optimal weights using gradient descent</span>

    <span style="color: #408080; font-style: italic"># Find the Python function that represents the gradient of the cost function</span>
    <span style="color: #408080; font-style: italic"># w.r.t the 0-th input argument -- that is the weights and biases in the hidden and output layer</span>
    cost_function_grad <span style="color: #666666">=</span> grad(cost_function,<span style="color: #666666">0</span>)

    <span style="color: #408080; font-style: italic"># Let the update be done num_iter times</span>
    <span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(num_iter):
        <span style="color: #408080; font-style: italic"># Evaluate the gradient at the current weights and biases in P.</span>
        <span style="color: #408080; font-style: italic"># The cost_grad consist now of two arrays;</span>
        <span style="color: #408080; font-style: italic"># one for the gradient w.r.t P_hidden and</span>
        <span style="color: #408080; font-style: italic"># one for the gradient w.r.t P_output</span>
        cost_grad <span style="color: #666666">=</span>  cost_function_grad(P, x)

        P[<span style="color: #666666">0</span>] <span style="color: #666666">=</span> P[<span style="color: #666666">0</span>] <span style="color: #666666">-</span> lmb <span style="color: #666666">*</span> cost_grad[<span style="color: #666666">0</span>]
        P[<span style="color: #666666">1</span>] <span style="color: #666666">=</span> P[<span style="color: #666666">1</span>] <span style="color: #666666">-</span> lmb <span style="color: #666666">*</span> cost_grad[<span style="color: #666666">1</span>]

    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Final cost: </span><span style="color: #BB6688; font-weight: bold">%g</span><span style="color: #BA2121">&#39;</span><span style="color: #666666">%</span>cost_function(P, x))

    <span style="color: #008000; font-weight: bold">return</span> P

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">g_analytic</span>(x, gamma <span style="color: #666666">=</span> <span style="color: #666666">2</span>, g0 <span style="color: #666666">=</span> <span style="color: #666666">10</span>):
    <span style="color: #008000; font-weight: bold">return</span> g0<span style="color: #666666">*</span>np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>gamma<span style="color: #666666">*</span>x)

<span style="color: #408080; font-style: italic"># Solve the given problem</span>
<span style="color: #008000; font-weight: bold">if</span> <span style="color: #19177C">__name__</span> <span style="color: #666666">==</span> <span style="color: #BA2121">&#39;__main__&#39;</span>:
    <span style="color: #408080; font-style: italic"># Set seed such that the weight are initialized</span>
    <span style="color: #408080; font-style: italic"># with same weights and biases for every run.</span>
    npr<span style="color: #666666">.</span>seed(<span style="color: #666666">15</span>)

    <span style="color: #408080; font-style: italic">## Decide the vales of arguments to the function to solve</span>
    N <span style="color: #666666">=</span> <span style="color: #666666">10</span>
    x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(<span style="color: #666666">0</span>, <span style="color: #666666">1</span>, N)

    <span style="color: #408080; font-style: italic">## Set up the initial parameters</span>
    num_hidden_neurons <span style="color: #666666">=</span> <span style="color: #666666">10</span>
    num_iter <span style="color: #666666">=</span> <span style="color: #666666">10000</span>
    lmb <span style="color: #666666">=</span> <span style="color: #666666">0.001</span>

    <span style="color: #408080; font-style: italic"># Use the network</span>
    P <span style="color: #666666">=</span> solve_ode_neural_network(x, num_hidden_neurons, num_iter, lmb)

    <span style="color: #408080; font-style: italic"># Print the deviation from the trial solution and true solution</span>
    res <span style="color: #666666">=</span> g_trial(x,P)
    res_analytical <span style="color: #666666">=</span> g_analytic(x)

    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Max absolute difference: </span><span style="color: #BB6688; font-weight: bold">%g</span><span style="color: #BA2121">&#39;</span><span style="color: #666666">%</span>np<span style="color: #666666">.</span>max(np<span style="color: #666666">.</span>abs(res <span style="color: #666666">-</span> res_analytical)))

    <span style="color: #408080; font-style: italic"># Plot the results</span>
    plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(<span style="color: #666666">10</span>,<span style="color: #666666">10</span>))

    plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&#39;Performance of neural network solving an ODE compared to the analytical solution&#39;</span>)
    plt<span style="color: #666666">.</span>plot(x, res_analytical)
    plt<span style="color: #666666">.</span>plot(x, res[<span style="color: #666666">0</span>,:])
    plt<span style="color: #666666">.</span>legend([<span style="color: #BA2121">&#39;analytical&#39;</span>,<span style="color: #BA2121">&#39;nn&#39;</span>])
    plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&#39;x&#39;</span>)
    plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&#39;g(x)&#39;</span>)
    plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
<p>
<!-- navigation buttons at the bottom of the page -->
<ul class="pagination">
<li><a href="._week43-bs022.html">&laquo;</a></li>
  <li><a href="._week43-bs000.html">1</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week43-bs015.html">16</a></li>
  <li><a href="._week43-bs016.html">17</a></li>
  <li><a href="._week43-bs017.html">18</a></li>
  <li><a href="._week43-bs018.html">19</a></li>
  <li><a href="._week43-bs019.html">20</a></li>
  <li><a href="._week43-bs020.html">21</a></li>
  <li><a href="._week43-bs021.html">22</a></li>
  <li><a href="._week43-bs022.html">23</a></li>
  <li class="active"><a href="._week43-bs023.html">24</a></li>
  <li><a href="._week43-bs024.html">25</a></li>
  <li><a href="._week43-bs025.html">26</a></li>
  <li><a href="._week43-bs026.html">27</a></li>
  <li><a href="._week43-bs027.html">28</a></li>
  <li><a href="._week43-bs028.html">29</a></li>
  <li><a href="._week43-bs029.html">30</a></li>
  <li><a href="._week43-bs030.html">31</a></li>
  <li><a href="._week43-bs031.html">32</a></li>
  <li><a href="._week43-bs032.html">33</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week43-bs076.html">77</a></li>
  <li><a href="._week43-bs024.html">&raquo;</a></li>
</ul>
<!-- ------------------- end of main content --------------- -->

</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>

<!-- Bootstrap footer
<footer>
<a href="http://..."><img width="250" align=right src="http://..."></a>
</footer>
-->


<center style="font-size:80%">
<!-- copyright only on the titlepage -->
</center>


</body>
</html>
    


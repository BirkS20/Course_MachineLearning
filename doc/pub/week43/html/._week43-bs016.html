<!--
Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Week 43: Solving Differential Equations with Deep Learning and Dimensionality Reduction methods">

<title>Week 43: Solving Differential Equations with Deep Learning and Dimensionality Reduction methods</title>

<!-- Bootstrap style: bootstrap -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->

<style type="text/css">

/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}

/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>


</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Recurrent Neural Networks', 2, None, '___sec0'),
              ('Solving ODEs with Deep Learning', 2, None, '___sec1'),
              ('Why should we think of reducing the dimensionality',
               2,
               None,
               '___sec2'),
              ('Basic ideas of the Principal Component Analysis (PCA)',
               2,
               None,
               '___sec3'),
              ('Introducing the Covariance and Correlation functions',
               2,
               None,
               '___sec4'),
              ('Correlation Function and Design/Feature Matrix',
               2,
               None,
               '___sec5'),
              ('Covariance Matrix Examples', 2, None, '___sec6'),
              ('Correlation Matrix', 2, None, '___sec7'),
              ('Correlation Matrix with Pandas', 2, None, '___sec8'),
              ('Correlation Matrix with Pandas and the Franke function',
               2,
               None,
               '___sec9'),
              ('Rewriting the Covariance and/or Correlation Matrix',
               2,
               None,
               '___sec10'),
              ('Towards the PCA theorem', 2, None, '___sec11'),
              ('The Algorithm before the Theorem', 2, None, '___sec12'),
              ('Writing our own PCA code', 2, None, '___sec13'),
              ('Compute the sample mean and center the data',
               3,
               None,
               '___sec14'),
              ('Compute the sample covariance', 3, None, '___sec15'),
              ('Diagonalize the sample covariance matrix to obtain the '
               'principal components',
               3,
               None,
               '___sec16'),
              ('Classical PCA Theorem', 2, None, '___sec17'),
              ('Proof of the PCA Theorem', 2, None, '___sec18'),
              ('PCA Proof continued', 2, None, '___sec19'),
              ('The final step', 2, None, '___sec20'),
              ('Geometric Interpretation and link with Singular Value '
               'Decomposition',
               2,
               None,
               '___sec21'),
              ('Principal Component Analysis', 2, None, '___sec22'),
              ('PCA and scikit-learn', 2, None, '___sec23'),
              ('Back to the Cancer Data', 2, None, '___sec24'),
              ('More on the PCA', 2, None, '___sec25'),
              ('Incremental PCA', 2, None, '___sec26'),
              ('Randomized PCA', 2, None, '___sec27'),
              ('Kernel PCA', 2, None, '___sec28'),
              ('LLE', 2, None, '___sec29'),
              ('Other techniques', 2, None, '___sec30')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="week43-bs.html">Week 43: Solving Differential Equations with Deep Learning and Dimensionality Reduction methods</a>
  </div>

  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="._week43-bs002.html#___sec0" style="font-size: 80%;"><b>Recurrent Neural Networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs003.html#___sec1" style="font-size: 80%;"><b>Solving ODEs with Deep Learning</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs004.html#___sec2" style="font-size: 80%;"><b>Why should we think of reducing the dimensionality</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs005.html#___sec3" style="font-size: 80%;"><b>Basic ideas of the Principal Component Analysis (PCA)</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs006.html#___sec4" style="font-size: 80%;"><b>Introducing the Covariance and Correlation functions</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs007.html#___sec5" style="font-size: 80%;"><b>Correlation Function and Design/Feature Matrix</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs008.html#___sec6" style="font-size: 80%;"><b>Covariance Matrix Examples</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs009.html#___sec7" style="font-size: 80%;"><b>Correlation Matrix</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs010.html#___sec8" style="font-size: 80%;"><b>Correlation Matrix with Pandas</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs011.html#___sec9" style="font-size: 80%;"><b>Correlation Matrix with Pandas and the Franke function</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs012.html#___sec10" style="font-size: 80%;"><b>Rewriting the Covariance and/or Correlation Matrix</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs013.html#___sec11" style="font-size: 80%;"><b>Towards the PCA theorem</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs014.html#___sec12" style="font-size: 80%;"><b>The Algorithm before the Theorem</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs015.html#___sec13" style="font-size: 80%;"><b>Writing our own PCA code</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs015.html#___sec14" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Compute the sample mean and center the data</a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs015.html#___sec15" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Compute the sample covariance</a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs015.html#___sec16" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Diagonalize the sample covariance matrix to obtain the principal components</a></li>
     <!-- navigation toc: --> <li><a href="#___sec17" style="font-size: 80%;"><b>Classical PCA Theorem</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs017.html#___sec18" style="font-size: 80%;"><b>Proof of the PCA Theorem</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs018.html#___sec19" style="font-size: 80%;"><b>PCA Proof continued</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs019.html#___sec20" style="font-size: 80%;"><b>The final step</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs020.html#___sec21" style="font-size: 80%;"><b>Geometric Interpretation and link with Singular Value Decomposition</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs021.html#___sec22" style="font-size: 80%;"><b>Principal Component Analysis</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs022.html#___sec23" style="font-size: 80%;"><b>PCA and scikit-learn</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs023.html#___sec24" style="font-size: 80%;"><b>Back to the Cancer Data</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs024.html#___sec25" style="font-size: 80%;"><b>More on the PCA</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs025.html#___sec26" style="font-size: 80%;"><b>Incremental PCA</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs026.html#___sec27" style="font-size: 80%;"><b>Randomized PCA</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs027.html#___sec28" style="font-size: 80%;"><b>Kernel PCA</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs028.html#___sec29" style="font-size: 80%;"><b>LLE</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs029.html#___sec30" style="font-size: 80%;"><b>Other techniques</b></a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->

<div class="container">

<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->

<a name="part0016"></a>
<!-- !split -->

<h2 id="___sec17" class="anchor">Classical PCA Theorem   </h2>

<p>
We assume now that we have a design matrix \( \boldsymbol{X} \) which has been
centered as discussed above. For the sake of simplicity we skip the
overline symbol. The matrix is defined in terms of the various column
vectors \( [\boldsymbol{x}_0,\boldsymbol{x}_1,\dots, \boldsymbol{x}_{p-1}] \) each with dimension
\( \boldsymbol{x}\in {\mathbb{R}}^{n} \).

<p>
We assume also that we have an orthogonal transformation \( \boldsymbol{W}\in {\mathbb{R}}^{p\times p} \).  We define the reconstruction error (which is similar to the mean squared error we have seen before) as
$$
J(\boldsymbol{W},\boldsymbol{Z}) = \frac{1}{n}\sum_i (\boldsymbol{x}_i - \overline{\boldsymbol{x}}_i)^2,
$$

with \( \overline{\boldsymbol{x}}_i = \boldsymbol{W}\boldsymbol{z}_i \), where \( \boldsymbol{z}_i \) is a row vector with dimension \( {\mathbb{R}}^{n} \) of the matrix
\( \boldsymbol{Z}\in{\mathbb{R}}^{p\times n} \).  When doing PCA we want to reduce this dimensionality.

<p>
The PCA theorem states that minimizing the above reconstruction error
corresponds to setting \( \boldsymbol{W}=\boldsymbol{S} \), the orthogonal matrix which
diagonalizes the empirical covariance(correlation) matrix. The optimal
low-dimensional encoding of the data is then given by a set of vectors
\( \boldsymbol{z}_i \) with at most \( l \) vectors, with \( l < < p \), defined by the
orthogonal projection of the data onto the columns spanned by the
eigenvectors of the covariance(correlations matrix).

<p>
The proof which follows will be updated by mid January 2020.

<p>
<p>
<!-- navigation buttons at the bottom of the page -->
<ul class="pagination">
<li><a href="._week43-bs015.html">&laquo;</a></li>
  <li><a href="._week43-bs000.html">1</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week43-bs008.html">9</a></li>
  <li><a href="._week43-bs009.html">10</a></li>
  <li><a href="._week43-bs010.html">11</a></li>
  <li><a href="._week43-bs011.html">12</a></li>
  <li><a href="._week43-bs012.html">13</a></li>
  <li><a href="._week43-bs013.html">14</a></li>
  <li><a href="._week43-bs014.html">15</a></li>
  <li><a href="._week43-bs015.html">16</a></li>
  <li class="active"><a href="._week43-bs016.html">17</a></li>
  <li><a href="._week43-bs017.html">18</a></li>
  <li><a href="._week43-bs018.html">19</a></li>
  <li><a href="._week43-bs019.html">20</a></li>
  <li><a href="._week43-bs020.html">21</a></li>
  <li><a href="._week43-bs021.html">22</a></li>
  <li><a href="._week43-bs022.html">23</a></li>
  <li><a href="._week43-bs023.html">24</a></li>
  <li><a href="._week43-bs024.html">25</a></li>
  <li><a href="._week43-bs025.html">26</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week43-bs029.html">30</a></li>
  <li><a href="._week43-bs017.html">&raquo;</a></li>
</ul>
<!-- ------------------- end of main content --------------- -->

</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>

<!-- Bootstrap footer
<footer>
<a href="http://..."><img width="250" align=right src="http://..."></a>
</footer>
-->


<center style="font-size:80%">
<!-- copyright only on the titlepage -->
</center>


</body>
</html>
    


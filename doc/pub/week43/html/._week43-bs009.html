<!--
Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Week 43: Deep Learning: Recurrent Neural Networks and other Deep Learning Methods. Principal Component analysis">

<title>Week 43: Deep Learning: Recurrent Neural Networks and other Deep Learning Methods. Principal Component analysis</title>

<!-- Bootstrap style: bootstrap -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->

<style type="text/css">

/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}

/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>


</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Plans for week 43', 2, None, '___sec0'),
              ('Reading Recommendations', 2, None, '___sec1'),
              ('Summary on Deep Learning Methods', 2, None, '___sec2'),
              ('CNNs in brief', 2, None, '___sec3'),
              ('Recurrent neural networks: Overarching view',
               2,
               None,
               '___sec4'),
              ('Set up of an RNN', 2, None, '___sec5'),
              ('A simple example', 2, None, '___sec6'),
              ('An extrapolation example', 2, None, '___sec7'),
              ('Formatting the Data', 2, None, '___sec8'),
              ('Predicting New Points With A Trained Recurrent Neural Network',
               2,
               None,
               '___sec9'),
              ('Other Things to Try', 2, None, '___sec10'),
              ('Other Types of Recurrent Neural Networks', 2, None, '___sec11'),
              ('Generative Models', 2, None, '___sec12'),
              ('Generative Adversarial Networks', 2, None, '___sec13'),
              ('Discriminator', 2, None, '___sec14'),
              ('Learning Process', 2, None, '___sec15'),
              ('More about the Learning Process', 2, None, '___sec16'),
              ('Additional References', 2, None, '___sec17'),
              ('Writing Our First Generative Adversarial Network',
               2,
               None,
               '___sec18'),
              ('MNIST and GANs', 2, None, '___sec19'),
              ('Other Models', 2, None, '___sec20'),
              ('Training Step', 2, None, '___sec21'),
              ('Checkpoints', 2, None, '___sec22'),
              ('Exploring the Latent Space', 2, None, '___sec23'),
              ('Getting Results', 2, None, '___sec24'),
              ('Interpolating Between MNIST Digits', 2, None, '___sec25'),
              ('Basic ideas of the Principal Component Analysis (PCA)',
               2,
               None,
               '___sec26'),
              ('Introducing the Covariance and Correlation functions',
               2,
               None,
               '___sec27'),
              ('More on the covariance', 2, None, '___sec28'),
              ('Reminding ourselves about Linear Regression',
               2,
               None,
               '___sec29'),
              ('Simple Example', 2, None, '___sec30'),
              ('The Correlation Matrix', 2, None, '___sec31'),
              ('Numpy Functionality', 2, None, '___sec32'),
              ('Correlation Matrix again', 2, None, '___sec33'),
              ('Using Pandas', 2, None, '___sec34'),
              ('And then the Franke Function', 2, None, '___sec35'),
              ('Lnks with the Design Matrix', 2, None, '___sec36'),
              ('Computing the Expectation Values', 2, None, '___sec37'),
              ('Towards the PCA theorem', 2, None, '___sec38'),
              ('More on the PCA Theorem', 2, None, '___sec39'),
              ('The Algorithm before the Theorem', 2, None, '___sec40'),
              ('Writing our own PCA code', 2, None, '___sec41'),
              ('Implementing it', 2, None, '___sec42'),
              ('First Step', 2, None, '___sec43'),
              ('Scaling', 2, None, '___sec44'),
              ('Centered Data', 2, None, '___sec45'),
              ('Exploring', 2, None, '___sec46'),
              ('Diagonalize the sample covariance matrix to obtain the '
               'principal components',
               2,
               None,
               '___sec47'),
              ('Collecting all Steps', 2, None, '___sec48'),
              ('Classical PCA Theorem', 2, None, '___sec49'),
              ('The PCA Theorem', 2, None, '___sec50'),
              ('Geometric Interpretation and link with Singular Value '
               'Decomposition',
               2,
               None,
               '___sec51'),
              ('PCA and scikit-learn', 2, None, '___sec52'),
              ('Back to the Cancer Data', 2, None, '___sec53'),
              ('Incremental PCA', 2, None, '___sec54'),
              ('Randomized PCA', 3, None, '___sec55'),
              ('Kernel PCA', 3, None, '___sec56'),
              ('Other techniques', 2, None, '___sec57')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="week43-bs.html">Week 43: Deep Learning: Recurrent Neural Networks and other Deep Learning Methods. Principal Component analysis</a>
  </div>

  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="._week43-bs001.html#___sec0" style="font-size: 80%;"><b>Plans for week 43</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs002.html#___sec1" style="font-size: 80%;"><b>Reading Recommendations</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs003.html#___sec2" style="font-size: 80%;"><b>Summary on Deep Learning Methods</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs004.html#___sec3" style="font-size: 80%;"><b>CNNs in brief</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs005.html#___sec4" style="font-size: 80%;"><b>Recurrent neural networks: Overarching view</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs006.html#___sec5" style="font-size: 80%;"><b>Set up of an RNN</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs007.html#___sec6" style="font-size: 80%;"><b>A simple example</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs008.html#___sec7" style="font-size: 80%;"><b>An extrapolation example</b></a></li>
     <!-- navigation toc: --> <li><a href="#___sec8" style="font-size: 80%;"><b>Formatting the Data</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs010.html#___sec9" style="font-size: 80%;"><b>Predicting New Points With A Trained Recurrent Neural Network</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs011.html#___sec10" style="font-size: 80%;"><b>Other Things to Try</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs012.html#___sec11" style="font-size: 80%;"><b>Other Types of Recurrent Neural Networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs013.html#___sec12" style="font-size: 80%;"><b>Generative Models</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs014.html#___sec13" style="font-size: 80%;"><b>Generative Adversarial Networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs015.html#___sec14" style="font-size: 80%;"><b>Discriminator</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs016.html#___sec15" style="font-size: 80%;"><b>Learning Process</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs017.html#___sec16" style="font-size: 80%;"><b>More about the Learning Process</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs018.html#___sec17" style="font-size: 80%;"><b>Additional References</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs019.html#___sec18" style="font-size: 80%;"><b>Writing Our First Generative Adversarial Network</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs020.html#___sec19" style="font-size: 80%;"><b>MNIST and GANs</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs021.html#___sec20" style="font-size: 80%;"><b>Other Models</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs022.html#___sec21" style="font-size: 80%;"><b>Training Step</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs023.html#___sec22" style="font-size: 80%;"><b>Checkpoints</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs024.html#___sec23" style="font-size: 80%;"><b>Exploring the Latent Space</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs025.html#___sec24" style="font-size: 80%;"><b>Getting Results</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs026.html#___sec25" style="font-size: 80%;"><b>Interpolating Between MNIST Digits</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs027.html#___sec26" style="font-size: 80%;"><b>Basic ideas of the Principal Component Analysis (PCA)</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs028.html#___sec27" style="font-size: 80%;"><b>Introducing the Covariance and Correlation functions</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs029.html#___sec28" style="font-size: 80%;"><b>More on the covariance</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs030.html#___sec29" style="font-size: 80%;"><b>Reminding ourselves about Linear Regression</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs031.html#___sec30" style="font-size: 80%;"><b>Simple Example</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs032.html#___sec31" style="font-size: 80%;"><b>The Correlation Matrix</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs033.html#___sec32" style="font-size: 80%;"><b>Numpy Functionality</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs034.html#___sec33" style="font-size: 80%;"><b>Correlation Matrix again</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs035.html#___sec34" style="font-size: 80%;"><b>Using Pandas</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs036.html#___sec35" style="font-size: 80%;"><b>And then the Franke Function</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs037.html#___sec36" style="font-size: 80%;"><b>Lnks with the Design Matrix</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs038.html#___sec37" style="font-size: 80%;"><b>Computing the Expectation Values</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs039.html#___sec38" style="font-size: 80%;"><b>Towards the PCA theorem</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs040.html#___sec39" style="font-size: 80%;"><b>More on the PCA Theorem</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs041.html#___sec40" style="font-size: 80%;"><b>The Algorithm before the Theorem</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs042.html#___sec41" style="font-size: 80%;"><b>Writing our own PCA code</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs043.html#___sec42" style="font-size: 80%;"><b>Implementing it</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs044.html#___sec43" style="font-size: 80%;"><b>First Step</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs045.html#___sec44" style="font-size: 80%;"><b>Scaling</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs046.html#___sec45" style="font-size: 80%;"><b>Centered Data</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs047.html#___sec46" style="font-size: 80%;"><b>Exploring</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs048.html#___sec47" style="font-size: 80%;"><b>Diagonalize the sample covariance matrix to obtain the principal components</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs049.html#___sec48" style="font-size: 80%;"><b>Collecting all Steps</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs050.html#___sec49" style="font-size: 80%;"><b>Classical PCA Theorem</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs051.html#___sec50" style="font-size: 80%;"><b>The PCA Theorem</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs052.html#___sec51" style="font-size: 80%;"><b>Geometric Interpretation and link with Singular Value Decomposition</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs053.html#___sec52" style="font-size: 80%;"><b>PCA and scikit-learn</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs054.html#___sec53" style="font-size: 80%;"><b>Back to the Cancer Data</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs055.html#___sec54" style="font-size: 80%;"><b>Incremental PCA</b></a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs055.html#___sec55" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Randomized PCA</a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs055.html#___sec56" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Kernel PCA</a></li>
     <!-- navigation toc: --> <li><a href="._week43-bs056.html#___sec57" style="font-size: 80%;"><b>Other techniques</b></a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->

<div class="container">

<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->

<a name="part0009"></a>
<!-- !split -->

<h2 id="___sec8" class="anchor">Formatting the Data </h2>

<p>
The way the recurrent neural networks are trained in this program
differs from how machine learning algorithms are usually trained.
Typically a machine learning algorithm is trained by learning the
relationship between the x data and the y data.  In this program, the
recurrent neural network will be trained to recognize the relationship
in a sequence of y values.  This is type of data formatting is
typically used time series forcasting, but it can also be used in any
extrapolation (time series forecasting is just a specific type of
extrapolation along the time axis).  This method of data formatting
does not use the x data and assumes that the y data are evenly spaced.

<p>
For a standard machine learning algorithm, the training data has the
form of (x,y) so the machine learning algorithm learns to assiciate a
y value with a given x value.  This is useful when the test data has x
values within the same range as the training data.  However, for this
application, the x values of the test data are outside of the x values
of the training data and the traditional method of training a machine
learning algorithm does not work as well.  For this reason, the
recurrent neural network is trained on sequences of y values of the
form ((y1, y2), y3), so that the network is concerned with learning
the pattern of the y data and not the relation between the x and y
data.  As long as the pattern of y data outside of the training region
stays relatively stable compared to what was inside the training
region, this method of training can produce accurate extrapolations to
y values far removed from the training data set.

<p>
<!--  -->
<!-- The idea behind formatting the data in this way comes from [this resource](https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/) and [this one](https://fairyonice.github.io/Understand-Keras%27s-RNN-behind-the-scenes-with-a-sin-wave-example.html). -->
<!--  -->
<!-- The following method takes in a y data set and formats it so the "x data" are of the form (y1, y2) and the "y data" are of the form y3, with extra brackets added in to make the resulting arrays compatable with both Keras and Tensorflow. -->
<!--  -->
<!-- Note: Using a sequence length of two is not required for time series forecasting so any lenght of sequence could be used (for example instead of ((y1, y2) y3) you could change the length of sequence to be 4 and the resulting data points would have the form ((y1, y2, y3, y4), y5)).  While the following method can be used to create a data set of any sequence length, the remainder of the code expects the length of sequence to be 2.  This is because the data sets are very small and the higher the lenght of the sequence the less resulting data points. -->

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #408080; font-style: italic"># FORMAT_DATA</span>
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">format_data</span>(data, length_of_sequence <span style="color: #666666">=</span> <span style="color: #666666">2</span>):  
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">        Inputs:</span>
<span style="color: #BA2121; font-style: italic">            data(a numpy array): the data that will be the inputs to the recurrent neural</span>
<span style="color: #BA2121; font-style: italic">                network</span>
<span style="color: #BA2121; font-style: italic">            length_of_sequence (an int): the number of elements in one iteration of the</span>
<span style="color: #BA2121; font-style: italic">                sequence patter.  For a function approximator use length_of_sequence = 2.</span>
<span style="color: #BA2121; font-style: italic">        Returns:</span>
<span style="color: #BA2121; font-style: italic">            rnn_input (a 3D numpy array): the input data for the recurrent neural network.  Its</span>
<span style="color: #BA2121; font-style: italic">                dimensions are length of data - length of sequence, length of sequence, </span>
<span style="color: #BA2121; font-style: italic">                dimnsion of data</span>
<span style="color: #BA2121; font-style: italic">            rnn_output (a numpy array): the training data for the neural network</span>
<span style="color: #BA2121; font-style: italic">        Formats data to be used in a recurrent neural network.</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>

    X, Y <span style="color: #666666">=</span> [], []
    <span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #008000">len</span>(data)<span style="color: #666666">-</span>length_of_sequence):
        <span style="color: #408080; font-style: italic"># Get the next length_of_sequence elements</span>
        a <span style="color: #666666">=</span> data[i:i<span style="color: #666666">+</span>length_of_sequence]
        <span style="color: #408080; font-style: italic"># Get the element that immediately follows that</span>
        b <span style="color: #666666">=</span> data[i<span style="color: #666666">+</span>length_of_sequence]
        <span style="color: #408080; font-style: italic"># Reshape so that each data point is contained in its own array</span>
        a <span style="color: #666666">=</span> np<span style="color: #666666">.</span>reshape (a, (<span style="color: #008000">len</span>(a), <span style="color: #666666">1</span>))
        X<span style="color: #666666">.</span>append(a)
        Y<span style="color: #666666">.</span>append(b)
    rnn_input <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array(X)
    rnn_output <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array(Y)

    <span style="color: #008000; font-weight: bold">return</span> rnn_input, rnn_output


<span style="color: #408080; font-style: italic"># ## Defining the Recurrent Neural Network Using Keras</span>
<span style="color: #408080; font-style: italic"># </span>
<span style="color: #408080; font-style: italic"># The following method defines a simple recurrent neural network in keras consisting of one input layer, one hidden layer, and one output layer.</span>

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">rnn</span>(length_of_sequences, batch_size <span style="color: #666666">=</span> <span style="color: #008000; font-weight: bold">None</span>, stateful <span style="color: #666666">=</span> <span style="color: #008000; font-weight: bold">False</span>):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">        Inputs:</span>
<span style="color: #BA2121; font-style: italic">            length_of_sequences (an int): the number of y values in &quot;x data&quot;.  This is determined</span>
<span style="color: #BA2121; font-style: italic">                when the data is formatted</span>
<span style="color: #BA2121; font-style: italic">            batch_size (an int): Default value is None.  See Keras documentation of SimpleRNN.</span>
<span style="color: #BA2121; font-style: italic">            stateful (a boolean): Default value is False.  See Keras documentation of SimpleRNN.</span>
<span style="color: #BA2121; font-style: italic">        Returns:</span>
<span style="color: #BA2121; font-style: italic">            model (a Keras model): The recurrent neural network that is built and compiled by this</span>
<span style="color: #BA2121; font-style: italic">                method</span>
<span style="color: #BA2121; font-style: italic">        Builds and compiles a recurrent neural network with one hidden layer and returns the model.</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>
    <span style="color: #408080; font-style: italic"># Number of neurons in the input and output layers</span>
    in_out_neurons <span style="color: #666666">=</span> <span style="color: #666666">1</span>
    <span style="color: #408080; font-style: italic"># Number of neurons in the hidden layer</span>
    hidden_neurons <span style="color: #666666">=</span> <span style="color: #666666">200</span>
    <span style="color: #408080; font-style: italic"># Define the input layer</span>
    inp <span style="color: #666666">=</span> Input(batch_shape<span style="color: #666666">=</span>(batch_size, 
                length_of_sequences, 
                in_out_neurons))  
    <span style="color: #408080; font-style: italic"># Define the hidden layer as a simple RNN layer with a set number of neurons and add it to </span>
    <span style="color: #408080; font-style: italic"># the network immediately after the input layer</span>
    rnn <span style="color: #666666">=</span> SimpleRNN(hidden_neurons, 
                    return_sequences<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>,
                    stateful <span style="color: #666666">=</span> stateful,
                    name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;RNN&quot;</span>)(inp)
    <span style="color: #408080; font-style: italic"># Define the output layer as a dense neural network layer (standard neural network layer)</span>
    <span style="color: #408080; font-style: italic">#and add it to the network immediately after the hidden layer.</span>
    dens <span style="color: #666666">=</span> Dense(in_out_neurons,name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;dense&quot;</span>)(rnn)
    <span style="color: #408080; font-style: italic"># Create the machine learning model starting with the input layer and ending with the </span>
    <span style="color: #408080; font-style: italic"># output layer</span>
    model <span style="color: #666666">=</span> Model(inputs<span style="color: #666666">=</span>[inp],outputs<span style="color: #666666">=</span>[dens])
    <span style="color: #408080; font-style: italic"># Compile the machine learning model using the mean squared error function as the loss </span>
    <span style="color: #408080; font-style: italic"># function and an Adams optimizer.</span>
    model<span style="color: #666666">.</span>compile(loss<span style="color: #666666">=</span><span style="color: #BA2121">&quot;mean_squared_error&quot;</span>, optimizer<span style="color: #666666">=</span><span style="color: #BA2121">&quot;adam&quot;</span>)  
    <span style="color: #008000; font-weight: bold">return</span> model
</pre></div>
<p>
<p>
<!-- navigation buttons at the bottom of the page -->
<ul class="pagination">
<li><a href="._week43-bs008.html">&laquo;</a></li>
  <li><a href="._week43-bs000.html">1</a></li>
  <li><a href="._week43-bs001.html">2</a></li>
  <li><a href="._week43-bs002.html">3</a></li>
  <li><a href="._week43-bs003.html">4</a></li>
  <li><a href="._week43-bs004.html">5</a></li>
  <li><a href="._week43-bs005.html">6</a></li>
  <li><a href="._week43-bs006.html">7</a></li>
  <li><a href="._week43-bs007.html">8</a></li>
  <li><a href="._week43-bs008.html">9</a></li>
  <li class="active"><a href="._week43-bs009.html">10</a></li>
  <li><a href="._week43-bs010.html">11</a></li>
  <li><a href="._week43-bs011.html">12</a></li>
  <li><a href="._week43-bs012.html">13</a></li>
  <li><a href="._week43-bs013.html">14</a></li>
  <li><a href="._week43-bs014.html">15</a></li>
  <li><a href="._week43-bs015.html">16</a></li>
  <li><a href="._week43-bs016.html">17</a></li>
  <li><a href="._week43-bs017.html">18</a></li>
  <li><a href="._week43-bs018.html">19</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week43-bs056.html">57</a></li>
  <li><a href="._week43-bs010.html">&raquo;</a></li>
</ul>
<!-- ------------------- end of main content --------------- -->

</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>

<!-- Bootstrap footer
<footer>
<a href="http://..."><img width="250" align=right src="http://..."></a>
</footer>
-->


<center style="font-size:80%">
<!-- copyright only on the titlepage -->
</center>


</body>
</html>
    


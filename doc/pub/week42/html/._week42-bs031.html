<!--
Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Week 42 Convolutional (CNN) and Recurrent (RNN) Neural Networks">

<title>Week 42 Convolutional (CNN) and Recurrent (RNN) Neural Networks</title>

<!-- Bootstrap style: bootstrap -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->

<style type="text/css">

/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}

/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>


</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Plan for week 42', 2, None, '___sec0'),
              ('Convolutional Neural Networks (recognizing images)',
               2,
               None,
               '___sec1'),
              ('Neural Networks vs CNNs', 2, None, '___sec2'),
              ('Why CNNS for images, sound files, medical images from CT scans '
               'etc?',
               2,
               None,
               '___sec3'),
              ('Regular NNs don’t scale well to full images',
               2,
               None,
               '___sec4'),
              ('3D volumes of neurons', 2, None, '___sec5'),
              ('Layers used to build CNNs', 2, None, '___sec6'),
              ('Transforming images', 2, None, '___sec7'),
              ('CNNs in brief', 2, None, '___sec8'),
              ('CNNs in more detail, building convolutional neural networks in '
               'Tensorflow and Keras',
               2,
               None,
               '___sec9'),
              ('Setting it up', 2, None, '___sec10'),
              ('The MNIST dataset again', 2, None, '___sec11'),
              ('Strong correlations', 2, None, '___sec12'),
              ('Layers of a CNN', 2, None, '___sec13'),
              ('Systematic reduction', 2, None, '___sec14'),
              ('Prerequisites: Collect and pre-process data',
               2,
               None,
               '___sec15'),
              ('Importing Keras and Tensorflow', 2, None, '___sec16'),
              ('Running with Keras', 2, None, '___sec17'),
              ('Final part', 2, None, '___sec18'),
              ('Final visualization', 2, None, '___sec19'),
              ('The CIFAR01 data set', 2, None, '___sec20'),
              ('Verifying the data set', 2, None, '___sec21'),
              ('Set up  the model', 2, None, '___sec22'),
              ('Add Dense layers on top', 2, None, '___sec23'),
              ('Compile and train the model', 2, None, '___sec24'),
              ('Finally, evaluate the model', 2, None, '___sec25'),
              ('Recurrent neural networks: Overarching view',
               2,
               None,
               '___sec26'),
              ('Set up of an RNN', 2, None, '___sec27'),
              ('A simple example', 2, None, '___sec28'),
              ('An extrapolation example', 2, None, '___sec29'),
              ('Formatting the Data', 2, None, '___sec30'),
              ('Predicting New Points With A Trained Recurrent Neural Network',
               2,
               None,
               '___sec31'),
              ('Other Things to Try', 2, None, '___sec32'),
              ('Other Types of Recurrent Neural Networks',
               2,
               None,
               '___sec33')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="week42-bs.html">Week 42 Convolutional (CNN) and Recurrent (RNN) Neural Networks</a>
  </div>

  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="._week42-bs001.html#___sec0" style="font-size: 80%;">Plan for week 42</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs002.html#___sec1" style="font-size: 80%;">Convolutional Neural Networks (recognizing images)</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs003.html#___sec2" style="font-size: 80%;">Neural Networks vs CNNs</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs004.html#___sec3" style="font-size: 80%;">Why CNNS for images, sound files, medical images from CT scans etc?</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs005.html#___sec4" style="font-size: 80%;">Regular NNs don’t scale well to full images</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs006.html#___sec5" style="font-size: 80%;">3D volumes of neurons</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs007.html#___sec6" style="font-size: 80%;">Layers used to build CNNs</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs008.html#___sec7" style="font-size: 80%;">Transforming images</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs009.html#___sec8" style="font-size: 80%;">CNNs in brief</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs010.html#___sec9" style="font-size: 80%;">CNNs in more detail, building convolutional neural networks in Tensorflow and Keras</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs011.html#___sec10" style="font-size: 80%;">Setting it up</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs012.html#___sec11" style="font-size: 80%;">The MNIST dataset again</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs013.html#___sec12" style="font-size: 80%;">Strong correlations</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs014.html#___sec13" style="font-size: 80%;">Layers of a CNN</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs015.html#___sec14" style="font-size: 80%;">Systematic reduction</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs016.html#___sec15" style="font-size: 80%;">Prerequisites: Collect and pre-process data</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs017.html#___sec16" style="font-size: 80%;">Importing Keras and Tensorflow</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs018.html#___sec17" style="font-size: 80%;">Running with Keras</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs019.html#___sec18" style="font-size: 80%;">Final part</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs020.html#___sec19" style="font-size: 80%;">Final visualization</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs021.html#___sec20" style="font-size: 80%;">The CIFAR01 data set</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs022.html#___sec21" style="font-size: 80%;">Verifying the data set</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs023.html#___sec22" style="font-size: 80%;">Set up  the model</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs024.html#___sec23" style="font-size: 80%;">Add Dense layers on top</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs025.html#___sec24" style="font-size: 80%;">Compile and train the model</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs026.html#___sec25" style="font-size: 80%;">Finally, evaluate the model</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs027.html#___sec26" style="font-size: 80%;">Recurrent neural networks: Overarching view</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs028.html#___sec27" style="font-size: 80%;">Set up of an RNN</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs029.html#___sec28" style="font-size: 80%;">A simple example</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs030.html#___sec29" style="font-size: 80%;">An extrapolation example</a></li>
     <!-- navigation toc: --> <li><a href="#___sec30" style="font-size: 80%;">Formatting the Data</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs032.html#___sec31" style="font-size: 80%;">Predicting New Points With A Trained Recurrent Neural Network</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs033.html#___sec32" style="font-size: 80%;">Other Things to Try</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs034.html#___sec33" style="font-size: 80%;">Other Types of Recurrent Neural Networks</a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->

<div class="container">

<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->

<a name="part0031"></a>
<!-- !split -->

<h2 id="___sec30" class="anchor">Formatting the Data </h2>

<p>
The way the recurrent neural networks are trained in this program
differs from how machine learning algorithms are usually trained.
Typically a machine learning algorithm is trained by learning the
relationship between the x data and the y data.  In this program, the
recurrent neural network will be trained to recognize the relationship
in a sequence of y values.  This is type of data formatting is
typically used time series forcasting, but it can also be used in any
extrapolation (time series forecasting is just a specific type of
extrapolation along the time axis).  This method of data formatting
does not use the x data and assumes that the y data are evenly spaced.

<p>
For a standard machine learning algorithm, the training data has the
form of (x,y) so the machine learning algorithm learns to assiciate a
y value with a given x value.  This is useful when the test data has x
values within the same range as the training data.  However, for this
application, the x values of the test data are outside of the x values
of the training data and the traditional method of training a machine
learning algorithm does not work as well.  For this reason, the
recurrent neural network is trained on sequences of y values of the
form ((y1, y2), y3), so that the network is concerned with learning
the pattern of the y data and not the relation between the x and y
data.  As long as the pattern of y data outside of the training region
stays relatively stable compared to what was inside the training
region, this method of training can produce accurate extrapolations to
y values far removed from the training data set.

<p>
<!--  -->
<!-- The idea behind formatting the data in this way comes from [this resource](https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/) and [this one](https://fairyonice.github.io/Understand-Keras%27s-RNN-behind-the-scenes-with-a-sin-wave-example.html). -->
<!--  -->
<!-- The following method takes in a y data set and formats it so the "x data" are of the form (y1, y2) and the "y data" are of the form y3, with extra brackets added in to make the resulting arrays compatable with both Keras and Tensorflow. -->
<!--  -->
<!-- Note: Using a sequence length of two is not required for time series forecasting so any lenght of sequence could be used (for example instead of ((y1, y2) y3) you could change the length of sequence to be 4 and the resulting data points would have the form ((y1, y2, y3, y4), y5)).  While the following method can be used to create a data set of any sequence length, the remainder of the code expects the length of sequence to be 2.  This is because the data sets are very small and the higher the lenght of the sequence the less resulting data points. -->

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #408080; font-style: italic"># FORMAT_DATA</span>
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">format_data</span>(data, length_of_sequence <span style="color: #666666">=</span> <span style="color: #666666">2</span>):  
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">        Inputs:</span>
<span style="color: #BA2121; font-style: italic">            data(a numpy array): the data that will be the inputs to the recurrent neural</span>
<span style="color: #BA2121; font-style: italic">                network</span>
<span style="color: #BA2121; font-style: italic">            length_of_sequence (an int): the number of elements in one iteration of the</span>
<span style="color: #BA2121; font-style: italic">                sequence patter.  For a function approximator use length_of_sequence = 2.</span>
<span style="color: #BA2121; font-style: italic">        Returns:</span>
<span style="color: #BA2121; font-style: italic">            rnn_input (a 3D numpy array): the input data for the recurrent neural network.  Its</span>
<span style="color: #BA2121; font-style: italic">                dimensions are length of data - length of sequence, length of sequence, </span>
<span style="color: #BA2121; font-style: italic">                dimnsion of data</span>
<span style="color: #BA2121; font-style: italic">            rnn_output (a numpy array): the training data for the neural network</span>
<span style="color: #BA2121; font-style: italic">        Formats data to be used in a recurrent neural network.</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>

    X, Y <span style="color: #666666">=</span> [], []
    <span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #008000">len</span>(data)<span style="color: #666666">-</span>length_of_sequence):
        <span style="color: #408080; font-style: italic"># Get the next length_of_sequence elements</span>
        a <span style="color: #666666">=</span> data[i:i<span style="color: #666666">+</span>length_of_sequence]
        <span style="color: #408080; font-style: italic"># Get the element that immediately follows that</span>
        b <span style="color: #666666">=</span> data[i<span style="color: #666666">+</span>length_of_sequence]
        <span style="color: #408080; font-style: italic"># Reshape so that each data point is contained in its own array</span>
        a <span style="color: #666666">=</span> np<span style="color: #666666">.</span>reshape (a, (<span style="color: #008000">len</span>(a), <span style="color: #666666">1</span>))
        X<span style="color: #666666">.</span>append(a)
        Y<span style="color: #666666">.</span>append(b)
    rnn_input <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array(X)
    rnn_output <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array(Y)

    <span style="color: #008000; font-weight: bold">return</span> rnn_input, rnn_output


<span style="color: #408080; font-style: italic"># ## Defining the Recurrent Neural Network Using Keras</span>
<span style="color: #408080; font-style: italic"># </span>
<span style="color: #408080; font-style: italic"># The following method defines a simple recurrent neural network in keras consisting of one input layer, one hidden layer, and one output layer.</span>

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">rnn</span>(length_of_sequences, batch_size <span style="color: #666666">=</span> <span style="color: #008000; font-weight: bold">None</span>, stateful <span style="color: #666666">=</span> <span style="color: #008000; font-weight: bold">False</span>):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">        Inputs:</span>
<span style="color: #BA2121; font-style: italic">            length_of_sequences (an int): the number of y values in &quot;x data&quot;.  This is determined</span>
<span style="color: #BA2121; font-style: italic">                when the data is formatted</span>
<span style="color: #BA2121; font-style: italic">            batch_size (an int): Default value is None.  See Keras documentation of SimpleRNN.</span>
<span style="color: #BA2121; font-style: italic">            stateful (a boolean): Default value is False.  See Keras documentation of SimpleRNN.</span>
<span style="color: #BA2121; font-style: italic">        Returns:</span>
<span style="color: #BA2121; font-style: italic">            model (a Keras model): The recurrent neural network that is built and compiled by this</span>
<span style="color: #BA2121; font-style: italic">                method</span>
<span style="color: #BA2121; font-style: italic">        Builds and compiles a recurrent neural network with one hidden layer and returns the model.</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>
    <span style="color: #408080; font-style: italic"># Number of neurons in the input and output layers</span>
    in_out_neurons <span style="color: #666666">=</span> <span style="color: #666666">1</span>
    <span style="color: #408080; font-style: italic"># Number of neurons in the hidden layer</span>
    hidden_neurons <span style="color: #666666">=</span> <span style="color: #666666">200</span>
    <span style="color: #408080; font-style: italic"># Define the input layer</span>
    inp <span style="color: #666666">=</span> Input(batch_shape<span style="color: #666666">=</span>(batch_size, 
                length_of_sequences, 
                in_out_neurons))  
    <span style="color: #408080; font-style: italic"># Define the hidden layer as a simple RNN layer with a set number of neurons and add it to </span>
    <span style="color: #408080; font-style: italic"># the network immediately after the input layer</span>
    rnn <span style="color: #666666">=</span> SimpleRNN(hidden_neurons, 
                    return_sequences<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>,
                    stateful <span style="color: #666666">=</span> stateful,
                    name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;RNN&quot;</span>)(inp)
    <span style="color: #408080; font-style: italic"># Define the output layer as a dense neural network layer (standard neural network layer)</span>
    <span style="color: #408080; font-style: italic">#and add it to the network immediately after the hidden layer.</span>
    dens <span style="color: #666666">=</span> Dense(in_out_neurons,name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;dense&quot;</span>)(rnn)
    <span style="color: #408080; font-style: italic"># Create the machine learning model starting with the input layer and ending with the </span>
    <span style="color: #408080; font-style: italic"># output layer</span>
    model <span style="color: #666666">=</span> Model(inputs<span style="color: #666666">=</span>[inp],outputs<span style="color: #666666">=</span>[dens])
    <span style="color: #408080; font-style: italic"># Compile the machine learning model using the mean squared error function as the loss </span>
    <span style="color: #408080; font-style: italic"># function and an Adams optimizer.</span>
    model<span style="color: #666666">.</span>compile(loss<span style="color: #666666">=</span><span style="color: #BA2121">&quot;mean_squared_error&quot;</span>, optimizer<span style="color: #666666">=</span><span style="color: #BA2121">&quot;adam&quot;</span>)  
    <span style="color: #008000; font-weight: bold">return</span> model
</pre></div>
<p>
<p>
<!-- navigation buttons at the bottom of the page -->
<ul class="pagination">
<li><a href="._week42-bs030.html">&laquo;</a></li>
  <li><a href="._week42-bs000.html">1</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week42-bs023.html">24</a></li>
  <li><a href="._week42-bs024.html">25</a></li>
  <li><a href="._week42-bs025.html">26</a></li>
  <li><a href="._week42-bs026.html">27</a></li>
  <li><a href="._week42-bs027.html">28</a></li>
  <li><a href="._week42-bs028.html">29</a></li>
  <li><a href="._week42-bs029.html">30</a></li>
  <li><a href="._week42-bs030.html">31</a></li>
  <li class="active"><a href="._week42-bs031.html">32</a></li>
  <li><a href="._week42-bs032.html">33</a></li>
  <li><a href="._week42-bs033.html">34</a></li>
  <li><a href="._week42-bs034.html">35</a></li>
  <li><a href="._week42-bs032.html">&raquo;</a></li>
</ul>
<!-- ------------------- end of main content --------------- -->

</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>

<!-- Bootstrap footer
<footer>
<a href="http://..."><img width="250" align=right src="http://..."></a>
</footer>
-->


<center style="font-size:80%">
<!-- copyright only on the titlepage -->
</center>


</body>
</html>
    


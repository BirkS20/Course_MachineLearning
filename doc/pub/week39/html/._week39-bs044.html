<!--
Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Week 39: Optimization and  Gradient Methods">

<title>Week 39: Optimization and  Gradient Methods</title>

<!-- Bootstrap style: bootstrap -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->

<style type="text/css">

/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}

/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>


</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Plan for week 39', 2, None, '___sec0'),
              ('Thursday September 24', 2, None, '___sec1'),
              ('Optimization, the central part of any Machine Learning '
               'algortithm',
               2,
               None,
               '___sec2'),
              ('Revisiting our Logistic Regression case', 2, None, '___sec3'),
              ('The equations to solve', 2, None, '___sec4'),
              ("Solving using Newton-Raphson's method", 2, None, '___sec5'),
              ("Brief reminder on Newton-Raphson's method", 2, None, '___sec6'),
              ('The equations', 2, None, '___sec7'),
              ('Simple geometric interpretation', 2, None, '___sec8'),
              ('Extending to more than one variable', 2, None, '___sec9'),
              ('Steepest descent', 2, None, '___sec10'),
              ('More on Steepest descent', 2, None, '___sec11'),
              ('The ideal', 2, None, '___sec12'),
              ('The sensitiveness of the gradient descent',
               2,
               None,
               '___sec13'),
              ('Convex functions', 2, None, '___sec14'),
              ('Convex function', 2, None, '___sec15'),
              ('Conditions on convex functions', 2, None, '___sec16'),
              ('More on convex functions', 2, None, '___sec17'),
              ('Some simple problems', 2, None, '___sec18'),
              ('Friday September 25', 2, None, '___sec19'),
              ('Standard steepest descent', 2, None, '___sec20'),
              ('Gradient method', 2, None, '___sec21'),
              ('Steepest descent  method', 2, None, '___sec22'),
              ('Steepest descent  method', 2, None, '___sec23'),
              ('Final expressions', 2, None, '___sec24'),
              ('Steepest descent example', 2, None, '___sec25'),
              ('Conjugate gradient method', 2, None, '___sec26'),
              ('Conjugate gradient method', 2, None, '___sec27'),
              ('Conjugate gradient method', 2, None, '___sec28'),
              ('Conjugate gradient method', 2, None, '___sec29'),
              ('Conjugate gradient method and iterations', 2, None, '___sec30'),
              ('Conjugate gradient method', 2, None, '___sec31'),
              ('Conjugate gradient method', 2, None, '___sec32'),
              ('Conjugate gradient method', 2, None, '___sec33'),
              ('Revisiting our first homework', 2, None, '___sec34'),
              ('Gradient descent example', 2, None, '___sec35'),
              ('The derivative of the cost/loss function', 2, None, '___sec36'),
              ('The Hessian matrix', 2, None, '___sec37'),
              ('Simple program', 2, None, '___sec38'),
              ('Gradient Descent Example', 2, None, '___sec39'),
              ('And a corresponding example using _scikit-learn_',
               2,
               None,
               '___sec40'),
              ('Gradient descent and Ridge', 2, None, '___sec41'),
              ('Program example for gradient descent with Ridge Regression',
               2,
               None,
               '___sec42'),
              ('Using gradient descent methods, limitations',
               2,
               None,
               '___sec43'),
              ('Stochastic Gradient Descent', 2, None, '___sec44'),
              ('Computation of gradients', 2, None, '___sec45'),
              ('SGD example', 2, None, '___sec46'),
              ('The gradient step', 2, None, '___sec47'),
              ('Simple example code', 2, None, '___sec48'),
              ('When do we stop?', 2, None, '___sec49'),
              ('Slightly different approach', 2, None, '___sec50'),
              ('Program for stochastic gradient', 2, None, '___sec51')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="week39-bs.html">Week 39: Optimization and  Gradient Methods</a>
  </div>

  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="._week39-bs001.html#___sec0" style="font-size: 80%;">Plan for week 39</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs002.html#___sec1" style="font-size: 80%;">Thursday September 24</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs003.html#___sec2" style="font-size: 80%;">Optimization, the central part of any Machine Learning algortithm</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs004.html#___sec3" style="font-size: 80%;">Revisiting our Logistic Regression case</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs005.html#___sec4" style="font-size: 80%;">The equations to solve</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs006.html#___sec5" style="font-size: 80%;">Solving using Newton-Raphson's method</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs007.html#___sec6" style="font-size: 80%;">Brief reminder on Newton-Raphson's method</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs008.html#___sec7" style="font-size: 80%;">The equations</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs009.html#___sec8" style="font-size: 80%;">Simple geometric interpretation</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs010.html#___sec9" style="font-size: 80%;">Extending to more than one variable</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs011.html#___sec10" style="font-size: 80%;">Steepest descent</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs012.html#___sec11" style="font-size: 80%;">More on Steepest descent</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs013.html#___sec12" style="font-size: 80%;">The ideal</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs014.html#___sec13" style="font-size: 80%;">The sensitiveness of the gradient descent</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs015.html#___sec14" style="font-size: 80%;">Convex functions</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs016.html#___sec15" style="font-size: 80%;">Convex function</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs017.html#___sec16" style="font-size: 80%;">Conditions on convex functions</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs018.html#___sec17" style="font-size: 80%;">More on convex functions</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs019.html#___sec18" style="font-size: 80%;">Some simple problems</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs020.html#___sec19" style="font-size: 80%;">Friday September 25</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs021.html#___sec20" style="font-size: 80%;">Standard steepest descent</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs022.html#___sec21" style="font-size: 80%;">Gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs023.html#___sec22" style="font-size: 80%;">Steepest descent  method</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs024.html#___sec23" style="font-size: 80%;">Steepest descent  method</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs025.html#___sec24" style="font-size: 80%;">Final expressions</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs026.html#___sec25" style="font-size: 80%;">Steepest descent example</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs027.html#___sec26" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs028.html#___sec27" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs029.html#___sec28" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs030.html#___sec29" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs031.html#___sec30" style="font-size: 80%;">Conjugate gradient method and iterations</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs032.html#___sec31" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs033.html#___sec32" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs034.html#___sec33" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs035.html#___sec34" style="font-size: 80%;">Revisiting our first homework</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs036.html#___sec35" style="font-size: 80%;">Gradient descent example</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs037.html#___sec36" style="font-size: 80%;">The derivative of the cost/loss function</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs038.html#___sec37" style="font-size: 80%;">The Hessian matrix</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs039.html#___sec38" style="font-size: 80%;">Simple program</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs040.html#___sec39" style="font-size: 80%;">Gradient Descent Example</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs041.html#___sec40" style="font-size: 80%;">And a corresponding example using <b>scikit-learn</b></a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs042.html#___sec41" style="font-size: 80%;">Gradient descent and Ridge</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs043.html#___sec42" style="font-size: 80%;">Program example for gradient descent with Ridge Regression</a></li>
     <!-- navigation toc: --> <li><a href="#___sec43" style="font-size: 80%;">Using gradient descent methods, limitations</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs045.html#___sec44" style="font-size: 80%;">Stochastic Gradient Descent</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs046.html#___sec45" style="font-size: 80%;">Computation of gradients</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs047.html#___sec46" style="font-size: 80%;">SGD example</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs048.html#___sec47" style="font-size: 80%;">The gradient step</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs049.html#___sec48" style="font-size: 80%;">Simple example code</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs050.html#___sec49" style="font-size: 80%;">When do we stop?</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs051.html#___sec50" style="font-size: 80%;">Slightly different approach</a></li>
     <!-- navigation toc: --> <li><a href="._week39-bs052.html#___sec51" style="font-size: 80%;">Program for stochastic gradient</a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->

<div class="container">

<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->

<a name="part0044"></a>
<!-- !split -->

<h2 id="___sec43" class="anchor">Using gradient descent methods, limitations </h2>

<ul>
<li> <b>Gradient descent (GD) finds local minima of our function</b>. Since the GD algorithm is deterministic, if it converges, it will converge to a local minimum of our cost/loss/risk function. Because in ML we are often dealing with extremely rugged landscapes with many local minima, this can lead to poor performance.</li>
<li> <b>GD is sensitive to initial conditions</b>. One consequence of the local nature of GD is that initial conditions matter. Depending on where one starts, one will end up at a different local minima. Therefore, it is very important to think about how one initializes the training process. This is true for GD as well as more complicated variants of GD.</li>
<li> <b>Gradients are computationally expensive to calculate for large datasets</b>. In many cases in statistics and ML, the cost/loss/risk function is a sum of terms, with one term for each data point. For example, in linear regression, \( E \propto \sum_{i=1}^n (y_i - \mathbf{w}^T\cdot\mathbf{x}_i)^2 \); for logistic regression, the square error is replaced by the cross entropy. To calculate the gradient we have to sum over <em>all</em> \( n \) data points. Doing this at every GD step becomes extremely computationally expensive. An ingenious solution to this, is to calculate the gradients using small subsets of the data called &quot;mini batches&quot;. This has the added benefit of introducing stochasticity into our algorithm.</li>
<li> <b>GD is very sensitive to choices of learning rates</b>. GD is extremely sensitive to the choice of learning rates. If the learning rate is very small, the training process take an extremely long time. For larger learning rates, GD can diverge and give poor results. Furthermore, depending on what the local landscape looks like, we have to modify the learning rates to ensure convergence. Ideally, we would <em>adaptively</em> choose the learning rates to match the landscape.</li>
<li> <b>GD treats all directions in parameter space uniformly.</b> Another major drawback of GD is that unlike Newton's method, the learning rate for GD is the same in all directions in parameter space. For this reason, the maximum learning rate is set by the behavior of the steepest direction and this can significantly slow down training. Ideally, we would like to take large steps in flat directions and small steps in steep directions. Since we are exploring rugged landscapes where curvatures change, this requires us to keep track of not only the gradient but second derivatives. The ideal scenario would be to calculate the Hessian but this proves to be too computationally expensive.</li> 
<li> GD can take exponential time to escape saddle points, even with random initialization. As we mentioned, GD is extremely sensitive to initial condition since it determines the particular local minimum GD would eventually reach. However, even with a good initialization scheme, through the introduction of randomness, GD can still take exponential time to escape saddle points.</li>
</ul>

<p>
<!-- navigation buttons at the bottom of the page -->
<ul class="pagination">
<li><a href="._week39-bs043.html">&laquo;</a></li>
  <li><a href="._week39-bs000.html">1</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week39-bs036.html">37</a></li>
  <li><a href="._week39-bs037.html">38</a></li>
  <li><a href="._week39-bs038.html">39</a></li>
  <li><a href="._week39-bs039.html">40</a></li>
  <li><a href="._week39-bs040.html">41</a></li>
  <li><a href="._week39-bs041.html">42</a></li>
  <li><a href="._week39-bs042.html">43</a></li>
  <li><a href="._week39-bs043.html">44</a></li>
  <li class="active"><a href="._week39-bs044.html">45</a></li>
  <li><a href="._week39-bs045.html">46</a></li>
  <li><a href="._week39-bs046.html">47</a></li>
  <li><a href="._week39-bs047.html">48</a></li>
  <li><a href="._week39-bs048.html">49</a></li>
  <li><a href="._week39-bs049.html">50</a></li>
  <li><a href="._week39-bs050.html">51</a></li>
  <li><a href="._week39-bs051.html">52</a></li>
  <li><a href="._week39-bs052.html">53</a></li>
  <li><a href="._week39-bs045.html">&raquo;</a></li>
</ul>
<!-- ------------------- end of main content --------------- -->

</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>

<!-- Bootstrap footer
<footer>
<a href="http://..."><img width="250" align=right src="http://..."></a>
</footer>
-->


<center style="font-size:80%">
<!-- copyright only on the titlepage -->
</center>


</body>
</html>
    


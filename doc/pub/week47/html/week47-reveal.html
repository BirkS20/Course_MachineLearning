<!--
HTML file automatically generated from DocOnce source
(https://github.com/doconce/doconce/)
doconce format html week47-reveal.html week47-reveal reveal --html_slide_theme=beige
-->
<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Week 47: Unsupervised learning (PCA and Clustering)  and Summary of Course">
<title>Week 47: Unsupervised learning (PCA and Clustering)  and Summary of Course</title>

<!-- reveal.js: https://lab.hakim.se/reveal-js/ -->

<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

<link rel="stylesheet" href="reveal.js/css/reveal.css">
<link rel="stylesheet" href="reveal.js/css/theme/beige.css" id="theme">
<!--
<link rel="stylesheet" href="reveal.js/css/reveal.css">
<link rel="stylesheet" href="reveal.js/css/theme/beige.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/beigesmall.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/solarized.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/serif.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/night.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/moon.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/simple.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/sky.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/darkgray.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/default.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/cbc.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/simula.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/white.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/league.css" id="theme">
-->

<!-- For syntax highlighting -->
<link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">

<!-- Printing and PDF exports -->
<script>
var link = document.createElement( 'link' );
link.rel = 'stylesheet';
link.type = 'text/css';
link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
document.getElementsByTagName( 'head' )[0].appendChild( link );
</script>

<style type="text/css">
hr { border: 0; width: 80%; border-bottom: 1px solid #aaa}
p.caption { width: 80%; font-size: 60%; font-style: italic; text-align: left; }
hr.figure { border: 0; width: 80%; border-bottom: 1px solid #aaa}
.reveal .alert-text-small   { font-size: 80%;  }
.reveal .alert-text-large   { font-size: 130%; }
.reveal .alert-text-normal  { font-size: 90%;  }
.reveal .alert {
  padding:8px 35px 8px 14px; margin-bottom:18px;
  text-shadow:0 1px 0 rgba(255,255,255,0.5);
  border:5px solid #bababa;
  -webkit-border-radius: 14px; -moz-border-radius: 14px;
  border-radius:14px;
  background-position: 10px 10px;
  background-repeat: no-repeat;
  background-size: 38px;
  padding-left: 30px; /* 55px; if icon */
}
.reveal .alert-block {padding-top:14px; padding-bottom:14px}
.reveal .alert-block > p, .alert-block > ul {margin-bottom:1em}
/*.reveal .alert li {margin-top: 1em}*/
.reveal .alert-block p+p {margin-top:5px}
/*.reveal .alert-notice { background-image: url(https://hplgit.github.io/doconce/bundled/html_images/small_gray_notice.png); }
.reveal .alert-summary  { background-image:url(https://hplgit.github.io/doconce/bundled/html_images/small_gray_summary.png); }
.reveal .alert-warning { background-image: url(https://hplgit.github.io/doconce/bundled/html_images/small_gray_warning.png); }
.reveal .alert-question {background-image:url(https://hplgit.github.io/doconce/bundled/html_images/small_gray_question.png); } */
/* Override reveal.js table border */
.reveal table td {
  border: 0;
}

<style type="text/css">
/* Override h1, h2, ... styles */
h1 { font-size: 2.8em; }
h2 { font-size: 1.5em; }
h3 { font-size: 1.4em; }
h4 { font-size: 1.3em; }
h1, h2, h3, h4 { font-weight: bold; line-height: 1.2; }
body { overflow: auto; } /* vertical scrolling */
hr { border: 0; width: 80%; border-bottom: 1px solid #aaa}
p.caption { width: 80%; font-size: 60%; font-style: italic; text-align: left; }
hr.figure { border: 0; width: 80%; border-bottom: 1px solid #aaa}
.slide .alert-text-small   { font-size: 80%;  }
.slide .alert-text-large   { font-size: 130%; }
.slide .alert-text-normal  { font-size: 90%;  }
.slide .alert {
  padding:8px 35px 8px 14px; margin-bottom:18px;
  text-shadow:0 1px 0 rgba(255,255,255,0.5);
  border:5px solid #bababa;
    -webkit-border-radius:14px; -moz-border-radius:14px;
  border-radius:14px
  background-position: 10px 10px;
  background-repeat: no-repeat;
  background-size: 38px;
  padding-left: 30px; /* 55px; if icon */
}
.slide .alert-block {padding-top:14px; padding-bottom:14px}
.slide .alert-block > p, .alert-block > ul {margin-bottom:0}
/*.slide .alert li {margin-top: 1em}*/
.deck .alert-block p+p {margin-top:5px}
/*.slide .alert-notice { background-image: url(https://hplgit.github.io/doconce/
bundled/html_images//small_gray_notice.png); }
.slide .alert-summary  { background-image:url(https://hplgit.github.io/doconce/
bundled/html_images//small_gray_summary.png); }
.slide .alert-warning { background-image: url(https://hplgit.github.io/doconce/
bundled/html_images//small_gray_warning.png); }
.slide .alert-question {background-image:url(https://hplgit.github.io/doconce/
bundled/html_images/small_gray_question.png); } */
.dotable table, .dotable th, .dotable tr, .dotable tr td {
  border: 2px solid black;
  border-collapse: collapse;
  padding: 2px;
}
</style>


<!-- Styles for table layout of slides -->
<style type="text/css">
td.padding {
  padding-top:20px;
  padding-bottom:20px;
  padding-right:50px;
  padding-left:50px;
}
</style>

</head>


<body>
<div class="reveal">
<div class="slides">





<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>




<section>
<!-- ------------------- main content ---------------------- -->
<center>
<h1 style="text-align: center;">Week 47: Unsupervised learning (PCA and Clustering)  and Summary of Course</h1>
</center>  <!-- document title -->

<!-- author(s): Morten Hjorth-Jensen -->
<center>
<b>Morten Hjorth-Jensen</b> [1, 2]
</center>
<!-- institution(s) -->
<center>
[1] <b>Department of Physics, University of Oslo</b>
</center>
<center>
[2] <b>Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University</b>
</center>
<br>
<center>
<h4>Nov 25, 2022</h4>
</center> <!-- date -->
<br>


<center style="font-size:80%">
<!-- copyright --> &copy; 1999-2022, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license
</center>
</section>

<section>
<h2 id="overview-of-week-47">Overview of week 47 </h2>

<ul>
<p><li> <b>Thursday</b>: Dimensionality reduction and unsupervised learning: Principal Component analysis (PCA) and clustering</li>
<ul>

<p><li> <a href="https://youtu.be/VJIsEQM2lCI" target="_blank">Video of lecture</a></li>
</ul>
<p>
<p><li> <b>Friday</b>: PCA and clustering  and Summary of Course</li>
</ul>
<p>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<ol>
<p><li> We recommend highly the video on PCA by <a href="http://www.databookuw.com/page-2/page-4/" target="_blank">Brunton and Kutz</a>, see in particular the video of section 1.5. Repeating about the singular value discussion is also very useful as we will use this material as background.</li>
<p><li> <a href="https://www.youtube.com/watch?v=FgakZw6K1QQ" target="_blank">And another good video on PCA</a></li>
<p><li> <a href="https://www.youtube.com/watch?v=4b5d3muPQmA" target="_blank">k-means clustering video</a></li>
</ol>
</div>


<div class="alert alert-block alert-block alert-text-normal">
<b>Reading recommendations:</b>
<p>
<ol>
<p><li> Geron's chapter 9 on PCA</li>
<p><li> Hastie et al Chapter 13 (sections 13.1-13.2 are the most relevant ones)</li>
</ol>
</div>
</section>

<section>
<h2 id="basic-ideas-of-the-principal-component-analysis-pca">Basic ideas of the Principal Component Analysis (PCA) </h2>

<p>The principal component analysis deals with the problem of fitting a
low-dimensional affine subspace \( S \) of dimension \( d \) much smaller than
the total dimension \( D \) of the problem at hand (our data
set). Mathematically it can be formulated as a statistical problem or
a geometric problem.  In our discussion of the theorem for the
classical PCA, we will stay with a statistical approach. 
Historically, the PCA was first formulated in a statistical setting in order to estimate the principal component of a multivariate random variable.
</p>

<p>We have a data set defined by a design/feature matrix \( \boldsymbol{X} \) (see below for its definition) </p>
<ul>
<p><li> Each data point is determined by \( p \) extrinsic (measurement) variables</li>
<p><li> We may want to ask the following question: Are there fewer intrinsic variables (say \( d < < p \)) that still approximately describe the data?</li>
<p><li> If so, these intrinsic variables may tell us something important and finding these intrinsic variables is what dimension reduction methods do.</li> 
</ul>
<p>
<p>A good read is for example <a href="https://www.springer.com/gp/book/9780387878102" target="_blank">Vidal, Ma and Sastry</a>.</p>
</section>

<section>
<h2 id="introducing-the-covariance-and-correlation-functions">Introducing the Covariance and Correlation functions  </h2>

<p>Before we discuss the PCA theorem, we need to remind ourselves about
the definition of the covariance and the correlation function. These are quantities 
</p>

<p>Suppose we have defined two vectors
\( \hat{x} \) and \( \hat{y} \) with \( n \) elements each. The covariance matrix \( \boldsymbol{C} \) is defined as 
</p>
<p>&nbsp;<br>
$$
\boldsymbol{C}[\boldsymbol{x},\boldsymbol{y}] = \begin{bmatrix} \mathrm{cov}[\boldsymbol{x},\boldsymbol{x}] & \mathrm{cov}[\boldsymbol{x},\boldsymbol{y}] \\
                              \mathrm{cov}[\boldsymbol{y},\boldsymbol{x}] & \mathrm{cov}[\boldsymbol{y},\boldsymbol{y}] \\
             \end{bmatrix},
$$
<p>&nbsp;<br>

<p>where for example</p>
<p>&nbsp;<br>
$$
\mathrm{cov}[\boldsymbol{x},\boldsymbol{y}] =\frac{1}{n} \sum_{i=0}^{n-1}(x_i- \overline{x})(y_i- \overline{y}).
$$
<p>&nbsp;<br>

<p>With this definition and recalling that the variance is defined as</p>
<p>&nbsp;<br>
$$
\mathrm{var}[\boldsymbol{x}]=\frac{1}{n} \sum_{i=0}^{n-1}(x_i- \overline{x})^2,
$$
<p>&nbsp;<br>

<p>we can rewrite the covariance matrix as </p>
<p>&nbsp;<br>
$$
\boldsymbol{C}[\boldsymbol{x},\boldsymbol{y}] = \begin{bmatrix} \mathrm{var}[\boldsymbol{x}] & \mathrm{cov}[\boldsymbol{x},\boldsymbol{y}] \\
                              \mathrm{cov}[\boldsymbol{x},\boldsymbol{y}] & \mathrm{var}[\boldsymbol{y}] \\
             \end{bmatrix}.
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="more-on-the-covariance">More on the covariance </h2>
<p>The covariance takes values between zero and infinity and may thus
lead to problems with loss of numerical precision for particularly
large values. It is common to scale the covariance matrix by
introducing instead the correlation matrix defined via the so-called
correlation function
</p>

<p>&nbsp;<br>
$$
\mathrm{corr}[\boldsymbol{x},\boldsymbol{y}]=\frac{\mathrm{cov}[\boldsymbol{x},\boldsymbol{y}]}{\sqrt{\mathrm{var}[\boldsymbol{x}] \mathrm{var}[\boldsymbol{y}]}}.
$$
<p>&nbsp;<br>

<p>The correlation function is then given by values \( \mathrm{corr}[\boldsymbol{x},\boldsymbol{y}]
\in [-1,1] \). This avoids eventual problems with too large values. We
can then define the correlation matrix for the two vectors \( \boldsymbol{x} \)
and \( \boldsymbol{y} \) as
</p>

<p>&nbsp;<br>
$$
\boldsymbol{K}[\boldsymbol{x},\boldsymbol{y}] = \begin{bmatrix} 1 & \mathrm{corr}[\boldsymbol{x},\boldsymbol{y}] \\
                              \mathrm{corr}[\boldsymbol{y},\boldsymbol{x}] & 1 \\
             \end{bmatrix},
$$
<p>&nbsp;<br>

<p>In the above example this is the function we constructed using <b>pandas</b>.</p>
</section>

<section>
<h2 id="reminding-ourselves-about-linear-regression">Reminding ourselves about Linear Regression </h2>
<p>In our derivation of the various regression algorithms like <b>Ordinary Least Squares</b> or <b>Ridge regression</b>
we defined the design/feature matrix \( \boldsymbol{X} \) as
</p>

<p>&nbsp;<br>
$$
\boldsymbol{X}=\begin{bmatrix}
x_{0,0} & x_{0,1} & x_{0,2}& \dots & \dots x_{0,p-1}\\
x_{1,0} & x_{1,1} & x_{1,2}& \dots & \dots x_{1,p-1}\\
x_{2,0} & x_{2,1} & x_{2,2}& \dots & \dots x_{2,p-1}\\
\dots & \dots & \dots & \dots \dots & \dots \\
x_{n-2,0} & x_{n-2,1} & x_{n-2,2}& \dots & \dots x_{n-2,p-1}\\
x_{n-1,0} & x_{n-1,1} & x_{n-1,2}& \dots & \dots x_{n-1,p-1}\\
\end{bmatrix},
$$
<p>&nbsp;<br>

<p>with \( \boldsymbol{X}\in {\mathbb{R}}^{n\times p} \), with the predictors/features \( p \)  refering to the column numbers and the
entries \( n \) being the row elements.
We can rewrite the design/feature matrix in terms of its column vectors as
</p>
<p>&nbsp;<br>
$$
\boldsymbol{X}=\begin{bmatrix} \boldsymbol{x}_0 & \boldsymbol{x}_1 & \boldsymbol{x}_2 & \dots & \dots & \boldsymbol{x}_{p-1}\end{bmatrix},
$$
<p>&nbsp;<br>

<p>with a given vector</p>
<p>&nbsp;<br>
$$
\boldsymbol{x}_i^T = \begin{bmatrix}x_{0,i} & x_{1,i} & x_{2,i}& \dots & \dots x_{n-1,i}\end{bmatrix}.
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="simple-example">Simple Example </h2>
<p>With these definitions, we can now rewrite our \( 2\times 2 \)
correlation/covariance matrix in terms of a moe general design/feature
matrix \( \boldsymbol{X}\in {\mathbb{R}}^{n\times p} \). This leads to a \( p\times p \)
covariance matrix for the vectors \( \boldsymbol{x}_i \) with \( i=0,1,\dots,p-1 \)
</p>

<p>&nbsp;<br>
$$
\boldsymbol{C}[\boldsymbol{x}] = \begin{bmatrix}
\mathrm{var}[\boldsymbol{x}_0] & \mathrm{cov}[\boldsymbol{x}_0,\boldsymbol{x}_1]  & \mathrm{cov}[\boldsymbol{x}_0,\boldsymbol{x}_2] & \dots & \dots & \mathrm{cov}[\boldsymbol{x}_0,\boldsymbol{x}_{p-1}]\\
\mathrm{cov}[\boldsymbol{x}_1,\boldsymbol{x}_0] & \mathrm{var}[\boldsymbol{x}_1]  & \mathrm{cov}[\boldsymbol{x}_1,\boldsymbol{x}_2] & \dots & \dots & \mathrm{cov}[\boldsymbol{x}_1,\boldsymbol{x}_{p-1}]\\
\mathrm{cov}[\boldsymbol{x}_2,\boldsymbol{x}_0]   & \mathrm{cov}[\boldsymbol{x}_2,\boldsymbol{x}_1] & \mathrm{var}[\boldsymbol{x}_2] & \dots & \dots & \mathrm{cov}[\boldsymbol{x}_2,\boldsymbol{x}_{p-1}]\\
\dots & \dots & \dots & \dots & \dots & \dots \\
\dots & \dots & \dots & \dots & \dots & \dots \\
\mathrm{cov}[\boldsymbol{x}_{p-1},\boldsymbol{x}_0]   & \mathrm{cov}[\boldsymbol{x}_{p-1},\boldsymbol{x}_1] & \mathrm{cov}[\boldsymbol{x}_{p-1},\boldsymbol{x}_{2}]  & \dots & \dots  & \mathrm{var}[\boldsymbol{x}_{p-1}]\\
\end{bmatrix},
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="the-correlation-matrix">The Correlation Matrix </h2>

<p>and the correlation matrix</p>
<p>&nbsp;<br>
$$
\boldsymbol{K}[\boldsymbol{x}] = \begin{bmatrix}
1 & \mathrm{corr}[\boldsymbol{x}_0,\boldsymbol{x}_1]  & \mathrm{corr}[\boldsymbol{x}_0,\boldsymbol{x}_2] & \dots & \dots & \mathrm{corr}[\boldsymbol{x}_0,\boldsymbol{x}_{p-1}]\\
\mathrm{corr}[\boldsymbol{x}_1,\boldsymbol{x}_0] & 1  & \mathrm{corr}[\boldsymbol{x}_1,\boldsymbol{x}_2] & \dots & \dots & \mathrm{corr}[\boldsymbol{x}_1,\boldsymbol{x}_{p-1}]\\
\mathrm{corr}[\boldsymbol{x}_2,\boldsymbol{x}_0]   & \mathrm{corr}[\boldsymbol{x}_2,\boldsymbol{x}_1] & 1 & \dots & \dots & \mathrm{corr}[\boldsymbol{x}_2,\boldsymbol{x}_{p-1}]\\
\dots & \dots & \dots & \dots & \dots & \dots \\
\dots & \dots & \dots & \dots & \dots & \dots \\
\mathrm{corr}[\boldsymbol{x}_{p-1},\boldsymbol{x}_0]   & \mathrm{corr}[\boldsymbol{x}_{p-1},\boldsymbol{x}_1] & \mathrm{corr}[\boldsymbol{x}_{p-1},\boldsymbol{x}_{2}]  & \dots & \dots  & 1\\
\end{bmatrix},
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="numpy-functionality">Numpy Functionality </h2>

<p>The Numpy function <b>np.cov</b> calculates the covariance elements using
the factor \( 1/(n-1) \) instead of \( 1/n \) since it assumes we do not have
the exact mean values.  The following simple function uses the
<b>np.vstack</b> function which takes each vector of dimension \( 1\times n \)
and produces a \( 2\times n \) matrix \( \boldsymbol{W} \)
</p>

<p>&nbsp;<br>
$$
\boldsymbol{W}^T = \begin{bmatrix} x_0 & y_0 \\
                          x_1 & y_1 \\
                          x_2 & y_2\\
                          \dots & \dots \\
                          x_{n-2} & y_{n-2}\\
                          x_{n-1} & y_{n-1} & 
             \end{bmatrix},
$$
<p>&nbsp;<br>

<p>which in turn is converted into into the \( 2\times 2 \) covariance matrix
\( \boldsymbol{C} \) via the Numpy function <b>np.cov()</b>. We note that we can also calculate
the mean value of each set of samples \( \boldsymbol{x} \) etc using the Numpy
function <b>np.mean(x)</b>. We can also extract the eigenvalues of the
covariance matrix through the <b>np.linalg.eig()</b> function.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;"><span style="color: #228B22"># Importing various packages</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
n = <span style="color: #B452CD">100</span>
x = np.random.normal(size=n)
<span style="color: #658b00">print</span>(np.mean(x))
y = <span style="color: #B452CD">4</span>+<span style="color: #B452CD">3</span>*x+np.random.normal(size=n)
<span style="color: #658b00">print</span>(np.mean(y))
W = np.vstack((x, y))
C = np.cov(W)
<span style="color: #658b00">print</span>(C)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section>
<h2 id="correlation-matrix-again">Correlation Matrix again  </h2>

<p>The previous example can be converted into the correlation matrix by
simply scaling the matrix elements with the variances.  We should also
subtract the mean values for each column. This leads to the following
code which sets up the correlations matrix for the previous example in
a more brute force way. Here we scale the mean values for each column of the design matrix, calculate the relevant mean values and variances and then finally set up the \( 2\times 2 \) correlation matrix (since we have only two vectors). 
</p>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;"><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
n = <span style="color: #B452CD">100</span>
<span style="color: #228B22"># define two vectors                                                                                           </span>
x = np.random.random(size=n)
y = <span style="color: #B452CD">4</span>+<span style="color: #B452CD">3</span>*x+np.random.normal(size=n)
<span style="color: #228B22">#scaling the x and y vectors                                                                                   </span>
x = x - np.mean(x)
y = y - np.mean(y)
variance_x = np.sum(x<span style="color: #707a7c">@x</span>)/n
variance_y = np.sum(y<span style="color: #707a7c">@y</span>)/n
<span style="color: #658b00">print</span>(variance_x)
<span style="color: #658b00">print</span>(variance_y)
cov_xy = np.sum(x<span style="color: #707a7c">@y</span>)/n
cov_xx = np.sum(x<span style="color: #707a7c">@x</span>)/n
cov_yy = np.sum(y<span style="color: #707a7c">@y</span>)/n
C = np.zeros((<span style="color: #B452CD">2</span>,<span style="color: #B452CD">2</span>))
C[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>]= cov_xx/variance_x
C[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>]= cov_yy/variance_y
C[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>]= cov_xy/np.sqrt(variance_y*variance_x)
C[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>]= C[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>]
<span style="color: #658b00">print</span>(C)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>We see that the matrix elements along the diagonal are one as they
should be and that the matrix is symmetric. Furthermore, diagonalizing
this matrix we easily see that it is a positive definite matrix.
</p>

<p>The above procedure with <b>numpy</b> can be made more compact if we use <b>pandas</b>.</p>
</section>

<section>
<h2 id="using-pandas">Using Pandas </h2>

<p>We whow here how we can set up the correlation matrix using <b>pandas</b>, as done in this simple code</p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;"><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">pandas</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">pd</span>
n = <span style="color: #B452CD">10</span>
x = np.random.normal(size=n)
x = x - np.mean(x)
y = <span style="color: #B452CD">4</span>+<span style="color: #B452CD">3</span>*x+np.random.normal(size=n)
y = y - np.mean(y)
X = (np.vstack((x, y))).T
<span style="color: #658b00">print</span>(X)
Xpd = pd.DataFrame(X)
<span style="color: #658b00">print</span>(Xpd)
correlation_matrix = Xpd.corr()
<span style="color: #658b00">print</span>(correlation_matrix)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section>
<h2 id="and-then-the-franke-function">And then the Franke Function </h2>

<p>We expand this model to the Franke function discussed above.</p>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;"><span style="color: #228B22"># Common imports</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">pandas</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">pd</span>


<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">FrankeFunction</span>(x,y):
	term1 = <span style="color: #B452CD">0.75</span>*np.exp(-(<span style="color: #B452CD">0.25</span>*(<span style="color: #B452CD">9</span>*x-<span style="color: #B452CD">2</span>)**<span style="color: #B452CD">2</span>) - <span style="color: #B452CD">0.25</span>*((<span style="color: #B452CD">9</span>*y-<span style="color: #B452CD">2</span>)**<span style="color: #B452CD">2</span>))
	term2 = <span style="color: #B452CD">0.75</span>*np.exp(-((<span style="color: #B452CD">9</span>*x+<span style="color: #B452CD">1</span>)**<span style="color: #B452CD">2</span>)/<span style="color: #B452CD">49.0</span> - <span style="color: #B452CD">0.1</span>*(<span style="color: #B452CD">9</span>*y+<span style="color: #B452CD">1</span>))
	term3 = <span style="color: #B452CD">0.5</span>*np.exp(-(<span style="color: #B452CD">9</span>*x-<span style="color: #B452CD">7</span>)**<span style="color: #B452CD">2</span>/<span style="color: #B452CD">4.0</span> - <span style="color: #B452CD">0.25</span>*((<span style="color: #B452CD">9</span>*y-<span style="color: #B452CD">3</span>)**<span style="color: #B452CD">2</span>))
	term4 = -<span style="color: #B452CD">0.2</span>*np.exp(-(<span style="color: #B452CD">9</span>*x-<span style="color: #B452CD">4</span>)**<span style="color: #B452CD">2</span> - (<span style="color: #B452CD">9</span>*y-<span style="color: #B452CD">7</span>)**<span style="color: #B452CD">2</span>)
	<span style="color: #8B008B; font-weight: bold">return</span> term1 + term2 + term3 + term4


<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">create_X</span>(x, y, n ):
	<span style="color: #8B008B; font-weight: bold">if</span> <span style="color: #658b00">len</span>(x.shape) &gt; <span style="color: #B452CD">1</span>:
		x = np.ravel(x)
		y = np.ravel(y)

	N = <span style="color: #658b00">len</span>(x)
	l = <span style="color: #658b00">int</span>((n+<span style="color: #B452CD">1</span>)*(n+<span style="color: #B452CD">2</span>)/<span style="color: #B452CD">2</span>)		<span style="color: #228B22"># Number of elements in beta</span>
	X = np.ones((N,l))

	<span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #B452CD">1</span>,n+<span style="color: #B452CD">1</span>):
		q = <span style="color: #658b00">int</span>((i)*(i+<span style="color: #B452CD">1</span>)/<span style="color: #B452CD">2</span>)
		<span style="color: #8B008B; font-weight: bold">for</span> k <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(i+<span style="color: #B452CD">1</span>):
			X[:,q+k] = (x**(i-k))*(y**k)

	<span style="color: #8B008B; font-weight: bold">return</span> X


<span style="color: #228B22"># Making meshgrid of datapoints and compute Franke&#39;s function</span>
n = <span style="color: #B452CD">4</span>
N = <span style="color: #B452CD">100</span>
x = np.sort(np.random.uniform(<span style="color: #B452CD">0</span>, <span style="color: #B452CD">1</span>, N))
y = np.sort(np.random.uniform(<span style="color: #B452CD">0</span>, <span style="color: #B452CD">1</span>, N))
z = FrankeFunction(x, y)
X = create_X(x, y, n=n)    

Xpd = pd.DataFrame(X)
<span style="color: #228B22"># subtract the mean values and set up the covariance matrix</span>
Xpd = Xpd - Xpd.mean()
covariance_matrix = Xpd.cov()
<span style="color: #658b00">print</span>(covariance_matrix)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>We note here that the covariance is zero for the first rows and
columns since all matrix elements in the design matrix were set to one
(we are fitting the function in terms of a polynomial of degree \( n \)). We would however not include the intercept
and wee can simply
drop these elements and construct a correlation
matrix without them by centering our matrix elements by subtracting the mean of each column. 
</p>
</section>

<section>
<h2 id="links-with-the-design-matrix">Links with the Design Matrix </h2>

<p>We can rewrite the covariance matrix in a more compact form in terms of the design/feature matrix \( \boldsymbol{X} \) as </p>
<p>&nbsp;<br>
$$
\boldsymbol{C}[\boldsymbol{x}] = \frac{1}{n}\boldsymbol{X}^T\boldsymbol{X}= \mathbb{E}[\boldsymbol{X}^T\boldsymbol{X}].
$$
<p>&nbsp;<br>

<p>To see this let us simply look at a design matrix \( \boldsymbol{X}\in {\mathbb{R}}^{2\times 2} \)</p>
<p>&nbsp;<br>
$$
\boldsymbol{X}=\begin{bmatrix}
x_{00} & x_{01}\\
x_{10} & x_{11}\\
\end{bmatrix}=\begin{bmatrix}
\boldsymbol{x}_{0} & \boldsymbol{x}_{1}\\
\end{bmatrix}.
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="computing-the-expectation-values">Computing the Expectation Values </h2>

<p>If we then compute the expectation value</p>
<p>&nbsp;<br>
$$
\mathbb{E}[\boldsymbol{X}^T\boldsymbol{X}] = \frac{1}{n}\boldsymbol{X}^T\boldsymbol{X}=\begin{bmatrix}
x_{00}^2+x_{01}^2 & x_{00}x_{10}+x_{01}x_{11}\\
x_{10}x_{00}+x_{11}x_{01} & x_{10}^2+x_{11}^2\\
\end{bmatrix},
$$
<p>&nbsp;<br>

<p>which is just </p>
<p>&nbsp;<br>
$$
\boldsymbol{C}[\boldsymbol{x}_0,\boldsymbol{x}_1] = \boldsymbol{C}[\boldsymbol{x}]=\begin{bmatrix} \mathrm{var}[\boldsymbol{x}_0] & \mathrm{cov}[\boldsymbol{x}_0,\boldsymbol{x}_1] \\
                              \mathrm{cov}[\boldsymbol{x}_1,\boldsymbol{x}_0] & \mathrm{var}[\boldsymbol{x}_1] \\
             \end{bmatrix},
$$
<p>&nbsp;<br>

<p>where we wrote <p>&nbsp;<br>
$$\boldsymbol{C}[\boldsymbol{x}_0,\boldsymbol{x}_1] = \boldsymbol{C}[\boldsymbol{x}]$$
<p>&nbsp;<br> to indicate that this the covariance of the vectors \( \boldsymbol{x} \) of the design/feature matrix \( \boldsymbol{X} \).</p>

<p>It is easy to generalize this to a matrix \( \boldsymbol{X}\in {\mathbb{R}}^{n\times p} \).</p>
</section>

<section>
<h2 id="towards-the-pca-theorem">Towards the PCA theorem </h2>

<p>We have that the covariance matrix (the correlation matrix involves a simple rescaling) is given as</p>
<p>&nbsp;<br>
$$
\boldsymbol{C}[\boldsymbol{x}] = \frac{1}{n}\boldsymbol{X}^T\boldsymbol{X}= \mathbb{E}[\boldsymbol{X}^T\boldsymbol{X}].
$$
<p>&nbsp;<br>

<p>Let us now assume that we can perform a series of orthogonal transformations where we employ some orthogonal matrices \( \boldsymbol{S} \).
These matrices are defined as \( \boldsymbol{S}\in {\mathbb{R}}^{p\times p} \) and obey the orthogonality requirements \( \boldsymbol{S}\boldsymbol{S}^T=\boldsymbol{S}^T\boldsymbol{S}=\boldsymbol{I} \). The matrix can be written out in terms of the column vectors \( \boldsymbol{s}_i \) as \( \boldsymbol{S}=[\boldsymbol{s}_0,\boldsymbol{s}_1,\dots,\boldsymbol{s}_{p-1}] \) and \( \boldsymbol{s}_i \in {\mathbb{R}}^{p} \).
</p>

<p>Assume also that there is a transformation \( \boldsymbol{S}^T\boldsymbol{C}[\boldsymbol{x}]\boldsymbol{S}=\boldsymbol{C}[\boldsymbol{y}] \) such that the new matrix \( \boldsymbol{C}[\boldsymbol{y}] \) is diagonal with elements \( [\lambda_0,\lambda_1,\lambda_2,\dots,\lambda_{p-1}] \).  </p>

<p>That is we have</p>
<p>&nbsp;<br>
$$
\boldsymbol{C}[\boldsymbol{y}] = \mathbb{E}[\boldsymbol{S}^T\boldsymbol{X}^T\boldsymbol{X}T\boldsymbol{S}]=\boldsymbol{S}^T\boldsymbol{C}[\boldsymbol{x}]\boldsymbol{S},
$$
<p>&nbsp;<br>

<p>since the matrix \( \boldsymbol{S} \) is not a data dependent matrix.   Multiplying with \( \boldsymbol{S} \) from the left we have</p>
<p>&nbsp;<br>
$$
\boldsymbol{S}\boldsymbol{C}[\boldsymbol{y}] = \boldsymbol{C}[\boldsymbol{x}]\boldsymbol{S},
$$
<p>&nbsp;<br>

<p>and since \( \boldsymbol{C}[\boldsymbol{y}] \) is diagonal we have for a given eigenvalue \( i \) of the covariance matrix that</p>

<p>&nbsp;<br>
$$
\boldsymbol{S}_i\lambda_i = \boldsymbol{C}[\boldsymbol{x}]\boldsymbol{S}_i.
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="more-on-the-pca-theorem">More on the PCA Theorem </h2>

<p>In the derivation of the PCA theorem we will assume that the
eigenvalues are ordered in descending order, that is \( \lambda_0 > \lambda_1 > \dots > \lambda_{p-1} \).
</p>

<p>The eigenvalues tell us then how much we need to stretch the
corresponding eigenvectors. Dimensions with large eigenvalues have
thus large variations (large variance) and define therefore useful
dimensions. The data points are more spread out in the direction of
these eigenvectors.  Smaller eigenvalues mean on the other hand that
the corresponding eigenvectors are shrunk accordingly and the data
points are tightly bunched together and there is not much variation in
these specific directions. Hopefully then we could leave it out
dimensions where the eigenvalues are very small. If \( p \) is very large,
we could then aim at reducing \( p \) to \( l < < p \) and handle only \( l \)
features/predictors.
</p>

<p>Here is how we would proceed in setting up the algorithm for the PCA, see also discussion below here. </p>
<ul>
<p><li> Set up the datapoints for the design/feature matrix with the predictors/features \( p \)  referring to the column numbers and the entries \( n \) being the row elements.</li>
<p><li> Center the data by subtracting the mean value for each column.</li> 
<p><li> Compute then the covariance/correlation matrix.</li>
<p><li> Find the eigenpairs of the covariance matrix with eigenvalues \( [\lambda_0,\lambda_1,\dots,\lambda_{p-1}] \) and eigenvectors \( [\boldsymbol{s}_0,\boldsymbol{s}_1,\dots,\boldsymbol{s}_{p-1}] \).</li>
<p><li> Order the eigenvalue (and the eigenvectors accordingly) in order of decreasing eigenvalues.</li>
<p><li> Keep only those \( l \) eigenvalues larger than a selected threshold value, discarding thus \( p-l \) features since we expect small variations in the data here.</li>
</ul>
</section>

<section>
<h2 id="a-kind-of-bird-s-view-on-pca">A kind of Bird's view  on PCA </h2>

<b>Why do we maximize variance during Principal Component Analysis?</b>

<p>Variance is a measure of the <em>variability</em> of the data you
have. Potentially the number of components is infinite, so you want to "squeeze" the most
information in each component of the finite set you build.
</p>

<p>If, to exaggerate, you were to select a single principal component,
you would want it to account for the most variability possible: hence
the search for maximum variance, so that the one component collects
the most "uniqueness" from the data set.
</p>

<p>Maximizing the component vector variances is the same as maximizing
the 'uniqueness' of those vectors. The vectors are as distant
from each other as possible (orthogonal to each other).
</p>

<p>Take for example a situation where you have 2 lines that are
orthogonal in a 3D space. You can capture the environment much more
completely with those orthogonal lines than 2 lines that are parallel
(or nearly parallel). When applied to very high dimensional states
using very few vectors, this becomes a much more important
relationship among the vectors to maintain. In a linear algebra sense
you want independent rows to be produced by PCA, otherwise some of
those rows will be redundant.
</p>
</section>

<section>
<h2 id="writing-our-own-pca-code">Writing our own PCA code </h2>

<p>We will use a simple example first with two-dimensional data
drawn from a multivariate normal distribution with the following mean and covariance matrix (we have fixed these quantities but will play around with them below):
</p>
<p>&nbsp;<br>
$$
\mu = (-1,2) \qquad \Sigma = \begin{bmatrix} 4 & 2 \\
2 & 2
\end{bmatrix}
$$
<p>&nbsp;<br>

<p>Note that the mean refers to each column of data. 
We will generate \( n = 10000 \) points \( X = \{ x_1, \ldots, x_N \} \) from
this distribution, and store them in the \( 1000 \times 2 \) matrix \( \boldsymbol{X} \). This is our design matrix where we have forced the covariance and mean values to take specific values.
</p>
</section>

<section>
<h2 id="implementing-it">Implementing it </h2>
<p>The following Python code aids in setting up the data and writing out the design matrix.
Note that the function <b>multivariate</b> returns also the covariance discussed above and that it is defined by dividing by \( n-1 \) instead of \( n \).
</p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;"><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">pandas</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">pd</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">IPython.display</span> <span style="color: #8B008B; font-weight: bold">import</span> display
n = <span style="color: #B452CD">10000</span>
mean = (-<span style="color: #B452CD">1</span>, <span style="color: #B452CD">2</span>)
cov = [[<span style="color: #B452CD">4</span>, <span style="color: #B452CD">2</span>], [<span style="color: #B452CD">2</span>, <span style="color: #B452CD">2</span>]]
X = np.random.multivariate_normal(mean, cov, n)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Now we are going to implement the PCA algorithm. We will break it down into various substeps.</p>
</section>

<section>
<h2 id="first-step">First Step </h2>

<p>The first step of PCA is to compute the sample mean of the data and use it to center the data. Recall that the sample mean is</p>
<p>&nbsp;<br>
$$
\mu_n = \frac{1}{n} \sum_{i=1}^n x_i
$$
<p>&nbsp;<br>

<p>and the mean-centered data \( \bar{X} = \{ \bar{x}_1, \ldots, \bar{x}_n \} \) takes the form</p>
<p>&nbsp;<br>
$$
\bar{x}_i = x_i - \mu_n.
$$
<p>&nbsp;<br>

<p>When you are done with these steps, print out \( \mu_n \) to verify it is
close to \( \mu \) and plot your mean centered data to verify it is
centered at the origin! 
The following code elements perform these operations using <b>pandas</b> or using our own functionality for doing so. The latter, using <b>numpy</b> is rather simple through the <b>mean()</b> function. 
</p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;">df = pd.DataFrame(X)
<span style="color: #228B22"># Pandas does the centering for us</span>
df = df -df.mean()
<span style="color: #228B22"># we center it ourselves</span>
X_centered = X - X.mean(axis=<span style="color: #B452CD">0</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section>
<h2 id="scaling">Scaling </h2>
<p>Alternatively, we could use the functions we discussed
earlier for scaling the data set.  That is, we could have used the
<b>StandardScaler</b> function in <b>Scikit-Learn</b>, a function which ensures
that for each feature/predictor we study the mean value is zero and
the variance is one (every column in the design/feature matrix).  You
would then not get the same results, since we divide by the
variance. The diagonal covariance matrix elements will then be one,
while the non-diagonal ones need to be divided by \( 2\sqrt{2} \) for our
specific case.
</p>
</section>

<section>
<h2 id="centered-data">Centered Data </h2>

<p>Now we are going to use the mean centered data to compute the sample covariance of the data by using the following equation</p>
<p>&nbsp;<br>
$$
\begin{equation*}
\Sigma_n = \frac{1}{n-1} \sum_{i=1}^n \bar{x}_i^T \bar{x}_i = \frac{1}{n-1} \sum_{i=1}^n (x_i - \mu_n)^T (x_i - \mu_n)
\end{equation*}
$$
<p>&nbsp;<br>

<p>where the data points \( x_i \in \mathbb{R}^p \) (here in this example \( p = 2 \)) are column vectors and \( x^T \) is the transpose of \( x \).
We can write our own code or simply use either the functionaly of <b>numpy</b> or that of <b>pandas</b>, as follows
</p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;"><span style="color: #658b00">print</span>(df.cov())
<span style="color: #658b00">print</span>(np.cov(X_centered.T))
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Note that the way we define the covariance matrix here has a factor \( n-1 \) instead of \( n \). This is included in the <b>cov()</b> function by <b>numpy</b> and <b>pandas</b>. 
Our own code here is not very elegant and asks for obvious improvements. It is tailored to this specific \( 2\times 2 \) covariance matrix. 
</p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;"><span style="color: #228B22"># extract the relevant columns from the centered design matrix of dim n x 2</span>
x = X_centered[:,<span style="color: #B452CD">0</span>]
y = X_centered[:,<span style="color: #B452CD">1</span>]
Cov = np.zeros((<span style="color: #B452CD">2</span>,<span style="color: #B452CD">2</span>))
Cov[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>] = np.sum(x.T<span style="color: #707a7c">@y</span>)/(n-<span style="color: #B452CD">1.0</span>)
Cov[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>] = np.sum(x.T<span style="color: #707a7c">@x</span>)/(n-<span style="color: #B452CD">1.0</span>)
Cov[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>] = np.sum(y.T<span style="color: #707a7c">@y</span>)/(n-<span style="color: #B452CD">1.0</span>)
Cov[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>]= Cov[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>]
<span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;Centered covariance using own code&quot;</span>)
<span style="color: #658b00">print</span>(Cov)
plt.plot(x, y, <span style="color: #CD5555">&#39;x&#39;</span>)
plt.axis(<span style="color: #CD5555">&#39;equal&#39;</span>)
plt.show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section>
<h2 id="exploring">Exploring </h2>

<p>Depending on the number of points \( n \), we will get results that are close to the covariance values defined above.
The plot shows how the data are clustered around a line with slope close to one. Is this expected?  Try to change the covariance and the mean values. For example, try to make the variance of the first element much larger than that of the second diagonal element. Try also to shrink the covariance  (the non-diagonal elements) and see how the data points are distributed. 
</p>
</section>

<section>
<h2 id="diagonalize-the-sample-covariance-matrix-to-obtain-the-principal-components">Diagonalize the sample covariance matrix to obtain the principal components </h2>

<p>Now we are ready to solve for the principal components! To do so we
diagonalize the sample covariance matrix \( \Sigma \). We can use the
function <b>np.linalg.eig</b> to do so. It will return the eigenvalues and
eigenvectors of \( \Sigma \). Once we have these we can perform the 
following tasks:
</p>

<ul>
<p><li> We compute the percentage of the total variance captured by the first principal component</li>
<p><li> We plot the mean centered data and lines along the first and second principal components</li>
<p><li> Then we project the mean centered data onto the first and second principal components, and plot the projected data.</li> 
<p><li> Finally, we approximate the data as</li>
</ul>
<p>
<p>&nbsp;<br>
$$
\begin{equation*}
x_i \approx \tilde{x}_i = \mu_n + \langle x_i, v_0 \rangle v_0
\end{equation*}
$$
<p>&nbsp;<br>

<p>where \( v_0 \) is the first principal component. </p>
</section>

<section>
<h2 id="collecting-all-steps">Collecting all Steps </h2>

<p>Collecting all these steps we can write our own PCA function and
compare this with the functionality included in <b>Scikit-Learn</b>.  
</p>

<p>The code here outlines some of the elements we could include in the
analysis. Feel free to extend upon this in order to address the above
questions.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;"><span style="color: #228B22"># diagonalize and obtain eigenvalues, not necessarily sorted</span>
EigValues, EigVectors = np.linalg.eig(Cov)
<span style="color: #228B22"># sort eigenvectors and eigenvalues</span>
<span style="color: #228B22">#permute = EigValues.argsort()</span>
<span style="color: #228B22">#EigValues = EigValues[permute]</span>
<span style="color: #228B22">#EigVectors = EigVectors[:,permute]</span>
<span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;Eigenvalues of Covariance matrix&quot;</span>)
<span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #B452CD">2</span>):
    <span style="color: #658b00">print</span>(EigValues[i])
FirstEigvector = EigVectors[:,<span style="color: #B452CD">0</span>]
SecondEigvector = EigVectors[:,<span style="color: #B452CD">1</span>]
<span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;First eigenvector&quot;</span>)
<span style="color: #658b00">print</span>(FirstEigvector)
<span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;Second eigenvector&quot;</span>)
<span style="color: #658b00">print</span>(SecondEigvector)
<span style="color: #228B22">#thereafter we do a PCA with Scikit-learn</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.decomposition</span> <span style="color: #8B008B; font-weight: bold">import</span> PCA
pca = PCA(n_components = <span style="color: #B452CD">2</span>)
X2Dsl = pca.fit_transform(X)
<span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;Eigenvector of largest eigenvalue&quot;</span>)
<span style="color: #658b00">print</span>(pca.components_.T[:, <span style="color: #B452CD">0</span>])
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>This code does not contain all the above elements, but it shows how we can use <b>Scikit-Learn</b> to extract the eigenvector which corresponds to the largest eigenvalue. Try to address the questions we pose before the above code.  Try also to change the values of the covariance matrix by making one of the diagonal elements much larger than the other. What do you observe then? </p>
</section>

<section>
<h2 id="classical-pca-theorem">Classical PCA Theorem   </h2>

<p>We assume now that we have a design matrix \( \boldsymbol{X} \) which has been
centered as discussed above. For the sake of simplicity we skip the
overline symbol. The matrix is defined in terms of the various column
vectors \( [\boldsymbol{x}_0,\boldsymbol{x}_1,\dots, \boldsymbol{x}_{p-1}] \) each with dimension
\( \boldsymbol{x}\in {\mathbb{R}}^{n} \).
</p>

<p>The PCA theorem states that minimizing the above reconstruction error
corresponds to setting \( \boldsymbol{W}=\boldsymbol{S} \), the orthogonal matrix which
diagonalizes the empirical covariance(correlation) matrix. The optimal
low-dimensional encoding of the data is then given by a set of vectors
\( \boldsymbol{z}_i \) with at most \( l \) vectors, with \( l < < p \), defined by the
orthogonal projection of the data onto the columns spanned by the
eigenvectors of the covariance(correlations matrix).
</p>
</section>

<section>
<h2 id="the-pca-theorem">The PCA Theorem </h2>

<p>To show the PCA theorem let us start with the assumption that there is one vector \( \boldsymbol{s}_0 \) which corresponds to a solution which minimized the reconstruction error \( J \). This is an orthogonal vector. It means that we now approximate the reconstruction error in terms of \( \boldsymbol{w}_0 \) and \( \boldsymbol{z}_0 \) as</p>

<p>We are almost there, we have obtained a relation between minimizing
the reconstruction error and the variance and the covariance
matrix. Minimizing the error is equivalent to maximizing the variance
of the projected data.
</p>

<p>We could trivially maximize the variance of the projection (and
thereby minimize the error in the reconstruction function) by letting
the norm-2 of \( \boldsymbol{w}_0 \) go to infinity. However, this norm since we
want the matrix \( \boldsymbol{W} \) to be an orthogonal matrix, is constrained by
\( \vert\vert \boldsymbol{w}_0 \vert\vert_2^2=1 \). Imposing this condition via a
Lagrange multiplier we can then in turn maximize
</p>

<p>&nbsp;<br>
$$
J(\boldsymbol{w}_0)= \boldsymbol{w}_0^T\boldsymbol{C}[\boldsymbol{x}]\boldsymbol{w}_0+\lambda_0(1-\boldsymbol{w}_0^T\boldsymbol{w}_0).
$$
<p>&nbsp;<br>

<p>Taking the derivative with respect to \( \boldsymbol{w}_0 \) we obtain</p>

<p>&nbsp;<br>
$$
\frac{\partial J(\boldsymbol{w}_0)}{\partial \boldsymbol{w}_0}= 2\boldsymbol{C}[\boldsymbol{x}]\boldsymbol{w}_0-2\lambda_0\boldsymbol{w}_0=0,
$$
<p>&nbsp;<br>

<p>meaning that</p>
<p>&nbsp;<br>
$$
\boldsymbol{C}[\boldsymbol{x}]\boldsymbol{w}_0=\lambda_0\boldsymbol{w}_0.
$$
<p>&nbsp;<br>

<p><b>The direction that maximizes the variance (or minimizes the construction error) is an eigenvector of the covariance matrix</b>! If we left multiply with \( \boldsymbol{w}_0^T \) we have the variance of the projected data is</p>
<p>&nbsp;<br>
$$
\boldsymbol{w}_0^T\boldsymbol{C}[\boldsymbol{x}]\boldsymbol{w}_0=\lambda_0.
$$
<p>&nbsp;<br>

<p>If we want to maximize the variance (minimize the construction error)
we simply pick the eigenvector of the covariance matrix with the
largest eigenvalue. This establishes the link between the minimization
of the reconstruction function \( J \) in terms of an orthogonal matrix
and the maximization of the variance and thereby the covariance of our
observations encoded in the design/feature matrix \( \boldsymbol{X} \).
</p>

<p>The proof
for the other eigenvectors \( \boldsymbol{w}_1,\boldsymbol{w}_2,\dots \) can be
established by applying the above arguments and using the fact that
our basis of eigenvectors is orthogonal, see <a href="https://mitpress.mit.edu/books/machine-learning-1" target="_blank">Murphy chapter
12.2</a>.  The
discussion in chapter 12.2 of Murphy's text has also a nice link with
the Singular Value Decomposition theorem. For categorical data, see
chapter 12.4 and discussion therein.
</p>

<p>For more details, see for example <a href="https://www.springer.com/gp/book/9780387878102" target="_blank">Vidal, Ma and Sastry, chapter 2</a>.</p>
</section>

<section>
<h2 id="geometric-interpretation-and-link-with-singular-value-decomposition">Geometric Interpretation and link with Singular Value Decomposition </h2>

<p>For a detailed demonstration of the geometric interpretation, see <a href="https://www.springer.com/gp/book/9780387878102" target="_blank">Vidal, Ma and Sastry, section 2.1.2</a>.</p>

<p>Principal Component Analysis (PCA) is by far the most popular dimensionality reduction algorithm.
First it identifies the hyperplane that lies closest to the data, and then it projects the data onto it.
</p>

<p>The following Python code uses NumPy&#8217;s <b>svd()</b> function to obtain all the principal components of the
training set, then extracts the first two principal components. First we center the data using either <b>pandas</b> or our own code
</p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;"><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">pandas</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">pd</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">IPython.display</span> <span style="color: #8B008B; font-weight: bold">import</span> display
np.random.seed(<span style="color: #B452CD">100</span>)
<span style="color: #228B22"># setting up a 10 x 5 vanilla matrix </span>
rows = <span style="color: #B452CD">10</span>
cols = <span style="color: #B452CD">5</span>
X = np.random.randn(rows,cols)
df = pd.DataFrame(X)
<span style="color: #228B22"># Pandas does the centering for us</span>
df = df -df.mean()
display(df)

<span style="color: #228B22"># we center it ourselves</span>
X_centered = X - X.mean(axis=<span style="color: #B452CD">0</span>)
<span style="color: #228B22"># Then check the difference between pandas and our own set up</span>
<span style="color: #658b00">print</span>(X_centered-df)
<span style="color: #228B22">#Now we do an SVD</span>
U, s, V = np.linalg.svd(X_centered)
c1 = V.T[:, <span style="color: #B452CD">0</span>]
c2 = V.T[:, <span style="color: #B452CD">1</span>]
W2 = V.T[:, :<span style="color: #B452CD">2</span>]
X2D = X_centered.dot(W2)
<span style="color: #658b00">print</span>(X2D)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>PCA assumes that the dataset is centered around the origin. Scikit-Learn&#8217;s PCA classes take care of centering
the data for you. However, if you implement PCA yourself (as in the preceding example), or if you use other libraries, don&#8217;t
forget to center the data first.
</p>

<p>Once you have identified all the principal components, you can reduce the dimensionality of the dataset
down to \( d \) dimensions by projecting it onto the hyperplane defined by the first \( d \) principal components.
Selecting this hyperplane ensures that the projection will preserve as much variance as possible. 
</p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;">W2 = V.T[:, :<span style="color: #B452CD">2</span>]
X2D = X_centered.dot(W2)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section>
<h2 id="pca-and-scikit-learn">PCA and scikit-learn </h2>

<p>Scikit-Learn&#8217;s PCA class implements PCA using SVD decomposition just like we did before. The
following code applies PCA to reduce the dimensionality of the dataset down to two dimensions (note
that it automatically takes care of centering the data):
</p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;"><span style="color: #228B22">#thereafter we do a PCA with Scikit-learn</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.decomposition</span> <span style="color: #8B008B; font-weight: bold">import</span> PCA
pca = PCA(n_components = <span style="color: #B452CD">2</span>)
X2D = pca.fit_transform(X)
<span style="color: #658b00">print</span>(X2D)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>After fitting the PCA transformer to the dataset, you can access the principal components using the
components variable (note that it contains the PCs as horizontal vectors, so, for example, the first
principal component is equal to 
</p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;">pca.components_.T[:, <span style="color: #B452CD">0</span>]
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Another very useful piece of information is the explained variance ratio of each principal component,
available via the \( explained\_variance\_ratio \) variable. It indicates the proportion of the dataset&#8217;s
variance that lies along the axis of each principal component. 
</p>
</section>

<section>
<h2 id="back-to-the-cancer-data">Back to the Cancer Data </h2>
<p>We can now repeat the above but applied to real data, in this case our breast cancer data.
Here we compute performance scores on the training data using logistic regression.
</p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;"><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.model_selection</span> <span style="color: #8B008B; font-weight: bold">import</span>  train_test_split 
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.datasets</span> <span style="color: #8B008B; font-weight: bold">import</span> load_breast_cancer
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.linear_model</span> <span style="color: #8B008B; font-weight: bold">import</span> LogisticRegression
cancer = load_breast_cancer()

X_train, X_test, y_train, y_test = train_test_split(cancer.data,cancer.target,random_state=<span style="color: #B452CD">0</span>)

logreg = LogisticRegression()
logreg.fit(X_train, y_train)
<span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;Train set accuracy from Logistic Regression: {:.2f}&quot;</span>.format(logreg.score(X_train,y_train)))
<span style="color: #228B22"># We scale the data</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.preprocessing</span> <span style="color: #8B008B; font-weight: bold">import</span> StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
<span style="color: #228B22"># Then perform again a log reg fit</span>
logreg.fit(X_train_scaled, y_train)
<span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;Train set accuracy scaled data: {:.2f}&quot;</span>.format(logreg.score(X_train_scaled,y_train)))
<span style="color: #228B22">#thereafter we do a PCA with Scikit-learn</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.decomposition</span> <span style="color: #8B008B; font-weight: bold">import</span> PCA
pca = PCA(n_components = <span style="color: #B452CD">2</span>)
X2D_train = pca.fit_transform(X_train_scaled)
<span style="color: #228B22"># and finally compute the log reg fit and the score on the training data	</span>
logreg.fit(X2D_train,y_train)
<span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;Train set accuracy scaled and PCA data: {:.2f}&quot;</span>.format(logreg.score(X2D_train,y_train)))
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>We see that our training data after the PCA decomposition has a performance similar to the non-scaled data. </p>

<p>Instead of arbitrarily choosing the number of dimensions to reduce down to, it is generally preferable to
choose the number of dimensions that add up to a sufficiently large portion of the variance (e.g., 95%).
Unless, of course, you are reducing dimensionality for data visualization &#8212; in that case you will
generally want to reduce the dimensionality down to 2 or 3.
The following code computes PCA without reducing dimensionality, then computes the minimum number
of dimensions required to preserve 95% of the training set&#8217;s variance:
</p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;">pca = PCA()
pca.fit(X)
cumsum = np.cumsum(pca.explained_variance_ratio_)
d = np.argmax(cumsum &gt;= <span style="color: #B452CD">0.95</span>) + <span style="color: #B452CD">1</span>
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>You could then set \( n\_components=d \) and run PCA again. However, there is a much better option: instead
of specifying the number of principal components you want to preserve, you can set \( n\_components \) to be
a float between 0.0 and 1.0, indicating the ratio of variance you wish to preserve:
</p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;">pca = PCA(n_components=<span style="color: #B452CD">0.95</span>)
X_reduced = pca.fit_transform(X)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section>
<h2 id="incremental-pca">Incremental PCA </h2>

<p>One problem with the preceding implementation of PCA is that it requires the whole training set to fit in
memory in order for the SVD algorithm to run. Fortunately, Incremental PCA (IPCA) algorithms have
been developed: you can split the training set into mini-batches and feed an IPCA algorithm one minibatch
at a time. This is useful for large training sets, and also to apply PCA online (i.e., on the fly, as new
instances arrive).
</p>
<h3 id="randomized-pca">Randomized PCA </h3>

<p>Scikit-Learn offers yet another option to perform PCA, called Randomized PCA. This is a stochastic
algorithm that quickly finds an approximation of the first d principal components. Its computational
complexity is \( O(m \times d^2)+O(d^3) \), instead of \( O(m \times n^2) + O(n^3) \), so it is dramatically faster than the
previous algorithms when \( d \) is much smaller than \( n \).
</p>
<h3 id="kernel-pca">Kernel PCA </h3>

<p>The kernel trick is a mathematical technique that implicitly maps instances into a
very high-dimensional space (called the feature space), enabling nonlinear classification and regression
with Support Vector Machines. Recall that a linear decision boundary in the high-dimensional feature
space corresponds to a complex nonlinear decision boundary in the original space.
It turns out that the same trick can be applied to PCA, making it possible to perform complex nonlinear
projections for dimensionality reduction. This is called Kernel PCA (kPCA). It is often good at
preserving clusters of instances after projection, or sometimes even unrolling datasets that lie close to a
twisted manifold.
For example, the following code uses Scikit-Learn&#8217;s KernelPCA class to perform kPCA with an
</p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;"><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.decomposition</span> <span style="color: #8B008B; font-weight: bold">import</span> KernelPCA
rbf_pca = KernelPCA(n_components = <span style="color: #B452CD">2</span>, kernel=<span style="color: #CD5555">&quot;rbf&quot;</span>, gamma=<span style="color: #B452CD">0.04</span>)
X_reduced = rbf_pca.fit_transform(X)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section>
<h2 id="other-techniques">Other techniques </h2>

<p>There are many other dimensionality reduction techniques, several of which are available in Scikit-Learn.</p>

<p>Here are some of the most popular:</p>
<ul>
<p><li> <b>Multidimensional Scaling (MDS)</b> reduces dimensionality while trying to preserve the distances between the instances.</li>
<p><li> <b>Isomap</b> creates a graph by connecting each instance to its nearest neighbors, then reduces dimensionality while trying to preserve the geodesic distances between the instances.</li>
<p><li> <b>t-Distributed Stochastic Neighbor Embedding</b> (t-SNE) reduces dimensionality while trying to keep similar instances close and dissimilar instances apart. It is mostly used for visualization, in particular to visualize clusters of instances in high-dimensional space (e.g., to visualize the MNIST images in 2D).</li>
<p><li> Linear Discriminant Analysis (LDA) is actually a classification algorithm, but during training it learns the most discriminative axes between the classes, and these axes can then be used to define a hyperplane onto which to project the data. The benefit is that the projection will keep classes as far apart as possible, so LDA is a good technique to reduce dimensionality before running another classification algorithm such as a Support Vector Machine (SVM) classifier discussed in the SVM lectures.</li>
</ul>
</section>

<section>
<h2 id="clustering-and-unsupervised-learning">Clustering and Unsupervised Learning </h2>

<p>In general terms cluster analysis, or clustering, is the task of grouping a
data-set into different distinct categories based on some measure of equality of
the data. This measure is often referred to as a <b>metric</b> or <b>similarity
measure</b> in the literature (note: sometimes we deal with a <b>dissimilarity
measure</b> instead). Usually, these metrics are formulated as some kind of
distance function between points in a high-dimensional space.
</p>

<p>The simplest, and also the most
common is the <b>Euclidean distance</b>.
</p>
</section>

<section>
<h2 id="basic-idea-of-the-k-means-clustering-algorithm">Basic Idea of the \( k \)-means Clustering Algorithm </h2>

<p>The simplest of all clustering algorithms is the  <b>k-means algorithm</b>
, sometimes also referred to as <em>Lloyds algorithm</em>. It is the simplest and also
the most common. From its simplicity it obtains both strengths and weaknesses.
These will be discussed in more detail later. The \( k \)-means algorithm is a
<b>centroid based</b> clustering algorithm.
</p>
</section>

<section>
<h2 id="the-k-means-algorithm">The \( k \)-means Algorithm </h2>

<p>Assume, we are given \( n \) data points and we wish to split the data into \( K < n \)
different categories, or clusters. We label each cluster by an integer
</p>

<p>&nbsp;<br>
$$ k\in\{1, \cdots, K \}.
$$
<p>&nbsp;<br>

<p>In the basic k-means algorithm each point is assigned to only
one cluster \( k \), and these assignments are <em>non-injective</em> i.e. many-to-one. We
can think of these mappings as an encoder \( k = C(i) \), which assigns the \( i \)-th
data-point \( \bf x_i \) to the \( k \)-th cluster.
</p>

<p>\( k \)-means algorithm in words:</p>
<ol>
<p><li> We start with guesses / random initializations of our \( k \) cluster centers/centroids</li>
<p><li> For each centroid the points that are most similar are identified</li>
<p><li> Then we move / replace each centroid with a coordinate average of all the points that were assigned to that centroid.</li>
<p><li> Iterate 2-3 until the centroids no longer move (to some tolerance)</li>
</ol>
</section>

<section>
<h2 id="basic-math-of-the-k-means-algorithm">Basic Math of the \( k \)-means Algorithm </h2>

<p>We assume we have \( n \) data-points</p>
<p>&nbsp;<br>
$$
\begin{equation}\tag{1}
  \boldsymbol{x_i}  = \{x_{i, 1}, \cdots, x_{i, p}\}\in\mathbb{R}^p.
\end{equation}
$$
<p>&nbsp;<br>

<p>which we wish to group into \( K < n \) clusters. For our dissimilarity measure we
use the <em>squared Euclidean distance</em>
</p>
<p>&nbsp;<br>
$$
\begin{equation}\tag{2}
  d(\boldsymbol{x_i}, \boldsymbol{x_i'}) = \sum_{j=1}^p(x_{ij} - x_{i'j})^2
                         = ||\boldsymbol{x_i} - \boldsymbol{x_{i'}}||^2
\end{equation}
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="within-cluster-point-scatter">Within Cluster Point Scatter </h2>

<p>We define the so called <em>within-cluster point scatter</em> which gives us a
measure of how close each data point assigned to the same cluster tends to be to
the all the others.
</p>
<p>&nbsp;<br>
$$
\begin{equation}\tag{3}
  W(C) = \frac{1}{2}\sum_{k=1}^K\sum_{C(i)=k}
          \sum_{C(i')=k}d(\boldsymbol{x_i}, \boldsymbol{x_{i'}}) =
          \sum_{k=1}^KN_k\sum_{C(i)=k}||\boldsymbol{x_i} - \boldsymbol{\overline{x_k}}||^2
\end{equation}
$$
<p>&nbsp;<br>

<p>where \( \boldsymbol{\overline{x_k}} \) is the mean vector associated with the \( k \)-th
cluster, and \( N_k = \sum_{i=1}^nI(C(i) = k) \), where the \( I() \) notation is
similar to the Kronecker delta (<em>Commonly used in statistics, it just means that
when \( i = k \) we have the encoder \( C(i) \)</em>). In other words,  the within-cluster
scatter measures the compactness of each cluster with respect to the data points
assigned to each cluster. This is the quantity that the \( k \)-means algorithm aims
to minimize. We refer to this quantity \( W(C) \) as the within cluster scatter
because of its relation to the <em>total scatter</em>.
</p>
</section>

<section>
<h2 id="more-details">More Details </h2>

<p>We have</p>
<p>&nbsp;<br>
$$
\begin{equation}\tag{4}
  T = W(C) + B(C) = \frac{1}{2}\sum_{i=1}^n
                    \sum_{i'=1}^nd(\boldsymbol{x_i}, \boldsymbol{x_{i'}})
                  = \frac{1}{2}\sum_{k=1}^K\sum_{C(i)=k}
                    \Big(\sum_{C(i') = k}d(\boldsymbol{x_i}, \boldsymbol{x_{i'}})
                  + \sum_{C(i')\neq k}d(\boldsymbol{x_i}, \boldsymbol{x_{i'}})\Big).
\end{equation}
$$
<p>&nbsp;<br>

<p>This is a quantity that is conserved throughout the \( k \)-means algorithm. It can
be thought of as the total amount of information in the data, and it is composed
of the aforementioned within-cluster scatter and the <em>between-cluster scatter</em>
\( B(C) \). In methods such as principle component analysis the total scatter is not
conserved.
</p>
</section>

<section>
<h2 id="total-cluster-variance">Total Cluster Variance </h2>
<p>Given a cluster mean \( \boldsymbol{m_k} \) we define the <b>total cluster variance</b></p>
<p>&nbsp;<br>
$$
\begin{equation}\tag{5}
  \min_{C, \{\boldsymbol{m_k}\}_1^K}\sum_{k=1}^KN_k\sum||\boldsymbol{x_i} - \boldsymbol{m_k}||^2
\end{equation}
$$
<p>&nbsp;<br>

<p>Now we have all the pieces necessary to formally revisit the \( k \)-means algorithm.</p>
</section>

<section>
<h2 id="the-k-means-clustering-algorithm">The \( k \)-means Clustering Algorithm </h2>

<p>The \( k \)-means clustering algorithm goes as follows </p>

<ol>
<p><li> For a given cluster assignment \( C \), and \( k \) cluster means \( \left\{m_1, \cdots, m_k\right\} \). We minimize the total cluster variance with respect to the cluster means \( \{m_k\} \) yielding the means of the currently assigned clusters.</li>
<p><li> Given a current set of \( k \) means \( \{m_k\} \) the total cluster variance is minimized by assigning each observation to the closest (current) cluster mean. That is <p>&nbsp;<br>
$$C(i) = \underset{1\leq k\leq K}{\mathrm{argmin}} ||\boldsymbol{x_i} - \boldsymbol{m_k}||^2$$
<p>&nbsp;<br></li>
<p><li> Steps 1 and 2 are repeated until the assignments do not change.</li>
</ol>
</section>

<section>
<h2 id="summarizing">Summarizing  </h2>

<ol>
<p><li> Before we start we specify a number \( k \) which is the number of clusters we want to try to separate our data into.</li>
<p><li> We initially choose \( k \) random data points in our data as our initial centroids, <em>or means</em> (this is where the name comes from).</li>
<p><li> Assign each data point to their closest centroid, based on the squared Euclidean distance.</li>
<p><li> For each of the \( k \) cluster we update the centroid by calculating new mean values for all the data points in the cluster.</li>
<p><li> Iteratively minimize the within cluster scatter by performing steps (3, 4) until the new assignments stop changing (can be to some tolerance) or until a maximum number of iterations have passed.</li>
</ol>
</section>

<section>
<h2 id="writing-our-own-code-the-data-set">Writing our own Code, the Data Set  </h2>

<p>Let us now program the most basic version of the algorithm using nothing but
Python with numpy arrays. This code is kept intentionally simple to gradually
progress our understanding. There is no vectorization of any kind, and even most
helper functions are not utilized.
</p>

<p>We need first a dataset to do our cluster analysis on. In our case
this is a plain <em>vanilla</em> data set using random numbers using a
Gaussian distribution.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;"><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">time</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">tensorflow</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">tf</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">matplotlib</span> <span style="color: #8B008B; font-weight: bold">import</span> image
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.cluster</span> <span style="color: #8B008B; font-weight: bold">import</span> KMeans
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">IPython.display</span> <span style="color: #8B008B; font-weight: bold">import</span> display

np.random.seed(<span style="color: #B452CD">2021</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Next we define functions, for ease of use later, to generate Gaussians and to
set up our toy data set.
</p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;"><span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">gaussian_points</span>(dim=<span style="color: #B452CD">2</span>, n_points=<span style="color: #B452CD">1000</span>, mean_vector=np.array([<span style="color: #B452CD">0</span>, <span style="color: #B452CD">0</span>]),
                    sample_variance=<span style="color: #B452CD">1</span>):
    <span style="color: #CD5555">&quot;&quot;&quot;</span>
<span style="color: #CD5555">    Very simple custom function to generate gaussian distributed point clusters</span>
<span style="color: #CD5555">    with variable dimension, number of points, means in each direction</span>
<span style="color: #CD5555">    (must match dim) and sample variance.</span>

<span style="color: #CD5555">    Inputs:</span>
<span style="color: #CD5555">        dim (int)</span>
<span style="color: #CD5555">        n_points (int)</span>
<span style="color: #CD5555">        mean_vector (np.array) (where index 0 is x, index 1 is y etc.)</span>
<span style="color: #CD5555">        sample_variance (float)</span>

<span style="color: #CD5555">    Returns:</span>
<span style="color: #CD5555">        data (np.array): with dimensions (dim x n_points)</span>
<span style="color: #CD5555">    &quot;&quot;&quot;</span>

    mean_matrix = np.zeros(dim) + mean_vector
    covariance_matrix = np.eye(dim) * sample_variance
    data = np.random.multivariate_normal(mean_matrix, covariance_matrix,
                                    n_points)
    <span style="color: #8B008B; font-weight: bold">return</span> data



<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">generate_simple_clustering_dataset</span>(dim=<span style="color: #B452CD">2</span>, n_points=<span style="color: #B452CD">1000</span>, plotting=<span style="color: #8B008B; font-weight: bold">True</span>,
                                    return_data=<span style="color: #8B008B; font-weight: bold">True</span>):
    <span style="color: #CD5555">&quot;&quot;&quot;</span>
<span style="color: #CD5555">    Toy model to illustrate k-means clustering</span>
<span style="color: #CD5555">    &quot;&quot;&quot;</span>

    data1 = gaussian_points(mean_vector=np.array([<span style="color: #B452CD">5</span>, <span style="color: #B452CD">5</span>]))
    data2 = gaussian_points()
    data3 = gaussian_points(mean_vector=np.array([<span style="color: #B452CD">1</span>, <span style="color: #B452CD">4.5</span>]))
    data4 = gaussian_points(mean_vector=np.array([<span style="color: #B452CD">5</span>, <span style="color: #B452CD">1</span>]))
    data = np.concatenate((data1, data2, data3, data4), axis=<span style="color: #B452CD">0</span>)

    <span style="color: #8B008B; font-weight: bold">if</span> plotting:
        fig, ax = plt.subplots()
        ax.scatter(data[:, <span style="color: #B452CD">0</span>], data[:, <span style="color: #B452CD">1</span>], alpha=<span style="color: #B452CD">0.2</span>)
        ax.set_title(<span style="color: #CD5555">&#39;Toy Model Dataset&#39;</span>)
        plt.show()


    <span style="color: #8B008B; font-weight: bold">if</span> return_data:
        <span style="color: #8B008B; font-weight: bold">return</span> data


data = generate_simple_clustering_dataset()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section>
<h2 id="implementing-the-k-means-algorithm">Implementing the \( k \)-means Algorithm </h2>

<p>With the above dataset we start
implementing the \( k \)-means algorithm.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;">n_samples, dimensions = data.shape
n_clusters = <span style="color: #B452CD">4</span>

<span style="color: #228B22"># we randomly initialize our centroids</span>
np.random.seed(<span style="color: #B452CD">2021</span>)
centroids = data[np.random.choice(n_samples, n_clusters, replace=<span style="color: #8B008B; font-weight: bold">False</span>), :]
distances = np.zeros((n_samples, n_clusters))

<span style="color: #228B22"># first we need to calculate the distance to each centroid from our data</span>
<span style="color: #8B008B; font-weight: bold">for</span> k <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(n_clusters):
    <span style="color: #8B008B; font-weight: bold">for</span> n <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(n_samples):
        dist = <span style="color: #B452CD">0</span>
        <span style="color: #8B008B; font-weight: bold">for</span> d <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(dimensions):
            dist += np.abs(data[n, d] - centroids[k, d])**<span style="color: #B452CD">2</span>
            distances[n, k] = dist

<span style="color: #228B22"># we initialize an array to keep track of to which cluster each point belongs</span>
<span style="color: #228B22"># the way we set it up here the index tracks which point and the value which</span>
<span style="color: #228B22"># cluster the point belongs to</span>
cluster_labels = np.zeros(n_samples, dtype=<span style="color: #CD5555">&#39;int&#39;</span>)

<span style="color: #228B22"># next we loop through our samples and for every point assign it to the cluster</span>
<span style="color: #228B22"># to which it has the smallest distance to</span>
<span style="color: #8B008B; font-weight: bold">for</span> n <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(n_samples):
    <span style="color: #228B22"># tracking variables (all of this is basically just an argmin)</span>
    smallest = <span style="color: #B452CD">1e10</span>
    smallest_row_index = <span style="color: #B452CD">1e10</span>
    <span style="color: #8B008B; font-weight: bold">for</span> k <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(n_clusters):
        <span style="color: #8B008B; font-weight: bold">if</span> distances[n, k] &lt; smallest:
            smallest = distances[n, k]
            smallest_row_index = k

    cluster_labels[n] = smallest_row_index
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section>
<h2 id="plotting">Plotting </h2>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;">fig = plt.figure()
ax = fig.add_subplot()
unique_cluster_labels = np.unique(cluster_labels)
<span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> unique_cluster_labels:
    ax.scatter(data[cluster_labels == i, <span style="color: #B452CD">0</span>],
               data[cluster_labels == i, <span style="color: #B452CD">1</span>],
               label = i,
               alpha = <span style="color: #B452CD">0.2</span>)
    ax.scatter(centroids[:, <span style="color: #B452CD">0</span>], centroids[:, <span style="color: #B452CD">1</span>], c=<span style="color: #CD5555">&#39;black&#39;</span>)

ax.set_title(<span style="color: #CD5555">&quot;First Grouping of Points to Centroids&quot;</span>)

plt.show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>So what do we have so far? We have 'picked' \( k \) centroids at random from our
data points. There are other ways of more intelligently choosing their
initializations, however for our purposes randomly is fine. Then we have
initialized an array 'distances' which holds the information of the distance,
<em>or dissimilarity</em>, of every point to of our centroids. Finally, we have
initialized an array 'cluster_labels' which according to our distances array
holds the information of to which centroid every point is assigned. This was the
first pass of our algorithm. Essentially, all we need to do now is repeat the
distance and assignment steps above until we have reached a desired convergence
or a maximum amount of iterations.
</p>
</section>

<section>
<h2 id="continuing">Continuing </h2>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;">max_iterations = <span style="color: #B452CD">100</span>
tolerance = <span style="color: #B452CD">1e-8</span>

<span style="color: #8B008B; font-weight: bold">for</span> iteration <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(max_iterations):
    prev_centroids = centroids.copy()
    <span style="color: #8B008B; font-weight: bold">for</span> k <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(n_clusters):
        <span style="color: #228B22"># this array will be used to update our centroid positions</span>
        vector_mean = np.zeros(dimensions)
        mean_divisor = <span style="color: #B452CD">0</span>
        <span style="color: #8B008B; font-weight: bold">for</span> n <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(n_samples):
            <span style="color: #8B008B; font-weight: bold">if</span> cluster_labels[n] == k:
                vector_mean += data[n, :]
                mean_divisor += <span style="color: #B452CD">1</span>

        <span style="color: #228B22"># update according to the k means</span>
        centroids[k, :] = vector_mean / mean_divisor

    <span style="color: #228B22"># we find the dissimilarity</span>
    <span style="color: #8B008B; font-weight: bold">for</span> k <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(n_clusters):
        <span style="color: #8B008B; font-weight: bold">for</span> n <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(n_samples):
            dist = <span style="color: #B452CD">0</span>
            <span style="color: #8B008B; font-weight: bold">for</span> d <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(dimensions):
                dist += np.abs(data[n, d] - centroids[k, d])**<span style="color: #B452CD">2</span>
                distances[n, k] = dist

    <span style="color: #228B22"># assign each point</span>
    <span style="color: #8B008B; font-weight: bold">for</span> n <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(n_samples):
        smallest = <span style="color: #B452CD">1e10</span>
        smallest_row_index = <span style="color: #B452CD">1e10</span>
        <span style="color: #8B008B; font-weight: bold">for</span> k <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(n_clusters):
            <span style="color: #8B008B; font-weight: bold">if</span> distances[n, k] &lt; smallest:
                smallest = distances[n, k]
                smallest_row_index = k

        cluster_labels[n] = smallest_row_index

    <span style="color: #228B22"># convergence criteria</span>
    centroid_difference = np.sum(np.abs(centroids - prev_centroids))
    <span style="color: #8B008B; font-weight: bold">if</span> centroid_difference &lt; tolerance:
        <span style="color: #658b00">print</span>(<span style="color: #CD5555">f&#39;Converged at iteration {</span>iteration<span style="color: #CD5555">}&#39;</span>)
        <span style="color: #8B008B; font-weight: bold">break</span>

    <span style="color: #8B008B; font-weight: bold">elif</span> iteration == max_iterations:
        <span style="color: #658b00">print</span>(<span style="color: #CD5555">f&#39;Did not converge in {</span>max_iterations<span style="color: #CD5555">} iterations&#39;</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section>
<h2 id="wrapping-it-up">Wrapping it up </h2>
<p>We now have a simple , un-optimized \( k \)-means
clustering implementation. Lets plot the final result
</p>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;">fig = plt.figure()
ax = fig.add_subplot()
unique_cluster_labels = np.unique(cluster_labels)
<span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> unique_cluster_labels:
    ax.scatter(data[cluster_labels == i, <span style="color: #B452CD">0</span>],
               data[cluster_labels == i, <span style="color: #B452CD">1</span>],
               label = i,
               alpha = <span style="color: #B452CD">0.2</span>)
    ax.scatter(centroids[:, <span style="color: #B452CD">0</span>], centroids[:, <span style="color: #B452CD">1</span>], c=<span style="color: #CD5555">&#39;black&#39;</span>)

ax.set_title(<span style="color: #CD5555">&quot;Final Result of K-means Clustering&quot;</span>)

plt.show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;"><span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">naive_kmeans</span>(data, n_clusters=<span style="color: #B452CD">4</span>, max_iterations=<span style="color: #B452CD">100</span>, tolerance=<span style="color: #B452CD">1e-8</span>):
    start_time = time.time()

    n_samples, dimensions = data.shape
    n_clusters = <span style="color: #B452CD">4</span>
    <span style="color: #228B22">#np.random.seed(2021)</span>
    centroids = data[np.random.choice(n_samples, n_clusters, replace=<span style="color: #8B008B; font-weight: bold">False</span>), :]
    distances = np.zeros((n_samples, n_clusters))

    <span style="color: #8B008B; font-weight: bold">for</span> k <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(n_clusters):
        <span style="color: #8B008B; font-weight: bold">for</span> n <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(n_samples):
            dist = <span style="color: #B452CD">0</span>
            <span style="color: #8B008B; font-weight: bold">for</span> d <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(dimensions):
                dist += np.abs(data[n, d] - centroids[k, d])**<span style="color: #B452CD">2</span>
                distances[n, k] = dist

    cluster_labels = np.zeros(n_samples, dtype=<span style="color: #CD5555">&#39;int&#39;</span>)

    <span style="color: #8B008B; font-weight: bold">for</span> n <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(n_samples):
        smallest = <span style="color: #B452CD">1e10</span>
        smallest_row_index = <span style="color: #B452CD">1e10</span>
        <span style="color: #8B008B; font-weight: bold">for</span> k <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(n_clusters):
            <span style="color: #8B008B; font-weight: bold">if</span> distances[n, k] &lt; smallest:
                smallest = distances[n, k]
                smallest_row_index = k

        cluster_labels[n] = smallest_row_index

    <span style="color: #8B008B; font-weight: bold">for</span> iteration <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(max_iterations):
        prev_centroids = centroids.copy()
        <span style="color: #8B008B; font-weight: bold">for</span> k <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(n_clusters):
            vector_mean = np.zeros(dimensions)
            mean_divisor = <span style="color: #B452CD">0</span>
            <span style="color: #8B008B; font-weight: bold">for</span> n <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(n_samples):
                <span style="color: #8B008B; font-weight: bold">if</span> cluster_labels[n] == k:
                    vector_mean += data[n, :]
                    mean_divisor += <span style="color: #B452CD">1</span>

            centroids[k, :] = vector_mean / mean_divisor

        <span style="color: #8B008B; font-weight: bold">for</span> k <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(n_clusters):
            <span style="color: #8B008B; font-weight: bold">for</span> n <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(n_samples):
                dist = <span style="color: #B452CD">0</span>
                <span style="color: #8B008B; font-weight: bold">for</span> d <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(dimensions):
                    dist += np.abs(data[n, d] - centroids[k, d])**<span style="color: #B452CD">2</span>
                    distances[n, k] = dist

        <span style="color: #8B008B; font-weight: bold">for</span> n <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(n_samples):
            smallest = <span style="color: #B452CD">1e10</span>
            smallest_row_index = <span style="color: #B452CD">1e10</span>
            <span style="color: #8B008B; font-weight: bold">for</span> k <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(n_clusters):
                <span style="color: #8B008B; font-weight: bold">if</span> distances[n, k] &lt; smallest:
                    smallest = distances[n, k]
                    smallest_row_index = k

            cluster_labels[n] = smallest_row_index

        centroid_difference = np.sum(np.abs(centroids - prev_centroids))
        <span style="color: #8B008B; font-weight: bold">if</span> centroid_difference &lt; tolerance:
            <span style="color: #658b00">print</span>(<span style="color: #CD5555">f&#39;Converged at iteration {</span>iteration<span style="color: #CD5555">}&#39;</span>)
            <span style="color: #658b00">print</span>(<span style="color: #CD5555">f&#39;Runtime: {</span>time.time() - start_time<span style="color: #CD5555">} seconds&#39;</span>)

            <span style="color: #8B008B; font-weight: bold">return</span> cluster_labels, centroids

    <span style="color: #658b00">print</span>(<span style="color: #CD5555">f&#39;Did not converge in {</span>max_iterations<span style="color: #CD5555">} iterations&#39;</span>)
    <span style="color: #658b00">print</span>(<span style="color: #CD5555">f&#39;Runtime: {</span>time.time() - start_time<span style="color: #CD5555">} seconds&#39;</span>)

    <span style="color: #8B008B; font-weight: bold">return</span> cluster_labels, centroids
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section>
<h2 id="summary-of-course">Summary of course </h2>
</section>

<section>
<h2 id="what-me-worry-no-final-exam-in-this-course">What? Me worry? No final exam in this course! </h2>
<br/><br/>
<center>
<p><img src="figures/exam1.jpeg" width="500" align="bottom"></p>
</center>
<br/><br/>
</section>

<section>
<h2 id="what-is-the-link-between-artificial-intelligence-and-machine-learning-and-some-general-remarks">What is the link between Artificial Intelligence and Machine Learning and some general Remarks </h2>

<p>Artificial intelligence is built upon integrated machine learning
algorithms as discussed in this course, which in turn are fundamentally rooted in optimization and
statistical learning.
</p>

<p>Can we have Artificial Intelligence without Machine Learning? See <a href="https://www.linkedin.com/pulse/what-artificial-intelligence-without-machine-learning-claudia-pohlink" target="_blank">this post for inspiration</a>.</p>
</section>

<section>
<h2 id="going-back-to-the-beginning-of-the-semester">Going back to the beginning of the semester </h2>

<p>Traditionally the field of machine learning has had its main focus on
predictions and correlations.  These concepts outline in some sense
the difference between machine learning and what is normally called
Bayesian statistics or Bayesian inference.
</p>

<p>In machine learning and prediction based tasks, we are often
interested in developing algorithms that are capable of learning
patterns from given data in an automated fashion, and then using these
learned patterns to make predictions or assessments of newly given
data. In many cases, our primary concern is the quality of the
predictions or assessments, and we are less concerned with the
underlying patterns that were learned in order to make these
predictions.  This leads to what normally has been labeled as a
frequentist approach.
</p>
</section>

<section>
<h2 id="not-so-sharp-distinctions">Not so sharp distinctions </h2>

<p>You should keep in mind that the division between a traditional
frequentist approach with focus on predictions and correlations only
and a Bayesian approach with an emphasis on estimations and
causations, is not that sharp. Machine learning can be frequentist
with ensemble methods (EMB) as examples and Bayesian with Gaussian
Processes as examples.
</p>

<p>If one views ML from a statistical learning
perspective, one is then equally interested in estimating errors as
one is in finding correlations and making predictions. It is important
to keep in mind that the frequentist and Bayesian approaches differ
mainly in their interpretations of probability. In the frequentist
world, we can only assign probabilities to repeated random
phenomena. From the observations of these phenomena, we can infer the
probability of occurrence of a specific event.  In Bayesian
statistics, we assign probabilities to specific events and the
probability represents the measure of belief/confidence for that
event. The belief can be updated in the light of new evidence.
</p>
</section>

<section>
<h2 id="topics-we-have-covered-this-year">Topics we have covered this year </h2>

<p>The course has two central parts</p>

<ol>
<p><li> Statistical analysis and optimization of data</li>
<p><li> Machine learning</li>
</ol>
</section>

<section>
<h2 id="statistical-analysis-and-optimization-of-data">Statistical analysis and optimization of data </h2>

<p>The following topics have been discussed:</p>
<ol>
<p><li> Basic concepts, expectation values, variance, covariance, correlation functions and errors;</li>
<p><li> Simpler models, binomial distribution, the Poisson distribution, simple and multivariate normal distributions;</li>
<p><li> Central elements from linear algebra, matrix inversion and SVD</li>
<p><li> Gradient methods for data optimization</li>
<p><li> Estimation of errors using cross-validation, bootstrapping and jackknife methods;</li>
<p><li> Practical optimization using Singular-value decomposition and least squares for parameterizing data.</li>
<p><li> Principal Component Analysis to reduce the number of features.</li>
</ol>
</section>

<section>
<h2 id="machine-learning">Machine learning </h2>

<p>The following topics will be covered</p>
<ol>
<p><li> Linear methods for regression and classification:
<ol type="a"></li>
 <p><li> Ordinary Least Squares</li>
 <p><li> Ridge regression</li>
 <p><li> Lasso regression</li>
 <p><li> Logistic regression</li>
</ol>
<p>
<p><li> Neural networks and deep learning:
<ol type="a"></li>
 <p><li> Feed Forward Neural Networks</li>
 <p><li> Convolutional Neural Networks</li>
 <p><li> Recurrent Neural Networks</li>
</ol>
<p>
<p><li> Decisions trees and ensemble methods:
<ol type="a"></li>
 <p><li> Decision trees</li>
 <p><li> Bagging and voting</li>
 <p><li> Random forests</li>
 <p><li> Boosting and gradient boosting</li>
</ol>
<p>
<p><li> Support vector machines
<ol type="a"></li>
 <p><li> Binary classification and multiclass classification</li>
 <p><li> Kernel methods</li>
 <p><li> Regression</li>
</ol>
<p>
</ol>
</section>

<section>
<h2 id="learning-outcomes-and-overarching-aims-of-this-course">Learning outcomes and overarching aims of this course </h2>

<p>The course introduces a variety of central algorithms and methods
essential for studies of data analysis and machine learning. The
course is project based and through the various projects, normally
three, you will be exposed to fundamental research problems
in these fields, with the aim to reproduce state of the art scientific
results. The students will learn to develop and structure large codes
for studying these systems, get acquainted with computing facilities
and learn to handle large scientific projects. A good scientific and
ethical conduct is emphasized throughout the course. 
</p>

<ul>
<p><li> Understand linear methods for regression and classification;</li>
<p><li> Learn about neural network;</li>
<p><li> Learn about bagging, boosting and trees</li>
<p><li> Support vector machines</li>
<p><li> Learn about basic data analysis;</li>
<p><li> Be capable of extending the acquired knowledge to other systems and cases;</li>
<p><li> Have an understanding of central algorithms used in data analysis and machine learning;</li>
<p><li> Work on numerical projects to illustrate the theory. The projects play a central role and you are expected to know modern programming languages like Python or C++.</li>
</ul>
</section>

<section>
<h2 id="perspective-on-machine-learning">Perspective on Machine Learning </h2>

<ol>
<p><li> Rapidly emerging application area</li>
<p><li> Experiment AND theory are evolving in many many fields. Still many low-hanging fruits.</li>
<p><li> Requires education/retraining for more widespread adoption</li>
<p><li> A lot of &#8220;word-of-mouth&#8221; development methods</li>
</ol>
<p>
<p>Huge amounts of data sets require automation, classical analysis tools often inadequate. 
High energy physics hit this wall in the 90&#8217;s.
In 2009 single top quark production was determined via <a href="https://arxiv.org/pdf/0903.0850.pdf" target="_blank">Boosted decision trees, Bayesian
Neural Networks, etc.</a>. Similarly, the search for Higgs was a statistical learning tour de force. See this link on <a href="https://www.kaggle.com/c/higgs-boson" target="_blank">Kaggle.com</a>.
</p>
</section>

<section>
<h2 id="machine-learning-research">Machine Learning Research </h2>

<p>Where to find recent results:</p>
<ol>
<p><li> Conference proceedings, arXiv and blog posts!</li>
<p><li> <b>NIPS</b>: <a href="https://papers.nips.cc" target="_blank">Neural Information Processing Systems</a></li>
<p><li> <b>ICLR</b>: <a href="https://openreview.net/group?id=ICLR.cc/2018/Conference#accepted-oral-papers" target="_blank">International Conference on Learning Representations</a></li>
<p><li> <b>ICML</b>: International Conference on Machine Learning</li>
<p><li> <a href="http://www.jmlr.org/papers/v19/" target="_blank">Journal of Machine Learning Research</a></li> 
<p><li> <a href="https://arxiv.org/list/cs.LG/recent" target="_blank">Follow ML on ArXiv</a></li>
</ol>
</section>

<section>
<h2 id="starting-your-machine-learning-project">Starting your Machine Learning Project  </h2>

<ol>
<p><li> Identify problem type: classification, regression</li>
<p><li> Consider your data carefully</li>
<p><li> Choose a simple model that fits 1. and 2.</li>
<p><li> Consider your data carefully again! Think of data representation more carefully.</li>
<p><li> Based on your results, feedback loop to earliest possible point</li>
</ol>
</section>

<section>
<h2 id="choose-a-model-and-algorithm">Choose a Model and Algorithm  </h2>

<ol>
<p><li> Supervised?</li>
<p><li> Start with the simplest model that fits your problem</li>
<p><li> Start with minimal processing of data</li>
</ol>
</section>

<section>
<h2 id="preparing-your-data">Preparing Your Data </h2>

<ol>
<p><li> Shuffle your data</li>
<p><li> Mean center your data</li>
<ul>

<p><li> Why?</li>
</ul>
<p>
<p><li> Normalize the variance</li>
<ul>

<p><li> Why?</li>
</ul>
<p>
<p><li> <a href="https://multivariatestatsjl.readthedocs.io/en/latest/whiten.html" target="_blank">Whitening</a></li>
<ul>

<p><li> Decorrelates data</li>

<p><li> Can be hit or miss</li>
</ul>
<p>
<p><li> When to do train/test split?</li>
</ol>
<p>
<p>Whitening is a decorrelation transformation that transforms a set of
random variables into a set of new random variables with identity
covariance (uncorrelated with unit variances).
</p>
</section>

<section>
<h2 id="which-activation-and-weights-to-choose-in-neural-networks">Which Activation and Weights to Choose in Neural Networks </h2>

<ol>
<p><li> RELU? ELU?</li>
<p><li> Sigmoid or Tanh?</li>
<p><li> Set all weights to 0?</li>
<ul>

<p><li> Terrible idea</li>
</ul>
<p>
<p><li> Set all weights to random values?</li>
<ul>

<p><li> Small random values</li>
</ul>
<p>
</ol>
</section>

<section>
<h2 id="optimization-methods-and-hyperparameters">Optimization Methods and Hyperparameters </h2>
<ol>
<p><li> Stochastic gradient descent
<ol type="a"></li>
<p><li> Stochastic gradient descent + momentum</li>
</ol>
<p>
<p><li> State-of-the-art approaches:</li>
<ul>

<p><li> RMSProp</li>

<p><li> Adam</li>

<p><li> and more</li>
</ul>
<p>
</ol>
<p>
<p>Which regularization and hyperparameters? \( L_1 \) or \( L_2 \), soft
classifiers, depths of trees and many other. Need to explore a large
set of hyperparameters and regularization methods.
</p>
</section>

<section>
<h2 id="resampling">Resampling </h2>

<p>When do we resample?</p>

<ol>
<p><li> <a href="https://www.cambridge.org/core/books/bootstrap-methods-and-their-application/ED2FD043579F27952363566DC09CBD6A" target="_blank">Bootstrap</a></li>
<p><li> <a href="https://www.youtube.com/watch?v=fSytzGwwBVw&ab_channel=StatQuestwithJoshStarmer" target="_blank">Cross-validation</a></li>
<p><li> Jackknife and many other</li>
</ol>
</section>

<section>
<h2 id="other-courses-on-data-science-and-machine-learning-at-uio">Other courses on Data science and Machine Learning  at UiO </h2>

<p>The link here <a href="https://www.mn.uio.no/english/research/about/centre-focus/innovation/data-science/studies/" target="_blank"><tt>https://www.mn.uio.no/english/research/about/centre-focus/innovation/data-science/studies/</tt></a>  gives an excellent overview of courses on Machine learning at UiO.</p>

<ol>
<p><li> <a href="http://www.uio.no/studier/emner/matnat/math/STK2100/index-eng.html" target="_blank">STK2100 Machine learning and statistical methods for prediction and classification</a>.</li> 
<p><li> <a href="https://www.uio.no/studier/emner/matnat/ifi/IN3050/index-eng.html" target="_blank">IN3050/IN4050 Introduction to Artificial Intelligence and Machine Learning</a>. Introductory course in machine learning and AI with an algorithmic approach.</li> 
<p><li> <a href="http://www.uio.no/studier/emner/matnat/math/STK-INF3000/index-eng.html" target="_blank">STK-INF3000/4000 Selected Topics in Data Science</a>. The course provides insight into selected contemporary relevant topics within Data Science.</li> 
<p><li> <a href="https://www.uio.no/studier/emner/matnat/ifi/IN4080/index.html" target="_blank">IN4080 Natural Language Processing</a>. Probabilistic and machine learning techniques applied to natural language processing.</li> 
<p><li> <a href="https://www.uio.no/studier/emner/matnat/math/STK-IN4300/index-eng.html" target="_blank">STK-IN4300 &#8211; Statistical learning methods in Data Science</a>. An advanced introduction to statistical and machine learning. For students with a good mathematics and statistics background.</li>
<p><li> <a href="https://www.uio.no/studier/emner/matnat/ifi/IN-STK5000/index-eng.html" target="_blank">IN-STK5000  Adaptive Methods for Data-Based Decision Making</a>. Methods for adaptive collection and processing of data based on machine learning techniques.</li> 
<p><li> <a href="https://www.uio.no/studier/emner/matnat/ifi/IN5400/" target="_blank">IN5400/INF5860 &#8211; Machine Learning for Image Analysis</a>. An introduction to deep learning with particular emphasis on applications within Image analysis, but useful for other application areas too.</li>
<p><li> <a href="https://www.uio.no/studier/emner/matnat/its/TEK5040/" target="_blank">TEK5040 &#8211; Dyp l&#230;ring for autonome systemer</a>. The course addresses advanced algorithms and architectures for deep learning with neural networks. The course provides an introduction to how deep-learning techniques can be used in the construction of key parts of advanced autonomous systems that exist in physical environments and cyber environments.</li>
</ol>
</section>

<section>
<h2 id="additional-courses-of-interest">Additional courses of interest </h2>

<ol>
<p><li> <a href="https://www.uio.no/studier/emner/matnat/math/STK4051/index-eng.html" target="_blank">STK4051 Computational Statistics</a></li>
<p><li> <a href="https://www.uio.no/studier/emner/matnat/math/STK4021/index-eng.html" target="_blank">STK4021 Applied Bayesian Analysis and Numerical Methods</a></li>
</ol>
</section>

<section>
<h2 id="what-s-the-future-like">What's the future like?  </h2>

<p>Based on multi-layer nonlinear neural networks, deep learning can
learn directly from raw data, automatically extract and abstract
features from layer to layer, and then achieve the goal of regression,
classification, or ranking. Deep learning has made breakthroughs in
computer vision, speech processing and natural language, and reached
or even surpassed human level. The success of deep learning is mainly
due to the three factors: big data, big model, and big computing.
</p>

<p>In the past few decades, many different architectures of deep neural
networks have been proposed, such as
</p>
<ol>
<p><li> Convolutional neural networks, which are mostly used in image and video data processing, and have also been applied to sequential data such as text processing;</li>
<p><li> Recurrent neural networks, which can process sequential data of variable length and have been widely used in natural language understanding and speech processing;</li>
<p><li> Encoder-decoder framework, which is mostly used for image or sequence generation, such as machine translation, text summarization, and image captioning.</li>
</ol>
</section>

<section>
<h2 id="types-of-machine-learning-a-repetition">Types of Machine Learning, a repetition </h2>

<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>The approaches to machine learning are many, but are often split into two main categories. 
In <em>supervised learning</em> we know the answer to a problem,
and let the computer deduce the logic behind it. On the other hand, <em>unsupervised learning</em>
is a method for finding patterns and relationship in data sets without any prior knowledge of the system.
Some authours also operate with a third category, namely <em>reinforcement learning</em>. This is a paradigm 
of learning inspired by behavioural psychology, where learning is achieved by trial-and-error, 
solely from rewards and punishment.
</p>

<p>Another way to categorize machine learning tasks is to consider the desired output of a system.
Some of the most common tasks are:
</p>

<ul>

<p><li> Classification: Outputs are divided into two or more classes. The goal is to   produce a model that assigns inputs into one of these classes. An example is to identify  digits based on pictures of hand-written ones. Classification is typically supervised learning.</li>

<p><li> Regression: Finding a functional relationship between an input data set and a reference data set.   The goal is to construct a function that maps input data to continuous output values.</li>

<p><li> Clustering: Data are divided into groups with certain common traits, without knowing the different groups beforehand.  It is thus a form of unsupervised learning.</li>

<p><li> Other unsupervised learning algortihms like <b>Boltzmann machines</b></li>
</ul>
</div>
</section>

<section>
<h2 id="why-boltzmann-machines">Why Boltzmann machines? </h2>

<p>What is known as restricted Boltzmann Machines (RMB) have received a lot of attention lately. 
One of the major reasons is that they can be stacked layer-wise to build deep neural networks that capture complicated statistics.
</p>

<p>The original RBMs had just one visible layer and a hidden layer, but recently so-called Gaussian-binary RBMs have gained quite some popularity in imaging since they are capable of modeling continuous data that are common to natural images. </p>

<p>Furthermore, they have been used to solve complicated <a href="https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.91.045002" target="_blank">quantum mechanical many-particle problems or classical statistical physics problems like the Ising and Potts classes of models</a>. </p>
</section>

<section>
<h2 id="boltzmann-machines">Boltzmann Machines </h2>

<p>Why use a generative model rather than the more well known discriminative deep neural networks (DNN)? </p>

<ul>
<p><li> Discriminitave methods have several limitations: They are mainly supervised learning methods, thus requiring labeled data. And there are tasks they cannot accomplish, like drawing new examples from an unknown probability distribution.</li>
<p><li> A generative model can learn to represent and sample from a probability distribution. The core idea is to learn a parametric model of the probability distribution from which the training data was drawn. As an example
<ol type="a"></li>
 <p><li> A model for images could learn to draw new examples of cats and dogs, given a training dataset of images of cats and dogs.</li>
 <p><li> Generate a sample of an ordered or disordered phase, having been given samples of such phases.</li>
 <p><li> Model the trial function for <a href="https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.91.045002" target="_blank">Monte Carlo calculations</a>.</li>
</ol>
<p>
</ul>
</section>

<section>
<h2 id="some-similarities-and-differences-from-dnns">Some similarities and differences from DNNs </h2>

<ol>
<p><li> Both use gradient-descent based learning procedures for minimizing cost functions</li>
<p><li> Energy based models don't use backpropagation and automatic differentiation for computing gradients, instead turning to Markov Chain Monte Carlo methods.</li>
<p><li> DNNs often have several hidden layers. A restricted Boltzmann machine has only one hidden layer, however several RBMs can be stacked to make up Deep Belief Networks, of which they constitute the building blocks.</li>
</ol>
<p>
<p>History: The RBM was developed by amongst others <a href="https://en.wikipedia.org/wiki/Geoffrey_Hinton" target="_blank">Geoffrey Hinton</a>, called by some the "Godfather of Deep Learning", working with the University of Toronto and Google.</p>
</section>

<section>
<h2 id="boltzmann-machines-bm">Boltzmann machines (BM) </h2>

<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>A BM is what we would call an undirected probabilistic graphical model
with stochastic continuous or discrete units.
</p>
</div>

<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>It is interpreted as a stochastic recurrent neural network where the
state of each unit(neurons/nodes) depends on the units it is connected
to. The weights in the network represent thus the strength of the
interaction between various units/nodes.
</p>
</div>

<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>It turns into a Hopfield network if we choose deterministic rather
than stochastic units. In contrast to a Hopfield network, a BM is a
so-called generative model. It allows us to generate new samples from
the learned distribution.
</p>
</div>
</section>

<section>
<h2 id="a-standard-bm-setup">A standard BM setup </h2>

<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>A standard BM network is divided into a set of observable and visible units \( \hat{x} \) and a set of unknown hidden units/nodes \( \hat{h} \).</p>
</div>


<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>Additionally there can be bias nodes for the hidden and visible layers. These biases are normally set to \( 1 \).</p>
</div>


<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>BMs are stackable, meaning they cwe can train a BM which serves as input to another BM. We can construct deep networks for learning complex PDFs. The layers can be trained one after another, a feature which makes them popular in deep learning</p>
</div>

<p>However, they are often hard to train. This leads to the introduction of so-called restricted BMs, or RBMS.
Here we take away all lateral connections between nodes in the visible layer as well as connections between nodes in the hidden layer. The network is illustrated in the figure below.
</p>
</section>

<section>
<h2 id="the-structure-of-the-rbm-network">The structure of the RBM network </h2>

<br/><br/>
<center>
<p><img src="figures/RBM.png" width="800" align="bottom"></p>
</center>
<br/><br/>
</section>

<section>
<h2 id="the-network">The network </h2>

<b>The network layers</b>:
<ol>
 <p><li> A function \( \mathbf{x} \) that represents the visible layer, a vector of \( M \) elements (nodes). This layer represents both what the RBM might be given as training input, and what we want it to be able to reconstruct. This might for example be given by the pixels of an image or coefficients representing speech, or the coordinates of a quantum mechanical state function.</li>
 <p><li> The function \( \mathbf{h} \) represents the hidden, or latent, layer. A vector of \( N \) elements (nodes). Also called "feature detectors".</li>
</ol>
</section>

<section>
<h2 id="goals">Goals </h2>

<p>The goal of the hidden layer is to increase the model's expressive
power. We encode complex interactions between visible variables by
introducing additional, hidden variables that interact with visible
degrees of freedom in a simple manner, yet still reproduce the complex
correlations between visible degrees in the data once marginalized
over (integrated out).
</p>

<b>The network parameters, to be optimized/learned</b>:
<ol>
 <p><li> \( \mathbf{a} \) represents the visible bias, a vector of same length as \( \mathbf{x} \).</li>
 <p><li> \( \mathbf{b} \) represents the hidden bias, a vector of same lenght as \( \mathbf{h} \).</li>
 <p><li> \( W \) represents the interaction weights, a matrix of size \( M\times N \).</li>
</ol>
</section>

<section>
<h2 id="joint-distribution">Joint distribution </h2>

<p>The restricted Boltzmann machine is described by a Boltzmann distribution</p>
<p>&nbsp;<br>
$$
\begin{align}
	P_{rbm}(\mathbf{x},\mathbf{h}) = \frac{1}{Z} e^{-\frac{1}{T_0}E(\mathbf{x},\mathbf{h})},
\tag{6}
\end{align}
$$
<p>&nbsp;<br>

<p>where \( Z \) is the normalization constant or partition function, defined as </p>
<p>&nbsp;<br>
$$
\begin{align}
	Z = \int \int e^{-\frac{1}{T_0}E(\mathbf{x},\mathbf{h})} d\mathbf{x} d\mathbf{h}.
\tag{7}
\end{align}
$$
<p>&nbsp;<br>

<p>It is common to ignore \( T_0 \) by setting it to one. </p>
</section>

<section>
<h2 id="network-elements-the-energy-function">Network Elements, the energy function  </h2>

<p>The function \( E(\mathbf{x},\mathbf{h}) \) gives the <b>energy</b> of a
configuration (pair of vectors) \( (\mathbf{x}, \mathbf{h}) \). The lower
the energy of a configuration, the higher the probability of it. This
function also depends on the parameters \( \mathbf{a} \), \( \mathbf{b} \) and
\( W \). Thus, when we adjust them during the learning procedure, we are
adjusting the energy function to best fit our problem.
</p>

<p>An expression for the energy function is</p>
<p>&nbsp;<br>
$$
E(\hat{x},\hat{h}) = -\sum_{ia}^{NA}b_i^a \alpha_i^a(x_i)-\sum_{jd}^{MD}c_j^d \beta_j^d(h_j)-\sum_{ijad}^{NAMD}b_i^a \alpha_i^a(x_i)c_j^d \beta_j^d(h_j)w_{ij}^{ad}.
$$
<p>&nbsp;<br>

<p>Here \( \beta_j^d(h_j) \) and \( \alpha_i^a(x_j) \) are so-called transfer functions that map a given input value to a desired feature value. The labels \( a \) and \( d \) denote that there can be multiple transfer functions per variable. The first sum depends only on the visible units. The second on the hidden ones. <b>Note</b> that there is no connection between nodes in a layer.</p>

<p>The quantities \( b \) and \( c \) can be interpreted as the visible and hidden biases, respectively.</p>

<p>The connection between the nodes in the two layers is given by the weights \( w_{ij} \). </p>
</section>

<section>
<h2 id="defining-different-types-of-rbms">Defining different types of RBMs </h2>
<p>There are different variants of RBMs, and the differences lie in the types of visible and hidden units we choose as well as in the implementation of the energy function \( E(\mathbf{x},\mathbf{h}) \). </p>

<div class="alert alert-block alert-block alert-text-normal">
<b>Binary-Binary RBM:</b>
<p>

<p>RBMs were first developed using binary units in both the visible and hidden layer. The corresponding energy function is defined as follows:</p>
<p>&nbsp;<br>
$$
\begin{align}
	E(\mathbf{x}, \mathbf{h}) = - \sum_i^M x_i a_i- \sum_j^N b_j h_j - \sum_{i,j}^{M,N} x_i w_{ij} h_j,
\tag{8}
\end{align}
$$
<p>&nbsp;<br>

<p>where the binary values taken on by the nodes are most commonly 0 and 1.</p>
</div>

<div class="alert alert-block alert-block alert-text-normal">
<b>Gaussian-Binary RBM:</b>
<p>

<p>Another varient is the RBM where the visible units are Gaussian while the hidden units remain binary:</p>
<p>&nbsp;<br>
$$
\begin{align}
	E(\mathbf{x}, \mathbf{h}) = \sum_i^M \frac{(x_i - a_i)^2}{2\sigma_i^2} - \sum_j^N b_j h_j - \sum_{i,j}^{M,N} \frac{x_i w_{ij} h_j}{\sigma_i^2}. 
\tag{9}
\end{align}
$$
<p>&nbsp;<br>
</div>
</section>

<section>
<h2 id="more-about-rbms">More about RBMs </h2>
<ol>
<p><li> Useful when we model continuous data (i.e., we wish \( \mathbf{x} \) to be continuous)</li>
<p><li> Requires a smaller learning rate, since there's no upper bound to the value a component might take in the reconstruction</li>
</ol>
<p>
<p>Other types of units include:</p>
<ol>
<p><li> Softmax and multinomial units</li>
<p><li> Gaussian visible and hidden units</li>
<p><li> Binomial units</li>
<p><li> Rectified linear units</li>
</ol>
<p>
<p>To read more, see <a href="https://github.com/CompPhysics/ComputationalPhysics2/blob/gh-pages/doc/pub/notebook2/ipynb/notebook2.ipynb" target="_blank">Lectures on Boltzmann machines in Physics</a>.</p>
</section>

<section>
<h2 id="autoencoders-overarching-view">Autoencoders: Overarching view </h2>

<p>Autoencoders are artificial neural networks capable of learning
efficient representations of the input data (these representations are called codings)  without
any supervision (i.e., the training set is unlabeled). These codings
typically have a much lower dimensionality than the input data, making
autoencoders useful for dimensionality reduction. 
</p>

<p>More importantly, autoencoders act as powerful feature detectors, and
they can be used for unsupervised pretraining of deep neural networks.
</p>

<p>Lastly, they are capable of randomly generating new data that looks
very similar to the training data; this is called a generative
model. For example, you could train an autoencoder on pictures of
faces, and it would then be able to generate new faces.  Surprisingly,
autoencoders work by simply learning to copy their inputs to their
outputs. This may sound like a trivial task, but we will see that
constraining the network in various ways can make it rather
difficult. For example, you can limit the size of the internal
representation, or you can add noise to the inputs and train the
network to recover the original inputs. These constraints prevent the
autoencoder from trivially copying the inputs directly to the outputs,
which forces it to learn efficient ways of representing the data. In
short, the codings are byproducts of the autoencoder&#8217;s attempt to
learn the identity function under some constraints.
</p>

<a href="https://www.coursera.org/lecture/building-deep-learning-models-with-tensorflow/autoencoders-1U4L3" target="_blank">Video on autoencoders</a>

<p>See also A. Geron's textbook, chapter 15.</p>
</section>

<section>
<h2 id="bayesian-machine-learning">Bayesian Machine Learning </h2>

<p>This is an important topic if we aim at extracting a probability
distribution. This gives us also a confidence interval and error
estimates.
</p>

<p>Bayesian machine learning allows us to encode our prior beliefs about
what those models should look like, independent of what the data tells
us. This is especially useful when we don&#8217;t have a ton of data to
confidently learn our model.
</p>

<a href="https://www.youtube.com/watch?v=E1qhGw8QxqY&ab_channel=AndrewGordonWilson" target="_blank">Video on Bayesian deep learning</a>

<p>See also the <a href="https://github.com/CompPhysics/MachineLearning/blob/master/doc/Articles/lec03.pdf" target="_blank">slides here</a>.</p>
</section>

<section>
<h2 id="reinforcement-learning">Reinforcement Learning </h2>

<p>Reinforcement Learning (RL) is one of the most exciting fields of
Machine Learning today, and also one of the oldest. It has been around
since the 1950s, producing many interesting applications over the
years.
</p>

<p>It studies
how agents take actions based on trial and error, so as to maximize
some notion of cumulative reward in a dynamic system or
environment. Due to its generality, the problem has also been studied
in many other disciplines, such as game theory, control theory,
operations research, information theory, multi-agent systems, swarm
intelligence, statistics, and genetic algorithms.
</p>

<p>In March 2016, AlphaGo, a computer program that plays the board game
Go, beat Lee Sedol in a five-game match. This was the first time a
computer Go program had beaten a 9-dan (highest rank) professional
without handicaps. AlphaGo is based on deep convolutional neural
networks and reinforcement learning. AlphaGo&#8217;s victory was a major
milestone in artificial intelligence and it has also made
reinforcement learning a hot research area in the field of machine
learning.
</p>

<p><a href="https://www.youtube.com/watch?v=FgzM3zpZ55o&ab_channel=stanfordonline" target="_blank">Lecture on Reinforcement Learning</a>.</p>

<p>See also A. Geron's textbook, chapter 16.</p>
</section>

<section>
<h2 id="transfer-learning">Transfer learning </h2>

<p>The goal of transfer learning is to transfer the model or knowledge
obtained from a source task to the target task, in order to resolve
the issues of insufficient training data in the target task. The
rationality of doing so lies in that usually the source and target
tasks have inter-correlations, and therefore either the features,
samples, or models in the source task might provide useful information
for us to better solve the target task. Transfer learning is a hot
research topic in recent years, with many problems still waiting to be studied.
</p>

<p><a href="https://www.ias.edu/video/machinelearning/2020/0331-SamoryKpotufe" target="_blank">Lecture on transfer learning</a>.</p>
</section>

<section>
<h2 id="adversarial-learning">Adversarial learning </h2>

<p>The conventional deep generative model has a potential problem: the
model tends to generate extreme instances to maximize the
probabilistic likelihood, which will hurt its performance. Adversarial
learning utilizes the adversarial behaviors (e.g., generating
adversarial instances or training an adversarial model) to enhance the
robustness of the model and improve the quality of the generated
data. In recent years, one of the most promising unsupervised learning
technologies, generative adversarial networks (GAN), has already been
successfully applied to image, speech, and text.
</p>

<p><a href="https://www.youtube.com/watch?v=CIfsB_EYsVI&ab_channel=StanfordUniversitySchoolofEngineering" target="_blank">Lecture on adversial learning</a>.</p>
</section>

<section>
<h2 id="dual-learning">Dual learning </h2>

<p>Dual learning is a new learning paradigm, the basic idea of which is
to use the primal-dual structure between machine learning tasks to
obtain effective feedback/regularization, and guide and strengthen the
learning process, thus reducing the requirement of large-scale labeled
data for deep learning. The idea of dual learning has been applied to
many problems in machine learning, including machine translation,
image style conversion, question answering and generation, image
classification and generation, text classification and generation,
image-to-text, and text-to-image.
</p>
</section>

<section>
<h2 id="distributed-machine-learning">Distributed machine learning </h2>

<p>Distributed computation will speed up machine learning algorithms,
significantly improve their efficiency, and thus enlarge their
application. When distributed meets machine learning, more than just
implementing the machine learning algorithms in parallel is required.
</p>
</section>

<section>
<h2 id="meta-learning">Meta learning </h2>

<p>Meta learning is an emerging research direction in machine
learning. Roughly speaking, meta learning concerns learning how to
learn, and focuses on the understanding and adaptation of the learning
itself, instead of just completing a specific learning task. That is,
a meta learner needs to be able to evaluate its own learning methods
and adjust its own learning methods according to specific learning
tasks.
</p>
</section>

<section>
<h2 id="the-challenges-facing-machine-learning">The Challenges Facing Machine Learning </h2>

<p>While there has been much progress in machine learning, there are also challenges.</p>

<p>For example, the mainstream machine learning technologies are
black-box approaches, making us concerned about their potential
risks. To tackle this challenge, we may want to make machine learning
more explainable and controllable. As another example, the
computational complexity of machine learning algorithms is usually
very high and we may want to invent lightweight algorithms or
implementations. Furthermore, in many domains such as physics,
chemistry, biology, and social sciences, people usually seek elegantly
simple equations (e.g., the Schr&#246;dinger equation) to uncover the
underlying laws behind various phenomena. In the field of machine
learning, can we reveal simple laws instead of designing more complex
models for data fitting? Although there are many challenges, we are
still very optimistic about the future of machine learning. As we look
forward to the future, here are what we think the research hotspots in
the next ten years will be.
</p>

<p>See the article on <a href="https://www.frontiersin.org/articles/10.3389/frai.2020.00025/full" target="_blank">Discovery of Physics From Data: Universal Laws and Discrepancies</a></p>
</section>

<section>
<h2 id="explainable-machine-learning">Explainable machine learning </h2>

<p>Machine learning, especially deep learning, evolves rapidly. The
ability gap between machine and human on many complex cognitive tasks
becomes narrower and narrower. However, we are still in the very early
stage in terms of explaining why those effective models work and how
they work.
</p>

<p><b>What is missing: the gap between correlation and causation</b>. Standard Machine Learning is based on what e have called a frequentist approach. </p>

<p>Most
machine learning techniques, especially the statistical ones, depend
highly on correlations in data sets to make predictions and analyses. In
contrast, rational humans tend to reply on clear and trustworthy
causality relations obtained via logical reasoning on real and clear
facts. It is one of the core goals of explainable machine learning to
transition from solving problems by data correlation to solving
problems by logical reasoning.
</p>

<b>Bayesian Machine Learning is one of the exciting research directions in this field</b>.
</section>

<section>
<h2 id="scientific-machine-learning">Scientific Machine Learning </h2>

<p>An important and emerging field is what has been dubbed as scientific ML, see the article by Deiana et al <a href="https://arxiv.org/abs/2110.13041" target="_blank">Applications and Techniques for Fast Machine Learning in Science, arXiv:2110.13041</a></p>

<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>The authors discuss applications and techniques for fast machine
learning (ML) in science &ndash; the concept of integrating power ML
methods into the real-time experimental data processing loop to
accelerate scientific discovery. The report covers three main areas
</p>

<ol>
<p><li> applications for fast ML across a number of scientific domains;</li>
<p><li> techniques for training and implementing performant and resource-efficient ML algorithms;</li>
<p><li> and computing architectures, platforms, and technologies for deploying these algorithms.</li>
</ol>
</div>
</section>

<section>
<h2 id="quantum-machine-learning">Quantum machine learning </h2>

<p>Quantum machine learning is an emerging interdisciplinary research
area at the intersection of quantum computing and machine learning.
</p>

<p>Quantum computers use effects such as quantum coherence and quantum
entanglement to process information, which is fundamentally different
from classical computers. Quantum algorithms have surpassed the best
classical algorithms in several problems (e.g., searching for an
unsorted database, inverting a sparse matrix), which we call quantum
acceleration.
</p>

<p>When quantum computing meets machine learning, it can be a mutually
beneficial and reinforcing process, as it allows us to take advantage
of quantum computing to improve the performance of classical machine
learning algorithms. In addition, we can also use the machine learning
algorithms (on classic computers) to analyze and improve quantum
computing systems.
</p>

<p><a href="https://www.youtube.com/watch?v=Xh9pUu3-WxM&ab_channel=InstituteforPure%26AppliedMathematics%28IPAM%29" target="_blank">Lecture on Quantum ML</a>.</p>

<p><a href="https://physics.aps.org/articles/v13/179?utm_campaign=weekly&utm_medium=email&utm_source=emailalert" target="_blank">Read interview with Maria Schuld on her work on Quantum Machine Learning</a>. See also <a href="https://www.springer.com/gp/book/9783319964232" target="_blank">her recent textbook</a>. </p>
</section>

<section>
<h2 id="quantum-machine-learning-algorithms-based-on-linear-algebra">Quantum machine learning algorithms based on linear algebra </h2>

<p>Many quantum machine learning algorithms are based on variants of
quantum algorithms for solving linear equations, which can efficiently
solve N-variable linear equations with complexity of O(log2 N) under
certain conditions. The quantum matrix inversion algorithm can
accelerate many machine learning methods, such as least square linear
regression, least square version of support vector machine, Gaussian
process, and more. The training of these algorithms can be simplified
to solve linear equations. The key bottleneck of this type of quantum
machine learning algorithms is data input&#8212;that is, how to initialize
the quantum system with the entire data set. Although efficient
data-input algorithms exist for certain situations, how to efficiently
input data into a quantum system is as yet unknown for most cases.
</p>
</section>

<section>
<h2 id="quantum-reinforcement-learning">Quantum reinforcement learning </h2>

<p>In quantum reinforcement learning, a quantum agent interacts with the
classical environment to obtain rewards from the environment, so as to
adjust and improve its behavioral strategies. In some cases, it
achieves quantum acceleration by the quantum processing capabilities
of the agent or the possibility of exploring the environment through
quantum superposition. Such algorithms have been proposed in
superconducting circuits and systems of trapped ions.
</p>
</section>

<section>
<h2 id="quantum-deep-learning">Quantum deep learning </h2>

<p>Dedicated quantum information processors, such as quantum annealers
and programmable photonic circuits, are well suited for building deep
quantum networks. The simplest deep quantum network is the Boltzmann
machine. The classical Boltzmann machine consists of bits with tunable
interactions and is trained by adjusting the interaction of these bits
so that the distribution of its expression conforms to the statistics
of the data. To quantize the Boltzmann machine, the neural network can
simply be represented as a set of interacting quantum spins that
correspond to an adjustable Ising model. Then, by initializing the
input neurons in the Boltzmann machine to a fixed state and allowing
the system to heat up, we can read out the output qubits to get the
result.
</p>
</section>

<section>
<h2 id="social-machine-learning">Social machine learning </h2>

<p>Machine learning aims to imitate how humans
learn. While we have developed successful machine learning algorithms,
until now we have ignored one important fact: humans are social. Each
of us is one part of the total society and it is difficult for us to
live, learn, and improve ourselves, alone and isolated. Therefore, we
should design machines with social properties. Can we let machines
evolve by imitating human society so as to achieve more effective,
intelligent, interpretable &#8220;social machine learning&#8221;?
</p>

<p>And much more.</p>
</section>

<section>
<h2 id="the-last-words">The last words? </h2>

<p>Early computer scientist Alan Kay said, <b>The best way to predict the
future is to create it</b>. Therefore, all machine learning
practitioners, whether scholars or engineers, professors or students,
need to work together to advance these important research
topics. Together, we will not just predict the future, but create it.
</p>
</section>

<section>
<h2 id="ai-ml-and-some-statements-you-may-have-heard-and-what-do-they-mean">AI/ML and some statements you may have heard (and what do they mean?)  </h2>

<ol>
<p><li> Fei-Fei Li on ImageNet: <b>map out the entire world of objects</b> (<a href="https://cacm.acm.org/news/219702-the-data-that-transformed-ai-research-and-possibly-the-world/fulltext" target="_blank">The data that transformed AI research</a>)</li>
<p><li> Russell and Norvig in their popular textbook: <b>relevant to any intellectual task; it is truly a universal field</b> (<a href="http://aima.cs.berkeley.edu/" target="_blank">Artificial Intelligence, A modern approach</a>)</li>
<p><li> Woody Bledsoe puts it more bluntly: <b>in the long run, AI is the only science</b> (quoted in Pamilla McCorduck, <a href="https://www.pamelamccorduck.com/machines-who-think" target="_blank">Machines who think</a>)</li>
</ol>
<p>
<p>If you wish to have a critical read on AI/ML from a societal point of view, see <a href="https://www.katecrawford.net/" target="_blank">Kate Crawford's recent text Atlas of AI</a></p>

<b>Here: with AI/ML we intend a collection of machine learning methods with an emphasis on statistical learning and data analysis</b>
</section>

<section>
<h2 id="best-wishes-to-you-all-and-thanks-so-much-for-your-heroic-efforts-this-semester">Best wishes to you all and thanks so much for your heroic efforts this semester </h2>

<br/><br/>
<center>
<p><img src="figures/Nebbdyr2.png" width="500" align="bottom"></p>
</center>
<br/><br/>
</section>



</div> <!-- class="slides" -->
</div> <!-- class="reveal" -->

<script src="reveal.js/lib/js/head.min.js"></script>
<script src="reveal.js/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

  // Display navigation controls in the bottom right corner
  controls: true,

  // Display progress bar (below the horiz. slider)
  progress: true,

  // Display the page number of the current slide
  slideNumber: true,

  // Push each slide change to the browser history
  history: false,

  // Enable keyboard shortcuts for navigation
  keyboard: true,

  // Enable the slide overview mode
  overview: true,

  // Vertical centering of slides
  //center: true,
  center: false,

  // Enables touch navigation on devices with touch input
  touch: true,

  // Loop the presentation
  loop: false,

  // Change the presentation direction to be RTL
  rtl: false,

  // Turns fragments on and off globally
  fragments: true,

  // Flags if the presentation is running in an embedded mode,
  // i.e. contained within a limited portion of the screen
  embedded: false,

  // Number of milliseconds between automatically proceeding to the
  // next slide, disabled when set to 0, this value can be overwritten
  // by using a data-autoslide attribute on your slides
  autoSlide: 0,

  // Stop auto-sliding after user input
  autoSlideStoppable: true,

  // Enable slide navigation via mouse wheel
  mouseWheel: false,

  // Hides the address bar on mobile devices
  hideAddressBar: true,

  // Opens links in an iframe preview overlay
  previewLinks: false,

  // Transition style
  transition: 'default', // default/cube/page/concave/zoom/linear/fade/none

  // Transition speed
  transitionSpeed: 'default', // default/fast/slow

  // Transition style for full page slide backgrounds
  backgroundTransition: 'default', // default/none/slide/concave/convex/zoom

  // Number of slides away from the current that are visible
  viewDistance: 3,

  // Parallax background image
    //parallaxBackgroundImage: '', // e.g. "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'"

  // Parallax background size
  //parallaxBackgroundSize: '' // CSS syntax, e.g. "2100px 900px"

  theme: Reveal.getQueryHash().theme, // available themes are in reveal.js/css/theme
    transition: Reveal.getQueryHash().transition || 'none', // default/cube/page/concave/zoom/linear/none

});

Reveal.initialize({
  dependencies: [
      // Cross-browser shim that fully implements classList - https://github.com/eligrey/classList.js/
      { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },

      // Interpret Markdown in <section> elements
      { src: 'reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
      { src: 'reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },

      // Syntax highlight for <code> elements
      { src: 'reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },

      // Zoom in and out with Alt+click
      { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },

      // Speaker notes
      { src: 'reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },

      // Remote control your reveal.js presentation using a touch device
      //{ src: 'reveal.js/plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } },

      // MathJax
      //{ src: 'reveal.js/plugin/math/math.js', async: true }
  ]
});

Reveal.initialize({

  // The "normal" size of the presentation, aspect ratio will be preserved
  // when the presentation is scaled to fit different resolutions. Can be
  // specified using percentage units.
  width: 1170,  // original: 960,
  height: 700,

  // Factor of the display size that should remain empty around the content
  margin: 0.1,

  // Bounds for smallest/largest possible scale to apply to content
  minScale: 0.2,
  maxScale: 1.0

});
</script>

<!-- begin footer logo
<div style="position: absolute; bottom: 0px; left: 0; margin-left: 0px">
<img src="somelogo.png">
</div>
   end footer logo -->




</body>
</html>

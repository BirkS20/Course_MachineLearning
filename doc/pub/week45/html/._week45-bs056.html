<!--
Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Week 45: Decisions Trees, Random Forests, Bagging  and Boosting">

<title>Week 45: Decisions Trees, Random Forests, Bagging  and Boosting</title>

<!-- Bootstrap style: bootstrap -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->

<style type="text/css">

/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}

/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>


</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Overview of week 45', 2, None, '___sec0'),
              ('Decision trees, overarching aims', 2, None, '___sec1'),
              ('Basics of a tree', 2, None, '___sec2'),
              ('A Sketch of a Tree, Regression problem', 2, None, '___sec3'),
              ('A Sketch of a Tree, Classification  problem',
               2,
               None,
               '___sec4'),
              ('A typical Decision Tree with its pertinent Jargon, '
               'Classification Problem',
               2,
               None,
               '___sec5'),
              ('General Features', 2, None, '___sec6'),
              ('How do we set it up?', 2, None, '___sec7'),
              ('Decision trees and Regression', 2, None, '___sec8'),
              ('Building a tree, regression', 2, None, '___sec9'),
              ('A top-down approach, recursive binary splitting',
               2,
               None,
<<<<<<< HEAD
               'a-top-down-approach-recursive-binary-splitting'),
              ('Making a tree', 2, None, 'making-a-tree'),
              ('Pruning the tree', 2, None, 'pruning-the-tree'),
              ('Cost complexity pruning', 2, None, 'cost-complexity-pruning'),
              ('Schematic Regression Procedure',
               2,
               None,
               'schematic-regression-procedure'),
              ('A Classification Tree', 2, None, 'a-classification-tree'),
              ('Growing a classification tree',
               2,
               None,
               'growing-a-classification-tree'),
              ('Classification tree, how to split nodes',
               2,
               None,
               'classification-tree-how-to-split-nodes'),
              ('Gini Index?Coefficient/Impurity',
               2,
               None,
               'gini-index-coefficient-impurity'),
              ('Why binary split?', 2, None, 'why-binary-split'),
              ('Visualizing the Tree, Classification',
               2,
               None,
               'visualizing-the-tree-classification'),
              ('Visualizing the Tree, The Moons',
               2,
               None,
               'visualizing-the-tree-the-moons'),
              ('Other ways of visualizing the trees',
               2,
               None,
               'other-ways-of-visualizing-the-trees'),
              ('Printing out as text', 2, None, 'printing-out-as-text'),
              ('Algorithms for Setting up Decision Trees',
               2,
               None,
               'algorithms-for-setting-up-decision-trees'),
              ('The CART algorithm for Classification',
               2,
               None,
               'the-cart-algorithm-for-classification'),
              ('The CART algorithm for Regression',
               2,
               None,
               'the-cart-algorithm-for-regression'),
              ('Computing the Gini index', 2, None, 'computing-the-gini-index'),
=======
               '___sec10'),
              ('Making a tree', 2, None, '___sec11'),
              ('Pruning the tree', 2, None, '___sec12'),
              ('Cost complexity pruning', 2, None, '___sec13'),
              ('Schematic Regression Procedure', 2, None, '___sec14'),
              ('A Classification Tree', 2, None, '___sec15'),
              ('Growing a classification tree', 2, None, '___sec16'),
              ('Classification tree, how to split nodes', 2, None, '___sec17'),
              ('Visualizing the Tree, Classification', 2, None, '___sec18'),
              ('Visualizing the Tree, The Moons', 2, None, '___sec19'),
              ('Other ways of visualizing the trees', 2, None, '___sec20'),
              ('Printing out as text', 2, None, '___sec21'),
              ('Algorithms for Setting up Decision Trees', 2, None, '___sec22'),
              ('The CART algorithm for Classification', 2, None, '___sec23'),
              ('The CART algorithm for Regression', 2, None, '___sec24'),
              ('Computing the Gini index', 2, None, '___sec25'),
>>>>>>> 0e5076dfbfc8cf7946a5cec60d742ecccdbbcd3e
              ('Simple Python Code to read in Data and perform Classification',
               2,
               None,
               '___sec26'),
              ('Computing the Gini Factor', 2, None, '___sec27'),
              ('Entropy and the ID3 algorithm', 2, None, '___sec28'),
              ('Cancer Data again now with Decision Trees and other Methods',
               2,
               None,
               '___sec29'),
              ('Another example, the moons again', 2, None, '___sec30'),
              ('Playing around with regions', 2, None, '___sec31'),
              ('Regression trees', 2, None, '___sec32'),
              ('Final regressor code', 2, None, '___sec33'),
              ('Pros and cons of trees, pros', 2, None, '___sec34'),
              ('Disadvantages', 2, None, '___sec35'),
              ('Ensemble Methods: From a Single Tree to Many Trees and Extreme '
               'Boosting, Meet the Jungle of Methods',
               2,
               None,
               '___sec36'),
              ('An Overview of Ensemble Methods', 2, None, '___sec37'),
              ('Bagging', 2, None, '___sec38'),
              ('More bagging', 2, None, '___sec39'),
              ('Making your own Bootstrap: Changing the Level of the Decision '
               'Tree',
               2,
               None,
               '___sec40'),
              ('Why Voting?', 2, None, '___sec41'),
              ('Tossing coins', 2, None, '___sec42'),
              ('Standard imports first', 2, None, '___sec43'),
              ('Simple Voting Example, head or tail', 2, None, '___sec44'),
              ('Using the Voting Classifier', 2, None, '___sec45'),
              ('Voting and Bagging', 2, None, '___sec46'),
              ('Random forests', 2, None, '___sec47'),
              ('Random Forest Algorithm', 2, None, '___sec48'),
              ('Random Forests Compared with other Methods on the Cancer Data',
               2,
               None,
               '___sec49'),
              ('Compare  Bagging on Trees with Random Forests',
               2,
               None,
               '___sec50'),
              ("Boosting, a Bird's Eye View", 2, None, '___sec51'),
              ('What is boosting? Additive Modelling/Iterative Fitting',
               2,
               None,
               '___sec52'),
              ('Iterative Fitting, Regression and Squared-error Cost Function',
               2,
               None,
               '___sec53'),
              ('Squared-Error Example and Iterative Fitting',
               2,
               None,
               '___sec54'),
              ('Iterative Fitting, Classification and AdaBoost',
               2,
               None,
               '___sec55'),
              ('Adaptive Boosting, AdaBoost', 2, None, '___sec56'),
              ('Building up AdaBoost', 2, None, '___sec57'),
              ('Adaptive boosting: AdaBoost, Basic Algorithm',
               2,
               None,
               '___sec58'),
              ('Basic Steps of AdaBoost', 2, None, '___sec59'),
              ('AdaBoost Examples', 2, None, '___sec60'),
              ('Gradient boosting: Basics with Steepest Descent/Functional '
               'Gradient Descent',
               2,
               None,
               '___sec61'),
              ('The Squared-Error again! Steepest Descent',
               2,
               None,
               '___sec62'),
              ('Steepest Descent Example', 2, None, '___sec63'),
              ('Gradient Boosting, algorithm', 2, None, '___sec64'),
              ('Gradient Boosting, Examples of Regression',
               2,
               None,
               '___sec65'),
              ('Gradient Boosting, Classification Example',
               2,
               None,
               '___sec66'),
              ('XGBoost: Extreme Gradient Boosting', 2, None, '___sec67'),
              ('Regression Case', 2, None, '___sec68'),
              ('Xgboost on the Cancer Data', 2, None, '___sec69')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="week45-bs.html">Week 45: Decisions Trees, Random Forests, Bagging  and Boosting</a>
  </div>

  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
<<<<<<< HEAD
     <!-- navigation toc: --> <li><a href="._week45-bs001.html#overview-of-week-45" style="font-size: 80%;">Overview of week 45</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs002.html#decision-trees-overarching-aims" style="font-size: 80%;">Decision trees, overarching aims</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs003.html#basics-of-a-tree" style="font-size: 80%;">Basics of a tree</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs004.html#a-sketch-of-a-tree-regression-problem" style="font-size: 80%;">A Sketch of a Tree, Regression problem</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs005.html#a-sketch-of-a-tree-classification-problem" style="font-size: 80%;">A Sketch of a Tree, Classification  problem</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs006.html#a-typical-decision-tree-with-its-pertinent-jargon-classification-problem" style="font-size: 80%;">A typical Decision Tree with its pertinent Jargon, Classification Problem</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs007.html#general-features" style="font-size: 80%;">General Features</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs008.html#how-do-we-set-it-up" style="font-size: 80%;">How do we set it up?</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs009.html#decision-trees-and-regression" style="font-size: 80%;">Decision trees and Regression</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs010.html#building-a-tree-regression" style="font-size: 80%;">Building a tree, regression</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs011.html#a-top-down-approach-recursive-binary-splitting" style="font-size: 80%;">A top-down approach, recursive binary splitting</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs012.html#making-a-tree" style="font-size: 80%;">Making a tree</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs013.html#pruning-the-tree" style="font-size: 80%;">Pruning the tree</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs014.html#cost-complexity-pruning" style="font-size: 80%;">Cost complexity pruning</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs015.html#schematic-regression-procedure" style="font-size: 80%;">Schematic Regression Procedure</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs016.html#a-classification-tree" style="font-size: 80%;">A Classification Tree</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs017.html#growing-a-classification-tree" style="font-size: 80%;">Growing a classification tree</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs018.html#classification-tree-how-to-split-nodes" style="font-size: 80%;">Classification tree, how to split nodes</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs019.html#gini-index-coefficient-impurity" style="font-size: 80%;">Gini Index?Coefficient/Impurity</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs020.html#why-binary-split" style="font-size: 80%;">Why binary split?</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs021.html#visualizing-the-tree-classification" style="font-size: 80%;">Visualizing the Tree, Classification</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs022.html#visualizing-the-tree-the-moons" style="font-size: 80%;">Visualizing the Tree, The Moons</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs023.html#other-ways-of-visualizing-the-trees" style="font-size: 80%;">Other ways of visualizing the trees</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs024.html#printing-out-as-text" style="font-size: 80%;">Printing out as text</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs025.html#algorithms-for-setting-up-decision-trees" style="font-size: 80%;">Algorithms for Setting up Decision Trees</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs026.html#the-cart-algorithm-for-classification" style="font-size: 80%;">The CART algorithm for Classification</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs027.html#the-cart-algorithm-for-regression" style="font-size: 80%;">The CART algorithm for Regression</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs028.html#computing-the-gini-index" style="font-size: 80%;">Computing the Gini index</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs029.html#simple-python-code-to-read-in-data-and-perform-classification" style="font-size: 80%;">Simple Python Code to read in Data and perform Classification</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs030.html#computing-the-gini-factor" style="font-size: 80%;">Computing the Gini Factor</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs031.html#entropy-and-the-id3-algorithm" style="font-size: 80%;">Entropy and the ID3 algorithm</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs032.html#cancer-data-again-now-with-decision-trees-and-other-methods" style="font-size: 80%;">Cancer Data again now with Decision Trees and other Methods</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs033.html#another-example-the-moons-again" style="font-size: 80%;">Another example, the moons again</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs034.html#playing-around-with-regions" style="font-size: 80%;">Playing around with regions</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs035.html#regression-trees" style="font-size: 80%;">Regression trees</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs036.html#final-regressor-code" style="font-size: 80%;">Final regressor code</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs037.html#pros-and-cons-of-trees-pros" style="font-size: 80%;">Pros and cons of trees, pros</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs038.html#disadvantages" style="font-size: 80%;">Disadvantages</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs039.html#ensemble-methods-from-a-single-tree-to-many-trees-and-extreme-boosting-meet-the-jungle-of-methods" style="font-size: 80%;">Ensemble Methods: From a Single Tree to Many Trees and Extreme Boosting, Meet the Jungle of Methods</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs040.html#an-overview-of-ensemble-methods" style="font-size: 80%;">An Overview of Ensemble Methods</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs041.html#bagging" style="font-size: 80%;">Bagging</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs042.html#more-bagging" style="font-size: 80%;">More bagging</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs051.html#simple-voting-example-head-or-tail" style="font-size: 80%;">Simple Voting Example, head or tail</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs052.html#using-the-voting-classifier" style="font-size: 80%;">Using the Voting Classifier</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs045.html#please-not-the-moons-again-voting-and-bagging" style="font-size: 80%;">Please, not the moons again! Voting and Bagging</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs046.html#bagging-examples" style="font-size: 80%;">Bagging Examples</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs047.html#making-your-own-bootstrap-changing-the-level-of-the-decision-tree" style="font-size: 80%;">Making your own Bootstrap: Changing the Level of the Decision Tree</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs048.html#why-voting" style="font-size: 80%;">Why Voting?</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs049.html#tossing-coins" style="font-size: 80%;">Tossing coins</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs050.html#standard-imports-first" style="font-size: 80%;">Standard imports first</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs051.html#simple-voting-example-head-or-tail" style="font-size: 80%;">Simple Voting Example, head or tail</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs052.html#using-the-voting-classifier" style="font-size: 80%;">Using the Voting Classifier</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs053.html#voting-and-bagging" style="font-size: 80%;">Voting and Bagging</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs054.html#random-forests" style="font-size: 80%;">Random forests</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs055.html#random-forest-algorithm" style="font-size: 80%;">Random Forest Algorithm</a></li>
     <!-- navigation toc: --> <li><a href="#random-forests-compared-with-other-methods-on-the-cancer-data" style="font-size: 80%;">Random Forests Compared with other Methods on the Cancer Data</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs057.html#compare-bagging-on-trees-with-random-forests" style="font-size: 80%;">Compare  Bagging on Trees with Random Forests</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs058.html#boosting-a-bird-s-eye-view" style="font-size: 80%;">Boosting, a Bird's Eye View</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs059.html#what-is-boosting-additive-modelling-iterative-fitting" style="font-size: 80%;">What is boosting? Additive Modelling/Iterative Fitting</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs060.html#iterative-fitting-regression-and-squared-error-cost-function" style="font-size: 80%;">Iterative Fitting, Regression and Squared-error Cost Function</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs061.html#squared-error-example-and-iterative-fitting" style="font-size: 80%;">Squared-Error Example and Iterative Fitting</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs062.html#iterative-fitting-classification-and-adaboost" style="font-size: 80%;">Iterative Fitting, Classification and AdaBoost</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs063.html#adaptive-boosting-adaboost" style="font-size: 80%;">Adaptive Boosting, AdaBoost</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs064.html#building-up-adaboost" style="font-size: 80%;">Building up AdaBoost</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs065.html#adaptive-boosting-adaboost-basic-algorithm" style="font-size: 80%;">Adaptive boosting: AdaBoost, Basic Algorithm</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs066.html#basic-steps-of-adaboost" style="font-size: 80%;">Basic Steps of AdaBoost</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs067.html#adaboost-examples" style="font-size: 80%;">AdaBoost Examples</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs068.html#gradient-boosting-basics-with-steepest-descent-functional-gradient-descent" style="font-size: 80%;">Gradient boosting: Basics with Steepest Descent/Functional Gradient Descent</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs069.html#the-squared-error-again-steepest-descent" style="font-size: 80%;">The Squared-Error again! Steepest Descent</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs070.html#steepest-descent-example" style="font-size: 80%;">Steepest Descent Example</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs071.html#gradient-boosting-algorithm" style="font-size: 80%;">Gradient Boosting, algorithm</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs072.html#gradient-boosting-examples-of-regression" style="font-size: 80%;">Gradient Boosting, Examples of Regression</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs073.html#gradient-boosting-classification-example" style="font-size: 80%;">Gradient Boosting, Classification Example</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs074.html#xgboost-extreme-gradient-boosting" style="font-size: 80%;">XGBoost: Extreme Gradient Boosting</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs075.html#regression-case" style="font-size: 80%;">Regression Case</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs076.html#xgboost-on-the-cancer-data" style="font-size: 80%;">Xgboost on the Cancer Data</a></li>
=======
     <!-- navigation toc: --> <li><a href="._week45-bs001.html#___sec0" style="font-size: 80%;">Overview of week 45</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs002.html#___sec1" style="font-size: 80%;">Decision trees, overarching aims</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs003.html#___sec2" style="font-size: 80%;">Basics of a tree</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs004.html#___sec3" style="font-size: 80%;">A Sketch of a Tree, Regression problem</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs005.html#___sec4" style="font-size: 80%;">A Sketch of a Tree, Classification  problem</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs006.html#___sec5" style="font-size: 80%;">A typical Decision Tree with its pertinent Jargon, Classification Problem</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs007.html#___sec6" style="font-size: 80%;">General Features</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs008.html#___sec7" style="font-size: 80%;">How do we set it up?</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs009.html#___sec8" style="font-size: 80%;">Decision trees and Regression</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs010.html#___sec9" style="font-size: 80%;">Building a tree, regression</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs011.html#___sec10" style="font-size: 80%;">A top-down approach, recursive binary splitting</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs012.html#___sec11" style="font-size: 80%;">Making a tree</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs013.html#___sec12" style="font-size: 80%;">Pruning the tree</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs014.html#___sec13" style="font-size: 80%;">Cost complexity pruning</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs015.html#___sec14" style="font-size: 80%;">Schematic Regression Procedure</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs016.html#___sec15" style="font-size: 80%;">A Classification Tree</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs017.html#___sec16" style="font-size: 80%;">Growing a classification tree</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs018.html#___sec17" style="font-size: 80%;">Classification tree, how to split nodes</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs019.html#___sec18" style="font-size: 80%;">Visualizing the Tree, Classification</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs020.html#___sec19" style="font-size: 80%;">Visualizing the Tree, The Moons</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs021.html#___sec20" style="font-size: 80%;">Other ways of visualizing the trees</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs022.html#___sec21" style="font-size: 80%;">Printing out as text</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs023.html#___sec22" style="font-size: 80%;">Algorithms for Setting up Decision Trees</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs024.html#___sec23" style="font-size: 80%;">The CART algorithm for Classification</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs025.html#___sec24" style="font-size: 80%;">The CART algorithm for Regression</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs026.html#___sec25" style="font-size: 80%;">Computing the Gini index</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs027.html#___sec26" style="font-size: 80%;">Simple Python Code to read in Data and perform Classification</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs028.html#___sec27" style="font-size: 80%;">Computing the Gini Factor</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs029.html#___sec28" style="font-size: 80%;">Entropy and the ID3 algorithm</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs030.html#___sec29" style="font-size: 80%;">Cancer Data again now with Decision Trees and other Methods</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs031.html#___sec30" style="font-size: 80%;">Another example, the moons again</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs032.html#___sec31" style="font-size: 80%;">Playing around with regions</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs033.html#___sec32" style="font-size: 80%;">Regression trees</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs034.html#___sec33" style="font-size: 80%;">Final regressor code</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs035.html#___sec34" style="font-size: 80%;">Pros and cons of trees, pros</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs036.html#___sec35" style="font-size: 80%;">Disadvantages</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs037.html#___sec36" style="font-size: 80%;">Ensemble Methods: From a Single Tree to Many Trees and Extreme Boosting, Meet the Jungle of Methods</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs038.html#___sec37" style="font-size: 80%;">An Overview of Ensemble Methods</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs039.html#___sec38" style="font-size: 80%;">Bagging</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs040.html#___sec39" style="font-size: 80%;">More bagging</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs041.html#___sec40" style="font-size: 80%;">Making your own Bootstrap: Changing the Level of the Decision Tree</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs042.html#___sec41" style="font-size: 80%;">Why Voting?</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs043.html#___sec42" style="font-size: 80%;">Tossing coins</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs044.html#___sec43" style="font-size: 80%;">Standard imports first</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs045.html#___sec44" style="font-size: 80%;">Simple Voting Example, head or tail</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs046.html#___sec45" style="font-size: 80%;">Using the Voting Classifier</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs047.html#___sec46" style="font-size: 80%;">Voting and Bagging</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs048.html#___sec47" style="font-size: 80%;">Random forests</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs049.html#___sec48" style="font-size: 80%;">Random Forest Algorithm</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs050.html#___sec49" style="font-size: 80%;">Random Forests Compared with other Methods on the Cancer Data</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs051.html#___sec50" style="font-size: 80%;">Compare  Bagging on Trees with Random Forests</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs052.html#___sec51" style="font-size: 80%;">Boosting, a Bird's Eye View</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs053.html#___sec52" style="font-size: 80%;">What is boosting? Additive Modelling/Iterative Fitting</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs054.html#___sec53" style="font-size: 80%;">Iterative Fitting, Regression and Squared-error Cost Function</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs055.html#___sec54" style="font-size: 80%;">Squared-Error Example and Iterative Fitting</a></li>
     <!-- navigation toc: --> <li><a href="#___sec55" style="font-size: 80%;">Iterative Fitting, Classification and AdaBoost</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs057.html#___sec56" style="font-size: 80%;">Adaptive Boosting, AdaBoost</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs058.html#___sec57" style="font-size: 80%;">Building up AdaBoost</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs059.html#___sec58" style="font-size: 80%;">Adaptive boosting: AdaBoost, Basic Algorithm</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs060.html#___sec59" style="font-size: 80%;">Basic Steps of AdaBoost</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs061.html#___sec60" style="font-size: 80%;">AdaBoost Examples</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs062.html#___sec61" style="font-size: 80%;">Gradient boosting: Basics with Steepest Descent/Functional Gradient Descent</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs063.html#___sec62" style="font-size: 80%;">The Squared-Error again! Steepest Descent</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs064.html#___sec63" style="font-size: 80%;">Steepest Descent Example</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs065.html#___sec64" style="font-size: 80%;">Gradient Boosting, algorithm</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs066.html#___sec65" style="font-size: 80%;">Gradient Boosting, Examples of Regression</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs067.html#___sec66" style="font-size: 80%;">Gradient Boosting, Classification Example</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs068.html#___sec67" style="font-size: 80%;">XGBoost: Extreme Gradient Boosting</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs069.html#___sec68" style="font-size: 80%;">Regression Case</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs070.html#___sec69" style="font-size: 80%;">Xgboost on the Cancer Data</a></li>
>>>>>>> 0e5076dfbfc8cf7946a5cec60d742ecccdbbcd3e

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->

<div class="container">

<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->

<a name="part0056"></a>
<!-- !split -->
<<<<<<< HEAD
<h2 id="random-forests-compared-with-other-methods-on-the-cancer-data" class="anchor">Random Forests Compared with other Methods on the Cancer Data </h2>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span>  train_test_split 
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.datasets</span> <span style="color: #008000; font-weight: bold">import</span> load_breast_cancer
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.svm</span> <span style="color: #008000; font-weight: bold">import</span> SVC
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> LogisticRegression
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.tree</span> <span style="color: #008000; font-weight: bold">import</span> DecisionTreeClassifier
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.ensemble</span> <span style="color: #008000; font-weight: bold">import</span> BaggingClassifier

<span style="color: #408080; font-style: italic"># Load the data</span>
cancer <span style="color: #666666">=</span> load_breast_cancer()

X_train, X_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(cancer<span style="color: #666666">.</span>data,cancer<span style="color: #666666">.</span>target,random_state<span style="color: #666666">=0</span>)
<span style="color: #008000">print</span>(X_train<span style="color: #666666">.</span>shape)
<span style="color: #008000">print</span>(X_test<span style="color: #666666">.</span>shape)
<span style="color: #408080; font-style: italic"># Logistic Regression</span>
logreg <span style="color: #666666">=</span> LogisticRegression(solver<span style="color: #666666">=</span><span style="color: #BA2121">&#39;lbfgs&#39;</span>)
logreg<span style="color: #666666">.</span>fit(X_train, y_train)
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test set accuracy with Logistic Regression: </span><span style="color: #BB6688; font-weight: bold">{:.2f}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(logreg<span style="color: #666666">.</span>score(X_test,y_test)))
<span style="color: #408080; font-style: italic"># Support vector machine</span>
svm <span style="color: #666666">=</span> SVC(gamma<span style="color: #666666">=</span><span style="color: #BA2121">&#39;auto&#39;</span>, C<span style="color: #666666">=100</span>)
svm<span style="color: #666666">.</span>fit(X_train, y_train)
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test set accuracy with SVM: </span><span style="color: #BB6688; font-weight: bold">{:.2f}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(svm<span style="color: #666666">.</span>score(X_test,y_test)))
<span style="color: #408080; font-style: italic"># Decision Trees</span>
deep_tree_clf <span style="color: #666666">=</span> DecisionTreeClassifier(max_depth<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">None</span>)
deep_tree_clf<span style="color: #666666">.</span>fit(X_train, y_train)
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test set accuracy with Decision Trees: </span><span style="color: #BB6688; font-weight: bold">{:.2f}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(deep_tree_clf<span style="color: #666666">.</span>score(X_test,y_test)))
<span style="color: #408080; font-style: italic">#now scale the data</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.preprocessing</span> <span style="color: #008000; font-weight: bold">import</span> StandardScaler
scaler <span style="color: #666666">=</span> StandardScaler()
scaler<span style="color: #666666">.</span>fit(X_train)
X_train_scaled <span style="color: #666666">=</span> scaler<span style="color: #666666">.</span>transform(X_train)
X_test_scaled <span style="color: #666666">=</span> scaler<span style="color: #666666">.</span>transform(X_test)
<span style="color: #408080; font-style: italic"># Logistic Regression</span>
logreg<span style="color: #666666">.</span>fit(X_train_scaled, y_train)
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test set accuracy Logistic Regression with scaled data: </span><span style="color: #BB6688; font-weight: bold">{:.2f}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(logreg<span style="color: #666666">.</span>score(X_test_scaled,y_test)))
<span style="color: #408080; font-style: italic"># Support Vector Machine</span>
svm<span style="color: #666666">.</span>fit(X_train_scaled, y_train)
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test set accuracy SVM with scaled data: </span><span style="color: #BB6688; font-weight: bold">{:.2f}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(logreg<span style="color: #666666">.</span>score(X_test_scaled,y_test)))
<span style="color: #408080; font-style: italic"># Decision Trees</span>
deep_tree_clf<span style="color: #666666">.</span>fit(X_train_scaled, y_train)
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test set accuracy with Decision Trees and scaled data: </span><span style="color: #BB6688; font-weight: bold">{:.2f}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(deep_tree_clf<span style="color: #666666">.</span>score(X_test_scaled,y_test)))


<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.ensemble</span> <span style="color: #008000; font-weight: bold">import</span> RandomForestClassifier
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.preprocessing</span> <span style="color: #008000; font-weight: bold">import</span> LabelEncoder
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> cross_validate
<span style="color: #408080; font-style: italic"># Data set not specificied</span>
<span style="color: #408080; font-style: italic">#Instantiate the model with 500 trees and entropy as splitting criteria</span>
Random_Forest_model <span style="color: #666666">=</span> RandomForestClassifier(n_estimators<span style="color: #666666">=500</span>,criterion<span style="color: #666666">=</span><span style="color: #BA2121">&quot;entropy&quot;</span>)
Random_Forest_model<span style="color: #666666">.</span>fit(X_train_scaled, y_train)
<span style="color: #408080; font-style: italic">#Cross validation</span>
accuracy <span style="color: #666666">=</span> cross_validate(Random_Forest_model,X_test_scaled,y_test,cv<span style="color: #666666">=10</span>)[<span style="color: #BA2121">&#39;test_score&#39;</span>]
<span style="color: #008000">print</span>(accuracy)
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test set accuracy with Random Forests and scaled data: </span><span style="color: #BB6688; font-weight: bold">{:.2f}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(Random_Forest_model<span style="color: #666666">.</span>score(X_test_scaled,y_test)))


<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">scikitplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">skplt</span>
y_pred <span style="color: #666666">=</span> Random_Forest_model<span style="color: #666666">.</span>predict(X_test_scaled)
skplt<span style="color: #666666">.</span>metrics<span style="color: #666666">.</span>plot_confusion_matrix(y_test, y_pred, normalize<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)
plt<span style="color: #666666">.</span>show()
y_probas <span style="color: #666666">=</span> Random_Forest_model<span style="color: #666666">.</span>predict_proba(X_test_scaled)
skplt<span style="color: #666666">.</span>metrics<span style="color: #666666">.</span>plot_roc(y_test, y_probas)
plt<span style="color: #666666">.</span>show()
skplt<span style="color: #666666">.</span>metrics<span style="color: #666666">.</span>plot_cumulative_gain(y_test, y_probas)
plt<span style="color: #666666">.</span>show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Recall that the cumulative gains curve shows the percentage of the
overall number of cases in a given category <em>gained</em> by targeting a
percentage of the total number of cases.
</p>

<p>Similarly, the receiver operating characteristic curve, or ROC curve,
displays the diagnostic ability of a binary classifier system as its
discrimination threshold is varied. It plots the true positive rate against the false positive rate.
</p>
=======

<h2 id="___sec55" class="anchor">Iterative Fitting, Classification and AdaBoost </h2>

<p>
Let us consider a binary classification problem with two outcomes \( y_i \in \{-1,1\} \) and \( i=0,1,2,\dots,n-1 \) as our set of
observations. We define a classification function \( G(x) \) which produces a prediction taking one or the other of the two values 
\( \{-1,1\} \).

<p>
The error rate of the training sample is then

$$
\mathrm{\overline{err}}=\frac{1}{n} \sum_{i=0}^{n-1} I(y_i\ne G(x_i)). 
$$

<p>
The iterative procedure starts with defining a weak classifier whose
error rate is barely better than random guessing.  The iterative
procedure in boosting is to sequentially apply a  weak
classification algorithm to repeatedly modified versions of the data
producing a sequence of weak classifiers \( G_m(x) \).
>>>>>>> 0e5076dfbfc8cf7946a5cec60d742ecccdbbcd3e

<p>
Here we will express our  function \( f(x) \) in terms of \( G(x) \). That is
$$
f_M(x) = \sum_{i=1}^M \beta_m b(x;\gamma_m),
$$

will be a function of 
$$
G_M(x) = \mathrm{sign} \sum_{i=1}^M \alpha_m G_m(x).
$$

<p>
<p>
<!-- navigation buttons at the bottom of the page -->
<ul class="pagination">
<li><a href="._week45-bs055.html">&laquo;</a></li>
  <li><a href="._week45-bs000.html">1</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week45-bs048.html">49</a></li>
  <li><a href="._week45-bs049.html">50</a></li>
  <li><a href="._week45-bs050.html">51</a></li>
  <li><a href="._week45-bs051.html">52</a></li>
  <li><a href="._week45-bs052.html">53</a></li>
  <li><a href="._week45-bs053.html">54</a></li>
  <li><a href="._week45-bs054.html">55</a></li>
  <li><a href="._week45-bs055.html">56</a></li>
  <li class="active"><a href="._week45-bs056.html">57</a></li>
  <li><a href="._week45-bs057.html">58</a></li>
  <li><a href="._week45-bs058.html">59</a></li>
  <li><a href="._week45-bs059.html">60</a></li>
  <li><a href="._week45-bs060.html">61</a></li>
  <li><a href="._week45-bs061.html">62</a></li>
  <li><a href="._week45-bs062.html">63</a></li>
  <li><a href="._week45-bs063.html">64</a></li>
  <li><a href="._week45-bs064.html">65</a></li>
  <li><a href="._week45-bs065.html">66</a></li>
  <li><a href="">...</a></li>
<<<<<<< HEAD
  <li><a href="._week45-bs076.html">77</a></li>
=======
  <li><a href="._week45-bs070.html">71</a></li>
>>>>>>> 0e5076dfbfc8cf7946a5cec60d742ecccdbbcd3e
  <li><a href="._week45-bs057.html">&raquo;</a></li>
</ul>
<!-- ------------------- end of main content --------------- -->

</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>

<!-- Bootstrap footer
<footer>
<a href="http://..."><img width="250" align=right src="http://..."></a>
</footer>
-->


<center style="font-size:80%">
<!-- copyright only on the titlepage -->
</center>


</body>
</html>
    


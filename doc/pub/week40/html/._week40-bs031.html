<!--
HTML file automatically generated from DocOnce source
(https://github.com/doconce/doconce/)
doconce format html week40.do.txt --html_style=bootstrap --pygments_html_style=default --html_admon=bootstrap_panel --html_output=week40-bs --no_mako
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Week 40: Neural networks">
<title>Week 40: Neural networks</title>
<!-- Bootstrap style: bootstrap -->
<!-- doconce format html week40.do.txt --html_style=bootstrap --pygments_html_style=default --html_admon=bootstrap_panel --html_output=week40-bs --no_mako -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->
<style type="text/css">
/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}
/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>
</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Plan for week 40', 2, None, 'plan-for-week-40'),
              ('Overview video on Stochastic Gradient Descent',
               2,
               None,
               'overview-video-on-stochastic-gradient-descent'),
              ('Practical tips', 2, None, 'practical-tips'),
              ('Automatic differentiation',
               2,
               None,
               'automatic-differentiation'),
              ('Videos on Neural Networks',
               2,
               None,
               'videos-on-neural-networks'),
              ('Neural networks', 2, None, 'neural-networks'),
              ('Artificial neurons', 2, None, 'artificial-neurons'),
              ('Neural network types', 2, None, 'neural-network-types'),
              ('Feed-forward neural networks',
               2,
               None,
               'feed-forward-neural-networks'),
              ('Convolutional Neural Network',
               2,
               None,
               'convolutional-neural-network'),
              ('Recurrent neural networks',
               2,
               None,
               'recurrent-neural-networks'),
              ('Other types of networks', 2, None, 'other-types-of-networks'),
              ('Multilayer perceptrons', 2, None, 'multilayer-perceptrons'),
              ('Why multilayer perceptrons?',
               2,
               None,
               'why-multilayer-perceptrons'),
              ('Illustration of a single perceptropn model and a '
               'multi-perceptron model',
               2,
               None,
               'illustration-of-a-single-perceptropn-model-and-a-multi-perceptron-model'),
              ('Examples of XOR, OR and AND gates',
               2,
               None,
               'examples-of-xor-or-and-and-gates'),
              ('Does Logistic Regression do a better Job?',
               2,
               None,
               'does-logistic-regression-do-a-better-job'),
              ('Adding Neural Networks', 2, None, 'adding-neural-networks'),
              ('Mathematical model', 2, None, 'mathematical-model'),
              ('Mathematical model', 2, None, 'mathematical-model'),
              ('Mathematical model', 2, None, 'mathematical-model'),
              ('Mathematical model', 2, None, 'mathematical-model'),
              ('Mathematical model', 2, None, 'mathematical-model'),
              ('Matrix-vector notation', 3, None, 'matrix-vector-notation'),
              ('Matrix-vector notation  and activation',
               3,
               None,
               'matrix-vector-notation-and-activation'),
              ('Activation functions', 3, None, 'activation-functions'),
              ('Activation functions, Logistic and Hyperbolic ones',
               3,
               None,
               'activation-functions-logistic-and-hyperbolic-ones'),
              ('Relevance', 3, None, 'relevance'),
              ('The multilayer  perceptron (MLP)',
               2,
               None,
               'the-multilayer-perceptron-mlp'),
              ('From one to many layers, the universal approximation theorem',
               2,
               None,
               'from-one-to-many-layers-the-universal-approximation-theorem'),
              ('Deriving the back propagation code for a multilayer perceptron '
               'model',
               2,
               None,
               'deriving-the-back-propagation-code-for-a-multilayer-perceptron-model'),
              ('Definitions', 2, None, 'definitions'),
              ('Derivatives and the chain rule',
               2,
               None,
               'derivatives-and-the-chain-rule'),
              ('Derivative of the cost function',
               2,
               None,
               'derivative-of-the-cost-function'),
              ('Bringing it together, first back propagation equation',
               2,
               None,
               'bringing-it-together-first-back-propagation-equation'),
              ('Derivatives in terms of $z_j^L$',
               2,
               None,
               'derivatives-in-terms-of-z-j-l'),
              ('Bringing it together', 2, None, 'bringing-it-together'),
              ('Final back propagating equation',
               2,
               None,
               'final-back-propagating-equation'),
              ('Setting up the Back propagation algorithm',
               2,
               None,
               'setting-up-the-back-propagation-algorithm'),
              ('Setting up the Back propagation algorithm',
               2,
               None,
               'setting-up-the-back-propagation-algorithm')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="week40-bs.html">Week 40: Neural networks</a>
  </div>
  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="._week40-bs001.html#plan-for-week-40" style="font-size: 80%;"><b>Plan for week 40</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs002.html#overview-video-on-stochastic-gradient-descent" style="font-size: 80%;"><b>Overview video on Stochastic Gradient Descent</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs003.html#practical-tips" style="font-size: 80%;"><b>Practical tips</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs004.html#automatic-differentiation" style="font-size: 80%;"><b>Automatic differentiation</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs005.html#videos-on-neural-networks" style="font-size: 80%;"><b>Videos on Neural Networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs006.html#neural-networks" style="font-size: 80%;"><b>Neural networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs007.html#artificial-neurons" style="font-size: 80%;"><b>Artificial neurons</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs008.html#neural-network-types" style="font-size: 80%;"><b>Neural network types</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs009.html#feed-forward-neural-networks" style="font-size: 80%;"><b>Feed-forward neural networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs010.html#convolutional-neural-network" style="font-size: 80%;"><b>Convolutional Neural Network</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs011.html#recurrent-neural-networks" style="font-size: 80%;"><b>Recurrent neural networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs012.html#other-types-of-networks" style="font-size: 80%;"><b>Other types of networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs013.html#multilayer-perceptrons" style="font-size: 80%;"><b>Multilayer perceptrons</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs014.html#why-multilayer-perceptrons" style="font-size: 80%;"><b>Why multilayer perceptrons?</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs015.html#illustration-of-a-single-perceptropn-model-and-a-multi-perceptron-model" style="font-size: 80%;"><b>Illustration of a single perceptropn model and a multi-perceptron model</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs016.html#examples-of-xor-or-and-and-gates" style="font-size: 80%;"><b>Examples of XOR, OR and AND gates</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs017.html#does-logistic-regression-do-a-better-job" style="font-size: 80%;"><b>Does Logistic Regression do a better Job?</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs018.html#adding-neural-networks" style="font-size: 80%;"><b>Adding Neural Networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs023.html#mathematical-model" style="font-size: 80%;"><b>Mathematical model</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs023.html#mathematical-model" style="font-size: 80%;"><b>Mathematical model</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs023.html#mathematical-model" style="font-size: 80%;"><b>Mathematical model</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs023.html#mathematical-model" style="font-size: 80%;"><b>Mathematical model</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs023.html#mathematical-model" style="font-size: 80%;"><b>Mathematical model</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs024.html#matrix-vector-notation" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Matrix-vector notation</a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs025.html#matrix-vector-notation-and-activation" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Matrix-vector notation  and activation</a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs026.html#activation-functions" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Activation functions</a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs027.html#activation-functions-logistic-and-hyperbolic-ones" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Activation functions, Logistic and Hyperbolic ones</a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs028.html#relevance" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Relevance</a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs029.html#the-multilayer-perceptron-mlp" style="font-size: 80%;"><b>The multilayer  perceptron (MLP)</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs030.html#from-one-to-many-layers-the-universal-approximation-theorem" style="font-size: 80%;"><b>From one to many layers, the universal approximation theorem</b></a></li>
     <!-- navigation toc: --> <li><a href="#deriving-the-back-propagation-code-for-a-multilayer-perceptron-model" style="font-size: 80%;"><b>Deriving the back propagation code for a multilayer perceptron model</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs032.html#definitions" style="font-size: 80%;"><b>Definitions</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs033.html#derivatives-and-the-chain-rule" style="font-size: 80%;"><b>Derivatives and the chain rule</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs034.html#derivative-of-the-cost-function" style="font-size: 80%;"><b>Derivative of the cost function</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs035.html#bringing-it-together-first-back-propagation-equation" style="font-size: 80%;"><b>Bringing it together, first back propagation equation</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs036.html#derivatives-in-terms-of-z-j-l" style="font-size: 80%;"><b>Derivatives in terms of \( z_j^L \)</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs037.html#bringing-it-together" style="font-size: 80%;"><b>Bringing it together</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs038.html#final-back-propagating-equation" style="font-size: 80%;"><b>Final back propagating equation</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs040.html#setting-up-the-back-propagation-algorithm" style="font-size: 80%;"><b>Setting up the Back propagation algorithm</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs040.html#setting-up-the-back-propagation-algorithm" style="font-size: 80%;"><b>Setting up the Back propagation algorithm</b></a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->
<div class="container">
<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->
<a name="part0031"></a>
<!-- !split -->
<h2 id="deriving-the-back-propagation-code-for-a-multilayer-perceptron-model" class="anchor">Deriving the back propagation code for a multilayer perceptron model </h2>

<p>As we have seen now in a feed forward network, we can express the final output of our network in terms of basic matrix-vector multiplications.
The unknowwn quantities are our weights \( w_{ij} \) and we need to find an algorithm for changing them so that our errors are as small as possible.
This leads us to the famous <a href="https://www.nature.com/articles/323533a0" target="_self">back propagation algorithm</a>.
</p>

<p>The questions we want to ask are how do changes in the biases and the
weights in our network change the cost function and how can we use the
final output to modify the weights?
</p>

<p>To derive these equations let us start with a plain regression problem
and define our cost function as
</p>

$$
{\cal C}(\hat{W})  =  \frac{1}{2}\sum_{i=1}^n\left(y_i - t_i\right)^2, 
$$

<p>where the $t_i$s are our \( n \) targets (the values we want to
reproduce), while the outputs of the network after having propagated
all inputs \( \hat{x} \) are given by \( y_i \).  Below we will demonstrate
how the basic equations arising from the back propagation algorithm
can be modified in order to study classification problems with \( K \)
classes.
</p>

<p>
<!-- navigation buttons at the bottom of the page -->
<ul class="pagination">
<li><a href="._week40-bs030.html">&laquo;</a></li>
  <li><a href="._week40-bs000.html">1</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week40-bs023.html">24</a></li>
  <li><a href="._week40-bs024.html">25</a></li>
  <li><a href="._week40-bs025.html">26</a></li>
  <li><a href="._week40-bs026.html">27</a></li>
  <li><a href="._week40-bs027.html">28</a></li>
  <li><a href="._week40-bs028.html">29</a></li>
  <li><a href="._week40-bs029.html">30</a></li>
  <li><a href="._week40-bs030.html">31</a></li>
  <li class="active"><a href="._week40-bs031.html">32</a></li>
  <li><a href="._week40-bs032.html">33</a></li>
  <li><a href="._week40-bs033.html">34</a></li>
  <li><a href="._week40-bs034.html">35</a></li>
  <li><a href="._week40-bs035.html">36</a></li>
  <li><a href="._week40-bs036.html">37</a></li>
  <li><a href="._week40-bs037.html">38</a></li>
  <li><a href="._week40-bs038.html">39</a></li>
  <li><a href="._week40-bs039.html">40</a></li>
  <li><a href="._week40-bs040.html">41</a></li>
  <li><a href="._week40-bs032.html">&raquo;</a></li>
</ul>
<!-- ------------------- end of main content --------------- -->
</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>
<!-- Bootstrap footer
<footer>
<a href="https://..."><img width="250" align=right src="https://..."></a>
</footer>
-->
<center style="font-size:80%">
<!-- copyright only on the titlepage -->
</center>
</body>
</html>


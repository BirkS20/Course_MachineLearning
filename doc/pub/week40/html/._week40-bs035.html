<!--
HTML file automatically generated from DocOnce source
(https://github.com/doconce/doconce/)
doconce format html week40.do.txt --html_style=bootstrap --pygments_html_style=default --html_admon=bootstrap_panel --html_output=week40-bs --no_mako
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Week 40: From Stochastic Gradient Descent to Neural networks">
<title>Week 40: From Stochastic Gradient Descent to Neural networks</title>
<!-- Bootstrap style: bootstrap -->
<!-- doconce format html week40.do.txt --html_style=bootstrap --pygments_html_style=default --html_admon=bootstrap_panel --html_output=week40-bs --no_mako -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->
<style type="text/css">
/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}
/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>
</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Plan for week 40', 2, None, 'plan-for-week-40'),
              ('Overview video on Stochastic Gradient Descent',
               2,
               None,
               'overview-video-on-stochastic-gradient-descent'),
              ('Batches and mini-batches', 2, None, 'batches-and-mini-batches'),
              ('Stochastic Gradient Descent (SGD)',
               2,
               None,
               'stochastic-gradient-descent-sgd'),
              ('Stochastic Gradient Descent',
               2,
               None,
               'stochastic-gradient-descent'),
              ('Computation of gradients', 2, None, 'computation-of-gradients'),
              ('SGD example', 2, None, 'sgd-example'),
              ('The gradient step', 2, None, 'the-gradient-step'),
              ('Simple example code', 2, None, 'simple-example-code'),
              ('When do we stop?', 2, None, 'when-do-we-stop'),
              ('Slightly different approach',
               2,
               None,
               'slightly-different-approach'),
              ('Program for stochastic gradient',
               2,
               None,
               'program-for-stochastic-gradient'),
              ('Momentum based GD', 2, None, 'momentum-based-gd'),
              ('More on momentum based approaches',
               2,
               None,
               'more-on-momentum-based-approaches'),
              ('Momentum parameter', 2, None, 'momentum-parameter'),
              ('Second moment of the gradient',
               2,
               None,
               'second-moment-of-the-gradient'),
              ('RMS prop', 2, None, 'rms-prop'),
              ('ADAM optimizer', 2, None, 'adam-optimizer'),
              ('Practical tips', 2, None, 'practical-tips'),
              ('Automatic differentiation',
               2,
               None,
               'automatic-differentiation'),
              ('Using autograd', 2, None, 'using-autograd'),
              ('Autograd with more complicated functions',
               2,
               None,
               'autograd-with-more-complicated-functions'),
              ('More complicated functions using the elements of their '
               'arguments directly',
               2,
               None,
               'more-complicated-functions-using-the-elements-of-their-arguments-directly'),
              ('Functions using mathematical functions from Numpy',
               2,
               None,
               'functions-using-mathematical-functions-from-numpy'),
              ('More autograd', 2, None, 'more-autograd'),
              ('And  with loops', 2, None, 'and-with-loops'),
              ('Using recursion', 2, None, 'using-recursion'),
              ('Unsupported functions', 2, None, 'unsupported-functions'),
              ('The syntax a.dot(b) when finding the dot product',
               2,
               None,
               'the-syntax-a-dot-b-when-finding-the-dot-product'),
              ('Recommended to avoid', 2, None, 'recommended-to-avoid'),
              ('Using Autograd with OLS', 2, None, 'using-autograd-with-ols'),
              ('And Logistic Regression', 2, None, 'and-logistic-regression'),
              ('Videos on Neural Networks',
               2,
               None,
               'videos-on-neural-networks'),
              ('Neural networks', 2, None, 'neural-networks'),
              ('Artificial neurons', 2, None, 'artificial-neurons'),
              ('Neural network types', 2, None, 'neural-network-types'),
              ('Feed-forward neural networks',
               2,
               None,
               'feed-forward-neural-networks'),
              ('Convolutional Neural Network',
               2,
               None,
               'convolutional-neural-network'),
              ('Recurrent neural networks',
               2,
               None,
               'recurrent-neural-networks'),
              ('Other types of networks', 2, None, 'other-types-of-networks'),
              ('Multilayer perceptrons', 2, None, 'multilayer-perceptrons'),
              ('Why multilayer perceptrons?',
               2,
               None,
               'why-multilayer-perceptrons'),
              ('Illustration of a single perceptropn model and a '
               'multi-perceptron model',
               2,
               None,
               'illustration-of-a-single-perceptropn-model-and-a-multi-perceptron-model'),
              ('Examples of XOR, OR and AND gates',
               2,
               None,
               'examples-of-xor-or-and-and-gates'),
              ('Does Logistic Regression do a better Job?',
               2,
               None,
               'does-logistic-regression-do-a-better-job'),
              ('Adding Neural Networks', 2, None, 'adding-neural-networks'),
              ('Mathematical model', 2, None, 'mathematical-model'),
              ('Mathematical model', 2, None, 'mathematical-model'),
              ('Mathematical model', 2, None, 'mathematical-model'),
              ('Mathematical model', 2, None, 'mathematical-model'),
              ('Mathematical model', 2, None, 'mathematical-model'),
              ('Matrix-vector notation', 3, None, 'matrix-vector-notation'),
              ('Matrix-vector notation  and activation',
               3,
               None,
               'matrix-vector-notation-and-activation'),
              ('Activation functions', 3, None, 'activation-functions'),
              ('Activation functions, Logistic and Hyperbolic ones',
               3,
               None,
               'activation-functions-logistic-and-hyperbolic-ones'),
              ('Relevance', 3, None, 'relevance'),
              ('The multilayer  perceptron (MLP)',
               2,
               None,
               'the-multilayer-perceptron-mlp'),
              ('From one to many layers, the universal approximation theorem',
               2,
               None,
               'from-one-to-many-layers-the-universal-approximation-theorem'),
              ('Deriving the back propagation code for a multilayer perceptron '
               'model',
               2,
               None,
               'deriving-the-back-propagation-code-for-a-multilayer-perceptron-model'),
              ('Definitions', 2, None, 'definitions'),
              ('Derivatives and the chain rule',
               2,
               None,
               'derivatives-and-the-chain-rule'),
              ('Derivative of the cost function',
               2,
               None,
               'derivative-of-the-cost-function'),
              ('Bringing it together, first back propagation equation',
               2,
               None,
               'bringing-it-together-first-back-propagation-equation'),
              ('Derivatives in terms of $z_j^L$',
               2,
               None,
               'derivatives-in-terms-of-z-j-l'),
              ('Bringing it together', 2, None, 'bringing-it-together'),
              ('Final back propagating equation',
               2,
               None,
               'final-back-propagating-equation'),
              ('Setting up the Back propagation algorithm',
               2,
               None,
               'setting-up-the-back-propagation-algorithm')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="week40-bs.html">Week 40: From Stochastic Gradient Descent to Neural networks</a>
  </div>
  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="._week40-bs001.html#plan-for-week-40" style="font-size: 80%;"><b>Plan for week 40</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs002.html#overview-video-on-stochastic-gradient-descent" style="font-size: 80%;"><b>Overview video on Stochastic Gradient Descent</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs003.html#batches-and-mini-batches" style="font-size: 80%;"><b>Batches and mini-batches</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs004.html#stochastic-gradient-descent-sgd" style="font-size: 80%;"><b>Stochastic Gradient Descent (SGD)</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs005.html#stochastic-gradient-descent" style="font-size: 80%;"><b>Stochastic Gradient Descent</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs006.html#computation-of-gradients" style="font-size: 80%;"><b>Computation of gradients</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs007.html#sgd-example" style="font-size: 80%;"><b>SGD example</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs008.html#the-gradient-step" style="font-size: 80%;"><b>The gradient step</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs009.html#simple-example-code" style="font-size: 80%;"><b>Simple example code</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs010.html#when-do-we-stop" style="font-size: 80%;"><b>When do we stop?</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs011.html#slightly-different-approach" style="font-size: 80%;"><b>Slightly different approach</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs012.html#program-for-stochastic-gradient" style="font-size: 80%;"><b>Program for stochastic gradient</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs013.html#momentum-based-gd" style="font-size: 80%;"><b>Momentum based GD</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs014.html#more-on-momentum-based-approaches" style="font-size: 80%;"><b>More on momentum based approaches</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs015.html#momentum-parameter" style="font-size: 80%;"><b>Momentum parameter</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs016.html#second-moment-of-the-gradient" style="font-size: 80%;"><b>Second moment of the gradient</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs017.html#rms-prop" style="font-size: 80%;"><b>RMS prop</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs018.html#adam-optimizer" style="font-size: 80%;"><b>ADAM optimizer</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs019.html#practical-tips" style="font-size: 80%;"><b>Practical tips</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs020.html#automatic-differentiation" style="font-size: 80%;"><b>Automatic differentiation</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs021.html#using-autograd" style="font-size: 80%;"><b>Using autograd</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs022.html#autograd-with-more-complicated-functions" style="font-size: 80%;"><b>Autograd with more complicated functions</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs023.html#more-complicated-functions-using-the-elements-of-their-arguments-directly" style="font-size: 80%;"><b>More complicated functions using the elements of their arguments directly</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs024.html#functions-using-mathematical-functions-from-numpy" style="font-size: 80%;"><b>Functions using mathematical functions from Numpy</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs025.html#more-autograd" style="font-size: 80%;"><b>More autograd</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs026.html#and-with-loops" style="font-size: 80%;"><b>And  with loops</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs027.html#using-recursion" style="font-size: 80%;"><b>Using recursion</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs028.html#unsupported-functions" style="font-size: 80%;"><b>Unsupported functions</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs029.html#the-syntax-a-dot-b-when-finding-the-dot-product" style="font-size: 80%;"><b>The syntax a.dot(b) when finding the dot product</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs030.html#recommended-to-avoid" style="font-size: 80%;"><b>Recommended to avoid</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs031.html#using-autograd-with-ols" style="font-size: 80%;"><b>Using Autograd with OLS</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs032.html#and-logistic-regression" style="font-size: 80%;"><b>And Logistic Regression</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs033.html#videos-on-neural-networks" style="font-size: 80%;"><b>Videos on Neural Networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs034.html#neural-networks" style="font-size: 80%;"><b>Neural networks</b></a></li>
     <!-- navigation toc: --> <li><a href="#artificial-neurons" style="font-size: 80%;"><b>Artificial neurons</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs036.html#neural-network-types" style="font-size: 80%;"><b>Neural network types</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs037.html#feed-forward-neural-networks" style="font-size: 80%;"><b>Feed-forward neural networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs038.html#convolutional-neural-network" style="font-size: 80%;"><b>Convolutional Neural Network</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs039.html#recurrent-neural-networks" style="font-size: 80%;"><b>Recurrent neural networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs040.html#other-types-of-networks" style="font-size: 80%;"><b>Other types of networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs041.html#multilayer-perceptrons" style="font-size: 80%;"><b>Multilayer perceptrons</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs042.html#why-multilayer-perceptrons" style="font-size: 80%;"><b>Why multilayer perceptrons?</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs043.html#illustration-of-a-single-perceptropn-model-and-a-multi-perceptron-model" style="font-size: 80%;"><b>Illustration of a single perceptropn model and a multi-perceptron model</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs044.html#examples-of-xor-or-and-and-gates" style="font-size: 80%;"><b>Examples of XOR, OR and AND gates</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs045.html#does-logistic-regression-do-a-better-job" style="font-size: 80%;"><b>Does Logistic Regression do a better Job?</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs046.html#adding-neural-networks" style="font-size: 80%;"><b>Adding Neural Networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs051.html#mathematical-model" style="font-size: 80%;"><b>Mathematical model</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs051.html#mathematical-model" style="font-size: 80%;"><b>Mathematical model</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs051.html#mathematical-model" style="font-size: 80%;"><b>Mathematical model</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs051.html#mathematical-model" style="font-size: 80%;"><b>Mathematical model</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs051.html#mathematical-model" style="font-size: 80%;"><b>Mathematical model</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs052.html#matrix-vector-notation" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Matrix-vector notation</a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs053.html#matrix-vector-notation-and-activation" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Matrix-vector notation  and activation</a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs054.html#activation-functions" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Activation functions</a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs055.html#activation-functions-logistic-and-hyperbolic-ones" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Activation functions, Logistic and Hyperbolic ones</a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs056.html#relevance" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Relevance</a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs057.html#the-multilayer-perceptron-mlp" style="font-size: 80%;"><b>The multilayer  perceptron (MLP)</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs058.html#from-one-to-many-layers-the-universal-approximation-theorem" style="font-size: 80%;"><b>From one to many layers, the universal approximation theorem</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs059.html#deriving-the-back-propagation-code-for-a-multilayer-perceptron-model" style="font-size: 80%;"><b>Deriving the back propagation code for a multilayer perceptron model</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs060.html#definitions" style="font-size: 80%;"><b>Definitions</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs061.html#derivatives-and-the-chain-rule" style="font-size: 80%;"><b>Derivatives and the chain rule</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs062.html#derivative-of-the-cost-function" style="font-size: 80%;"><b>Derivative of the cost function</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs063.html#bringing-it-together-first-back-propagation-equation" style="font-size: 80%;"><b>Bringing it together, first back propagation equation</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs064.html#derivatives-in-terms-of-z-j-l" style="font-size: 80%;"><b>Derivatives in terms of \( z_j^L \)</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs065.html#bringing-it-together" style="font-size: 80%;"><b>Bringing it together</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs066.html#final-back-propagating-equation" style="font-size: 80%;"><b>Final back propagating equation</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs067.html#setting-up-the-back-propagation-algorithm" style="font-size: 80%;"><b>Setting up the Back propagation algorithm</b></a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->
<div class="container">
<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->
<a name="part0035"></a>
<!-- !split -->
<h2 id="artificial-neurons" class="anchor">Artificial neurons  </h2>

<p>The field of artificial neural networks has a long history of
development, and is closely connected with the advancement of computer
science and computers in general. A model of artificial neurons was
first developed by McCulloch and Pitts in 1943 to study signal
processing in the brain and has later been refined by others. The
general idea is to mimic neural networks in the human brain, which is
composed of billions of neurons that communicate with each other by
sending electrical signals.  Each neuron accumulates its incoming
signals, which must exceed an activation threshold to yield an
output. If the threshold is not overcome, the neuron remains inactive,
i.e. has zero output.
</p>

<p>This behaviour has inspired a simple mathematical model for an artificial neuron.</p>

$$
\begin{equation}
 y = f\left(\sum_{i=1}^n w_ix_i\right) = f(u)
\tag{6}
\end{equation}
$$

<p>Here, the output \( y \) of the neuron is the value of its activation function, which have as input
a weighted sum of signals \( x_i, \dots ,x_n \) received by \( n \) other neurons.
</p>

<p>Conceptually, it is helpful to divide neural networks into four
categories:
</p>
<ol>
<li> general purpose neural networks for supervised learning,</li>
<li> neural networks designed specifically for image processing, the most prominent example of this class being Convolutional Neural Networks (CNNs),</li>
<li> neural networks for sequential data such as Recurrent Neural Networks (RNNs), and</li>
<li> neural networks for unsupervised learning such as Deep Boltzmann Machines.</li>
</ol>
<p>In natural science, DNNs and CNNs have already found numerous
applications. In statistical physics, they have been applied to detect
phase transitions in 2D Ising and Potts models, lattice gauge
theories, and different phases of polymers, or solving the
Navier-Stokes equation in weather forecasting.  Deep learning has also
found interesting applications in quantum physics. Various quantum
phase transitions can be detected and studied using DNNs and CNNs,
topological phases, and even non-equilibrium many-body
localization. Representing quantum states as DNNs quantum state
tomography are among some of the impressive achievements to reveal the
potential of DNNs to facilitate the study of quantum systems.
</p>

<p>In quantum information theory, it has been shown that one can perform
gate decompositions with the help of neural. 
</p>

<p>The applications are not limited to the natural sciences. There is a
plethora of applications in essentially all disciplines, from the
humanities to life science and medicine.
</p>

<p>
<!-- navigation buttons at the bottom of the page -->
<ul class="pagination">
<li><a href="._week40-bs034.html">&laquo;</a></li>
  <li><a href="._week40-bs000.html">1</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week40-bs027.html">28</a></li>
  <li><a href="._week40-bs028.html">29</a></li>
  <li><a href="._week40-bs029.html">30</a></li>
  <li><a href="._week40-bs030.html">31</a></li>
  <li><a href="._week40-bs031.html">32</a></li>
  <li><a href="._week40-bs032.html">33</a></li>
  <li><a href="._week40-bs033.html">34</a></li>
  <li><a href="._week40-bs034.html">35</a></li>
  <li class="active"><a href="._week40-bs035.html">36</a></li>
  <li><a href="._week40-bs036.html">37</a></li>
  <li><a href="._week40-bs037.html">38</a></li>
  <li><a href="._week40-bs038.html">39</a></li>
  <li><a href="._week40-bs039.html">40</a></li>
  <li><a href="._week40-bs040.html">41</a></li>
  <li><a href="._week40-bs041.html">42</a></li>
  <li><a href="._week40-bs042.html">43</a></li>
  <li><a href="._week40-bs043.html">44</a></li>
  <li><a href="._week40-bs044.html">45</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week40-bs067.html">68</a></li>
  <li><a href="._week40-bs036.html">&raquo;</a></li>
</ul>
<!-- ------------------- end of main content --------------- -->
</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>
<!-- Bootstrap footer
<footer>
<a href="https://..."><img width="250" align=right src="https://..."></a>
</footer>
-->
<center style="font-size:80%">
<!-- copyright only on the titlepage -->
</center>
</body>
</html>


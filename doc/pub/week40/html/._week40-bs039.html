<!--
Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Week 40: From Stochastic Gradient Descent to Neural networks">

<title>Week 40: From Stochastic Gradient Descent to Neural networks</title>

<!-- Bootstrap style: bootstrap -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->

<style type="text/css">

/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}

/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>


</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Plan for week 40', 2, None, '___sec0'),
              ('Overview video for week 40', 2, None, '___sec1'),
              ('Stochastic Gradient Descent', 2, None, '___sec2'),
              ('Computation of gradients', 2, None, '___sec3'),
              ('SGD example', 2, None, '___sec4'),
              ('The gradient step', 2, None, '___sec5'),
              ('Simple example code', 2, None, '___sec6'),
              ('When do we stop?', 2, None, '___sec7'),
              ('Slightly different approach', 2, None, '___sec8'),
              ('Program for stochastic gradient', 2, None, '___sec9'),
              ('Momentum based GD', 2, None, '___sec10'),
              ('More on momentum based approaches', 2, None, '___sec11'),
              ('Momentum parameter', 2, None, '___sec12'),
              ('Second moment of the gradient', 2, None, '___sec13'),
              ('RMS prop', 2, None, '___sec14'),
              ('ADAM optimizer', 2, None, '___sec15'),
              ('Practical tips', 2, None, '___sec16'),
              ('Automatic differentiation', 2, None, '___sec17'),
              ('Using autograd', 2, None, '___sec18'),
              ('Autograd with more complicated functions', 2, None, '___sec19'),
              ('More complicated functions using the elements of their '
               'arguments directly',
               2,
               None,
               '___sec20'),
              ('Functions using mathematical functions from Numpy',
               2,
               None,
               '___sec21'),
              ('More autograd', 2, None, '___sec22'),
              ('And  with loops', 2, None, '___sec23'),
              ('Using recursion', 2, None, '___sec24'),
              ('Unsupported functions', 2, None, '___sec25'),
              ('The syntax a.dot(b) when finding the dot product',
               2,
               None,
               '___sec26'),
              ('Recommended to avoid', 2, None, '___sec27'),
              ('Neural networks', 2, None, '___sec28'),
              ('Artificial neurons', 2, None, '___sec29'),
              ('Neural network types', 2, None, '___sec30'),
              ('Feed-forward neural networks', 2, None, '___sec31'),
              ('Convolutional Neural Network', 2, None, '___sec32'),
              ('Recurrent neural networks', 2, None, '___sec33'),
              ('Other types of networks', 2, None, '___sec34'),
              ('Multilayer perceptrons', 2, None, '___sec35'),
              ('Why multilayer perceptrons?', 2, None, '___sec36'),
              ('Mathematical model', 2, None, '___sec37'),
              ('Mathematical model', 2, None, '___sec38'),
              ('Mathematical model', 2, None, '___sec39'),
              ('Mathematical model', 2, None, '___sec40'),
              ('Mathematical model', 2, None, '___sec41'),
              ('Matrix-vector notation', 3, None, '___sec42'),
              ('Matrix-vector notation  and activation', 3, None, '___sec43'),
              ('Activation functions', 3, None, '___sec44'),
              ('Activation functions, Logistic and Hyperbolic ones',
               3,
               None,
               '___sec45'),
              ('Relevance', 3, None, '___sec46'),
              ('The multilayer  perceptron (MLP)', 2, None, '___sec47'),
              ('From one to many layers, the universal approximation theorem',
               2,
               None,
               '___sec48'),
              ('Deriving the back propagation code for a multilayer perceptron '
               'model',
               2,
               None,
               '___sec49'),
              ('Definitions', 2, None, '___sec50'),
              ('Derivatives and the chain rule', 2, None, '___sec51'),
              ('Derivative of the cost function', 2, None, '___sec52'),
              ('Bringing it together, first back propagation equation',
               2,
               None,
               '___sec53'),
              ('Derivatives in terms of $z_j^L$', 2, None, '___sec54'),
              ('Bringing it together', 2, None, '___sec55'),
              ('Final back propagating equation', 2, None, '___sec56'),
              ('Setting up the Back propagation algorithm',
               2,
               None,
               '___sec57')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="week40-bs.html">Week 40: From Stochastic Gradient Descent to Neural networks</a>
  </div>

  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="._week40-bs001.html#___sec0" style="font-size: 80%;"><b>Plan for week 40</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs002.html#___sec1" style="font-size: 80%;"><b>Overview video for week 40</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs003.html#___sec2" style="font-size: 80%;"><b>Stochastic Gradient Descent</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs004.html#___sec3" style="font-size: 80%;"><b>Computation of gradients</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs005.html#___sec4" style="font-size: 80%;"><b>SGD example</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs006.html#___sec5" style="font-size: 80%;"><b>The gradient step</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs007.html#___sec6" style="font-size: 80%;"><b>Simple example code</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs008.html#___sec7" style="font-size: 80%;"><b>When do we stop?</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs009.html#___sec8" style="font-size: 80%;"><b>Slightly different approach</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs010.html#___sec9" style="font-size: 80%;"><b>Program for stochastic gradient</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs011.html#___sec10" style="font-size: 80%;"><b>Momentum based GD</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs012.html#___sec11" style="font-size: 80%;"><b>More on momentum based approaches</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs013.html#___sec12" style="font-size: 80%;"><b>Momentum parameter</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs014.html#___sec13" style="font-size: 80%;"><b>Second moment of the gradient</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs015.html#___sec14" style="font-size: 80%;"><b>RMS prop</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs016.html#___sec15" style="font-size: 80%;"><b>ADAM optimizer</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs017.html#___sec16" style="font-size: 80%;"><b>Practical tips</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs018.html#___sec17" style="font-size: 80%;"><b>Automatic differentiation</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs019.html#___sec18" style="font-size: 80%;"><b>Using autograd</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs020.html#___sec19" style="font-size: 80%;"><b>Autograd with more complicated functions</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs021.html#___sec20" style="font-size: 80%;"><b>More complicated functions using the elements of their arguments directly</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs022.html#___sec21" style="font-size: 80%;"><b>Functions using mathematical functions from Numpy</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs023.html#___sec22" style="font-size: 80%;"><b>More autograd</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs024.html#___sec23" style="font-size: 80%;"><b>And  with loops</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs025.html#___sec24" style="font-size: 80%;"><b>Using recursion</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs026.html#___sec25" style="font-size: 80%;"><b>Unsupported functions</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs027.html#___sec26" style="font-size: 80%;"><b>The syntax a.dot(b) when finding the dot product</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs028.html#___sec27" style="font-size: 80%;"><b>Recommended to avoid</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs029.html#___sec28" style="font-size: 80%;"><b>Neural networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs030.html#___sec29" style="font-size: 80%;"><b>Artificial neurons</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs031.html#___sec30" style="font-size: 80%;"><b>Neural network types</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs032.html#___sec31" style="font-size: 80%;"><b>Feed-forward neural networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs033.html#___sec32" style="font-size: 80%;"><b>Convolutional Neural Network</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs034.html#___sec33" style="font-size: 80%;"><b>Recurrent neural networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs035.html#___sec34" style="font-size: 80%;"><b>Other types of networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs036.html#___sec35" style="font-size: 80%;"><b>Multilayer perceptrons</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs037.html#___sec36" style="font-size: 80%;"><b>Why multilayer perceptrons?</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs038.html#___sec37" style="font-size: 80%;"><b>Mathematical model</b></a></li>
     <!-- navigation toc: --> <li><a href="#___sec38" style="font-size: 80%;"><b>Mathematical model</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs040.html#___sec39" style="font-size: 80%;"><b>Mathematical model</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs041.html#___sec40" style="font-size: 80%;"><b>Mathematical model</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs042.html#___sec41" style="font-size: 80%;"><b>Mathematical model</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs043.html#___sec42" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Matrix-vector notation</a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs044.html#___sec43" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Matrix-vector notation  and activation</a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs045.html#___sec44" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Activation functions</a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs046.html#___sec45" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Activation functions, Logistic and Hyperbolic ones</a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs047.html#___sec46" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Relevance</a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs048.html#___sec47" style="font-size: 80%;"><b>The multilayer  perceptron (MLP)</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs049.html#___sec48" style="font-size: 80%;"><b>From one to many layers, the universal approximation theorem</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs050.html#___sec49" style="font-size: 80%;"><b>Deriving the back propagation code for a multilayer perceptron model</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs051.html#___sec50" style="font-size: 80%;"><b>Definitions</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs052.html#___sec51" style="font-size: 80%;"><b>Derivatives and the chain rule</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs053.html#___sec52" style="font-size: 80%;"><b>Derivative of the cost function</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs054.html#___sec53" style="font-size: 80%;"><b>Bringing it together, first back propagation equation</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs055.html#___sec54" style="font-size: 80%;"><b>Derivatives in terms of \( z_j^L \)</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs056.html#___sec55" style="font-size: 80%;"><b>Bringing it together</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs057.html#___sec56" style="font-size: 80%;"><b>Final back propagating equation</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs058.html#___sec57" style="font-size: 80%;"><b>Setting up the Back propagation algorithm</b></a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->

<div class="container">

<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->

<a name="part0039"></a>
<!-- !split -->

<h2 id="___sec38" class="anchor">Mathematical model  </h2>

<p>
First, for each node \( i \) in the first hidden layer, we calculate a weighted sum \( z_i^1 \) of the input coordinates \( x_j \),

$$
\begin{equation} z_i^1 = \sum_{j=1}^{M} w_{ij}^1 x_j + b_i^1
\tag{7}
\end{equation}
$$

<p>
Here \( b_i \) is the so-called bias which is normally needed in
case of zero activation weights or inputs. How to fix the biases and
the weights will be discussed below.  The value of \( z_i^1 \) is the
argument to the activation function \( f_i \) of each node \( i \), The
variable \( M \) stands for all possible inputs to a given node \( i \) in the
first layer.  We define  the output \( y_i^1 \) of all neurons in layer 1 as

$$
\begin{equation}
 y_i^1 = f(z_i^1) = f\left(\sum_{j=1}^M w_{ij}^1 x_j  + b_i^1\right)
\tag{8}
\end{equation}
$$

<p>
where we assume that all nodes in the same layer have identical
activation functions, hence the notation \( f \). In general, we could assume in the more general case that different layers have different activation functions.
In this case we would identify these functions with a superscript \( l \) for the \( l \)-th layer,

$$
\begin{equation}
 y_i^l = f^l(u_i^l) = f^l\left(\sum_{j=1}^{N_{l-1}} w_{ij}^l y_j^{l-1} + b_i^l\right)
\tag{9}
\end{equation}
$$

<p>
where \( N_l \) is the number of nodes in layer \( l \). When the output of
all the nodes in the first hidden layer are computed, the values of
the subsequent layer can be calculated and so forth until the output
is obtained.

<p>
<p>
<!-- navigation buttons at the bottom of the page -->
<ul class="pagination">
<li><a href="._week40-bs038.html">&laquo;</a></li>
  <li><a href="._week40-bs000.html">1</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week40-bs031.html">32</a></li>
  <li><a href="._week40-bs032.html">33</a></li>
  <li><a href="._week40-bs033.html">34</a></li>
  <li><a href="._week40-bs034.html">35</a></li>
  <li><a href="._week40-bs035.html">36</a></li>
  <li><a href="._week40-bs036.html">37</a></li>
  <li><a href="._week40-bs037.html">38</a></li>
  <li><a href="._week40-bs038.html">39</a></li>
  <li class="active"><a href="._week40-bs039.html">40</a></li>
  <li><a href="._week40-bs040.html">41</a></li>
  <li><a href="._week40-bs041.html">42</a></li>
  <li><a href="._week40-bs042.html">43</a></li>
  <li><a href="._week40-bs043.html">44</a></li>
  <li><a href="._week40-bs044.html">45</a></li>
  <li><a href="._week40-bs045.html">46</a></li>
  <li><a href="._week40-bs046.html">47</a></li>
  <li><a href="._week40-bs047.html">48</a></li>
  <li><a href="._week40-bs048.html">49</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week40-bs058.html">59</a></li>
  <li><a href="._week40-bs040.html">&raquo;</a></li>
</ul>
<!-- ------------------- end of main content --------------- -->

</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>

<!-- Bootstrap footer
<footer>
<a href="http://..."><img width="250" align=right src="http://..."></a>
</footer>
-->


<center style="font-size:80%">
<!-- copyright only on the titlepage -->
</center>


</body>
</html>
    


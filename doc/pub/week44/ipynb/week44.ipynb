{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e9d457a",
   "metadata": {},
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html week44.do.txt --no_mako -->\n",
    "<!-- dom:TITLE: Week 44: Decision Trees, Ensemble methods  and Random Forests -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da83506",
   "metadata": {},
   "source": [
    "# Week 44: Decision Trees, Ensemble methods  and Random Forests\n",
    "**Morten Hjorth-Jensen**, Department of Physics, University of Oslo and Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University\n",
    "\n",
    "Date: **Nov 4, 2022**\n",
    "\n",
    "Copyright 1999-2022, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d38a800",
   "metadata": {},
   "source": [
    "## Overview of week 44\n",
    "\n",
    "* Thursday: Basics of decision trees, classification and regression algorithms\n",
    "\n",
    "  * [Video of lecture](https://youtu.be/7jexGH5SOOE)\n",
    "\n",
    "  * **Note**: Thursday's lecture is digital only due to [High-school post-education day](https://www.uio.no/om/samarbeid/skole/fagped-dag/)\n",
    "\n",
    "* Friday: Decision trees and  ensemble models (bagging and random forests)\n",
    "\n",
    "**Videos.**\n",
    "\n",
    "1. [Video on Decision trees](https://www.youtube.com/watch?v=RmajweUFKvM&ab_channel=Simplilearn)\n",
    "\n",
    "**Reading.**\n",
    "\n",
    "1. Decision Trees: Geron's chapter 6 covers decision trees while ensemble models, voting and bagging are discussed in chapter 7. See also lecture from [STK-IN4300, lecture 7](https://www.uio.no/studier/emner/matnat/math/STK-IN4300/h20/slides/lecture_7.pdf). Chapter 9.2 of Hastie et al contains also a good discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31105a9",
   "metadata": {},
   "source": [
    "## Digression First\n",
    "\n",
    "For those of you interested in the fast growing areas of applications of Machine Learning, this article about [Applications and techniques for fast machine learning in science](https://arxiv.org/abs/2110.13041)  may be interesting.\n",
    "\n",
    "It has several interesting perspectives and highly interesting\n",
    "applications that link scientific discoveries with efficient software\n",
    "and hardware. The emphasis is onintegrating power Machine Learning\n",
    "methods into the real-time experimental data processing loop to\n",
    "accelerate scientific discovery."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fe25d8",
   "metadata": {},
   "source": [
    "## Decision trees, overarching aims\n",
    "\n",
    "We start here with the most basic algorithm, the so-called decision\n",
    "tree. With this basic algorithm we can in turn build more complex\n",
    "networks, spanning from homogeneous and heterogenous forests (bagging,\n",
    "random forests and more) to one of the most popular supervised\n",
    "algorithms nowadays, the extreme gradient boosting, or just\n",
    "XGBoost. But let us start with the simplest possible ingredient.\n",
    "\n",
    "Decision trees are supervised learning algorithms used for both,\n",
    "classification and regression tasks.\n",
    "\n",
    "The main idea of decision trees\n",
    "is to find those descriptive features which contain the most\n",
    "**information** regarding the target feature and then split the dataset\n",
    "along the values of these features such that the target feature values\n",
    "for the resulting underlying datasets are as pure as possible.\n",
    "\n",
    "The descriptive features which reproduce best the target/output features are normally  said\n",
    "to be the most informative ones. The process of finding the **most\n",
    "informative** feature is done until we accomplish a stopping criteria\n",
    "where we then finally end up in so called **leaf nodes**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6686ff",
   "metadata": {},
   "source": [
    "## Basics of a tree\n",
    "\n",
    "A decision tree is typically divided into a **root node**, the **interior nodes**,\n",
    "and the final **leaf nodes** or just **leaves**. These entities are then connected by so-called **branches**.\n",
    "\n",
    "The leaf nodes\n",
    "contain the predictions we will make for new query instances presented\n",
    "to our trained model. This is possible since the model has \n",
    "learned the underlying structure of the training data and hence can,\n",
    "given some assumptions, make predictions about the target feature value\n",
    "(class) of unseen query instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce62a1a1",
   "metadata": {},
   "source": [
    "## A Sketch of a Tree, Regression problem\n",
    "\n",
    "[See handwritten notes November 3](https://github.com/CompPhysics/MachineLearning/blob/master/doc/HandWrittenNotes/2022/NotesNov32022.pdf)\n",
    "\n",
    "<!-- FIGURE: [DataFiles/Regsimpletree.png, width=600 frac=0.8] -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8c3236",
   "metadata": {},
   "source": [
    "## A Sketch of a Tree, Classification  problem\n",
    "\n",
    "[See handwritten notes November 3](https://github.com/CompPhysics/MachineLearning/blob/master/doc/HandWrittenNotes/2022/NotesNov32022.pdf)\n",
    "<!-- FIGURE: [DataFiles/Classimpletree.png, width=600 frac=0.8] -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d450c9c5",
   "metadata": {},
   "source": [
    "## A typical Decision Tree with its pertinent Jargon, Classification Problem\n",
    "\n",
    "<!-- dom:FIGURE: [DataFiles/cancer.png, width=600 frac=0.8] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"DataFiles/cancer.png\" width=\"600\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->\n",
    "\n",
    "This tree was produced using the Wisconsin cancer data (discussed here as well, see code examples below) using **Scikit-Learn**'s decision tree classifier. Here we have used the so-called **gini** index (see below) to split the various branches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6065bf9d",
   "metadata": {},
   "source": [
    "## General Features\n",
    "\n",
    "The overarching approach to decision trees is a top-down approach.\n",
    "\n",
    "* A leaf provides the classification of a given instance.\n",
    "\n",
    "* A node specifies a test of some attribute of the instance.\n",
    "\n",
    "* A branch corresponds to a possible values of an attribute.\n",
    "\n",
    "* An instance is classified by starting at the root node of the tree, testing the attribute specified by this node, then moving down the tree branch corresponding to the value of the attribute in the given example.\n",
    "\n",
    "This process is then repeated for the subtree rooted at the new\n",
    "node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab43c19",
   "metadata": {},
   "source": [
    "## How do we set it up?\n",
    "\n",
    "In simplified terms, the process of training a decision tree and\n",
    "predicting the target features of query instances is as follows:\n",
    "\n",
    "1. Present a dataset containing of a number of training instances characterized by a number of descriptive features and a target feature\n",
    "\n",
    "2. Train the decision tree model by continuously splitting the target feature along the values of the descriptive features using a measure of information gain during the training process\n",
    "\n",
    "3. Grow the tree until we accomplish a stopping criteria create leaf nodes which represent the *predictions* we want to make for new query instances\n",
    "\n",
    "4. Show query instances to the tree and run down the tree until we arrive at leaf nodes\n",
    "\n",
    "Then we are essentially done!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35f5834",
   "metadata": {},
   "source": [
    "## Decision trees and Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6caa26f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "steps=250\n",
    "\n",
    "distance=0\n",
    "x=0\n",
    "distance_list=[]\n",
    "steps_list=[]\n",
    "while x<steps:\n",
    "    distance+=np.random.randint(-1,2)\n",
    "    distance_list.append(distance)\n",
    "    x+=1\n",
    "    steps_list.append(x)\n",
    "plt.plot(steps_list,distance_list, color='green', label=\"Random Walk Data\")\n",
    "\n",
    "steps_list=np.asarray(steps_list)\n",
    "distance_list=np.asarray(distance_list)\n",
    "\n",
    "X=steps_list[:,np.newaxis]\n",
    "\n",
    "#Polynomial fits\n",
    "\n",
    "#Degree 2\n",
    "poly_features=PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly=poly_features.fit_transform(X)\n",
    "\n",
    "lin_reg=LinearRegression()\n",
    "poly_fit=lin_reg.fit(X_poly,distance_list)\n",
    "b=lin_reg.coef_\n",
    "c=lin_reg.intercept_\n",
    "print (\"2nd degree coefficients:\")\n",
    "print (\"zero power: \",c)\n",
    "print (\"first power: \", b[0])\n",
    "print (\"second power: \",b[1])\n",
    "\n",
    "z = np.arange(0, steps, .01)\n",
    "z_mod=b[1]*z**2+b[0]*z+c\n",
    "\n",
    "fit_mod=b[1]*X**2+b[0]*X+c\n",
    "plt.plot(z, z_mod, color='r', label=\"2nd Degree Fit\")\n",
    "plt.title(\"Polynomial Regression\")\n",
    "\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Distance\")\n",
    "\n",
    "#Degree 10\n",
    "poly_features10=PolynomialFeatures(degree=10, include_bias=False)\n",
    "X_poly10=poly_features10.fit_transform(X)\n",
    "\n",
    "poly_fit10=lin_reg.fit(X_poly10,distance_list)\n",
    "\n",
    "y_plot=poly_fit10.predict(X_poly10)\n",
    "plt.plot(X, y_plot, color='black', label=\"10th Degree Fit\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Decision Tree Regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regr_1=DecisionTreeRegressor(max_depth=2)\n",
    "regr_2=DecisionTreeRegressor(max_depth=5)\n",
    "regr_3=DecisionTreeRegressor(max_depth=7)\n",
    "regr_1.fit(X, distance_list)\n",
    "regr_2.fit(X, distance_list)\n",
    "regr_3.fit(X, distance_list)\n",
    "\n",
    "X_test = np.arange(0.0, steps, 0.01)[:, np.newaxis]\n",
    "y_1 = regr_1.predict(X_test)\n",
    "y_2 = regr_2.predict(X_test)\n",
    "y_3=regr_3.predict(X_test)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure()\n",
    "plt.scatter(X, distance_list, s=2.5, c=\"black\", label=\"data\")\n",
    "plt.plot(X_test, y_1, color=\"red\",\n",
    "         label=\"max_depth=2\", linewidth=2)\n",
    "plt.plot(X_test, y_2, color=\"green\", label=\"max_depth=5\", linewidth=2)\n",
    "plt.plot(X_test, y_3, color=\"m\", label=\"max_depth=7\", linewidth=2)\n",
    "\n",
    "plt.xlabel(\"Data\")\n",
    "plt.ylabel(\"Darget\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb4d7cf",
   "metadata": {},
   "source": [
    "## Building a tree, regression\n",
    "\n",
    "There are mainly two steps\n",
    "1. We split the predictor space (the set of possible values $x_1,x_2,\\dots, x_p$) into $J$ distinct and non-non-overlapping regions, $R_1,R_2,\\dots,R_J$.  \n",
    "\n",
    "2. For every observation that falls into the region $R_j$ , we make the same prediction, which is simply the mean of the response values for the training observations in $R_j$.\n",
    "\n",
    "How do we construct the regions $R_1,\\dots,R_J$?  In theory, the\n",
    "regions could have any shape. However, we choose to divide the\n",
    "predictor space into high-dimensional rectangles, or boxes, for\n",
    "simplicity and for ease of interpretation of the resulting predictive\n",
    "model. The goal is to find boxes $R_1,\\dots,R_J$ that minimize the\n",
    "MSE, given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c984bd",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sum_{j=1}^J\\sum_{i\\in R_j}(y_i-\\overline{y}_{R_j})^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9906c4c",
   "metadata": {},
   "source": [
    "where $\\overline{y}_{R_j}$  is the mean response for the training observations \n",
    "within box $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96557142",
   "metadata": {},
   "source": [
    "## A top-down approach, recursive binary splitting\n",
    "\n",
    "Unfortunately, it is computationally infeasible to consider every\n",
    "possible partition of the feature space into $J$ boxes.  The common\n",
    "strategy is to take a top-down approach\n",
    "\n",
    "The approach is top-down because it begins at the top of the tree (all\n",
    "observations belong to a single region) and then successively splits\n",
    "the predictor space; each split is indicated via two new branches\n",
    "further down on the tree. It is greedy because at each step of the\n",
    "tree-building process, the best split is made at that particular step,\n",
    "rather than looking ahead and picking a split that will lead to a\n",
    "better tree in some future step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30bcd98",
   "metadata": {},
   "source": [
    "## Making a tree\n",
    "\n",
    "In order to implement the recursive binary splitting we start by selecting\n",
    "the predictor $x_j$ and a cutpoint $s$ that splits the predictor space into two regions $R_1$ and $R_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea89bbb5",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left\\{X\\vert x_j < s\\right\\},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1516b6cc",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893e24a3",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left\\{X\\vert x_j \\geq s\\right\\},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b356f7",
   "metadata": {},
   "source": [
    "so that we obtain the lowest MSE, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0d9aca",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sum_{i:x_i\\in R_j}(y_i-\\overline{y}_{R_1})^2+\\sum_{i:x_i\\in R_2}(y_i-\\overline{y}_{R_2})^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f3358d",
   "metadata": {},
   "source": [
    "which we want to minimize by considering all predictors\n",
    "$x_1,x_2,\\dots,x_p$.  We consider also all possible values of $s$ for\n",
    "each predictor. These values could be determined by randomly assigned\n",
    "numbers or by starting at the midpoint and then proceed till we find\n",
    "an optimal value.\n",
    "\n",
    "For any $j$ and $s$, we define the pair of half-planes where\n",
    "$\\overline{y}_{R_1}$ is the mean response for the training\n",
    "observations in $R_1(j,s)$, and $\\overline{y}_{R_2}$ is the mean\n",
    "response for the training observations in $R_2(j,s)$.\n",
    "\n",
    "Finding the values of $j$ and $s$ that minimize the above equation can be\n",
    "done quite quickly, especially when the number of features $p$ is not\n",
    "too large.\n",
    "\n",
    "Next, we repeat the process, looking\n",
    "for the best predictor and best cutpoint in order to split the data\n",
    "further so as to minimize the MSE within each of the resulting\n",
    "regions. However, this time, instead of splitting the entire predictor\n",
    "space, we split one of the two previously identified regions. We now\n",
    "have three regions. Again, we look to split one of these three regions\n",
    "further, so as to minimize the MSE. The process continues until a\n",
    "stopping criterion is reached; for instance, we may continue until no\n",
    "region contains more than five observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a339714",
   "metadata": {},
   "source": [
    "## Pruning the tree\n",
    "\n",
    "The above procedure is rather straightforward, but leads often to\n",
    "overfitting and unnecessarily large and complicated trees. The basic\n",
    "idea is to grow a large tree $T_0$ and then prune it back in order to\n",
    "obtain a subtree. A smaller tree with fewer splits (fewer regions) can\n",
    "lead to smaller variance and better interpretation at the cost of a\n",
    "little more bias.\n",
    "\n",
    "The so-called Cost complexity pruning algorithm gives us a\n",
    "way to do just this. Rather than considering every possible subtree,\n",
    "we consider a sequence of trees indexed by a nonnegative tuning\n",
    "parameter $\\alpha$.\n",
    "\n",
    "Read more at the following [Scikit-Learn link on pruning](https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html#sphx-glr-auto-examples-tree-plot-cost-complexity-pruning-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f066ed",
   "metadata": {},
   "source": [
    "## Cost complexity pruning\n",
    "\n",
    "For each value of $\\alpha$  there corresponds a subtree $T \\in T_0$ such that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753ee3d1",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sum_{m=1}^{\\overline{T}}\\sum_{i:x_i\\in R_m}(y_i-\\overline{y}_{R_m})^2+\\alpha\\overline{T},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e0c324",
   "metadata": {},
   "source": [
    "is as small as possible. Here $\\overline{T}$ is \n",
    "the number of terminal nodes of the tree $T$ , $R_m$ is the\n",
    "rectangle (i.e. the subset of predictor space)  corresponding to the $m$-th terminal node.\n",
    "\n",
    "The tuning parameter $\\alpha$ controls a trade-off between the subtree’s\n",
    "complexity and its fit to the training data. When $\\alpha = 0$, then the\n",
    "subtree $T$ will simply equal $T_0$, \n",
    "because then the above equation just measures the\n",
    "training error. \n",
    "However, as $\\alpha$ increases, there is a price to pay for\n",
    "having a tree with many terminal nodes. The above equation will\n",
    "tend to be minimized for a smaller subtree. \n",
    "\n",
    "It turns out that as we increase $\\alpha$ from zero\n",
    "branches get pruned from the tree in a nested and predictable fashion,\n",
    "so obtaining the whole sequence of subtrees as a function of $\\alpha$ is\n",
    "easy. We can select a value of $\\alpha$ using a validation set or using\n",
    "cross-validation. We then return to the full data set and obtain the\n",
    "subtree corresponding to $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813adcf0",
   "metadata": {},
   "source": [
    "## Schematic Regression Procedure\n",
    "\n",
    "**Building a Regression Tree.**\n",
    "\n",
    "1. Use recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations.\n",
    "\n",
    "2. Apply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees, as a function of $\\alpha$.\n",
    "\n",
    "3. Use for example $K$-fold cross-validation to choose $\\alpha$. Divide the training observations into $K$ folds. For each $k=1,2,\\dots,K$ we: \n",
    "\n",
    "  * repeat steps 1 and 2 on all but the $k$-th fold of the training data. \n",
    "\n",
    "  * Then we valuate the mean squared prediction error on the data in the left-out $k$-th fold, as a function of $\\alpha$.\n",
    "\n",
    "  * Finally  we average the results for each value of $\\alpha$, and pick $\\alpha$ to minimize the average error.\n",
    "\n",
    "4. Return the subtree from Step 2 that corresponds to the chosen value of $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48b2ef5",
   "metadata": {},
   "source": [
    "## A Classification Tree\n",
    "\n",
    "A classification tree is very similar to a regression tree, except\n",
    "that it is used to predict a qualitative response rather than a\n",
    "quantitative one. Recall that for a regression tree, the predicted\n",
    "response for an observation is given by the mean response of the\n",
    "training observations that belong to the same terminal node. In\n",
    "contrast, for a classification tree, we predict that each observation\n",
    "belongs to the most commonly occurring class of training observations\n",
    "in the region to which it belongs. In interpreting the results of a\n",
    "classification tree, we are often interested not only in the class\n",
    "prediction corresponding to a particular terminal node region, but\n",
    "also in the class proportions among the training observations that\n",
    "fall into that region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddbe38b",
   "metadata": {},
   "source": [
    "## Growing a classification tree\n",
    "\n",
    "The task of growing a\n",
    "classification tree is quite similar to the task of growing a\n",
    "regression tree. Just as in the regression setting, we use recursive\n",
    "binary splitting to grow a classification tree. However, in the\n",
    "classification setting, the MSE cannot be used as a criterion for making\n",
    "the binary splits.  A natural alternative to MSE is the **classification\n",
    "error rate**. Since we plan to assign an observation in a given region\n",
    "to the most commonly occurring error rate class of training\n",
    "observations in that region, the classification error rate is simply\n",
    "the fraction of the training observations in that region that do not\n",
    "belong to the most common class. \n",
    "\n",
    "When building a classification tree, either the Gini index or the\n",
    "entropy are typically used to evaluate the quality of a particular\n",
    "split, since these two approaches are more sensitive to node purity\n",
    "than is the classification error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fba89d",
   "metadata": {},
   "source": [
    "## Classification tree, how to split nodes\n",
    "\n",
    "If our targets are the outcome of a classification process that takes\n",
    "for example $k=1,2,\\dots,K$ values, the only thing we need to think of\n",
    "is to set up the splitting criteria for each node.\n",
    "\n",
    "We define a PDF $p_{mk}$ that represents the number of observations of\n",
    "a class $k$ in a region $R_m$ with $N_m$ observations. We represent\n",
    "this likelihood function in terms of the proportion $I(y_i=k)$ of\n",
    "observations of this class in the region $R_m$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22166b43",
   "metadata": {},
   "source": [
    "$$\n",
    "p_{mk} = \\frac{1}{N_m}\\sum_{x_i\\in R_m}I(y_i=k).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e4074c",
   "metadata": {},
   "source": [
    "We let $p_{mk}$ represent the majority class of observations in region\n",
    "$m$. The three most common ways of splitting a node are given by\n",
    "\n",
    "* Misclassification error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e1e02c",
   "metadata": {},
   "source": [
    "$$\n",
    "p_{mk} = \\frac{1}{N_m}\\sum_{x_i\\in R_m}I(y_i\\ne k) = 1-p_{mk}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25506a4a",
   "metadata": {},
   "source": [
    "* Gini index $g$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e859f7d",
   "metadata": {},
   "source": [
    "$$\n",
    "g = \\sum_{k=1}^K p_{mk}(1-p_{mk}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75118af9",
   "metadata": {},
   "source": [
    "* Information entropy or just entropy $s$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad7a4a9",
   "metadata": {},
   "source": [
    "$$\n",
    "s = -\\sum_{k=1}^K p_{mk}\\log{p_{mk}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80392de1",
   "metadata": {},
   "source": [
    "## Visualizing the Tree, Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "448f2c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "from IPython.display import Image \n",
    "from pydot import graph_from_dot_data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "print(X)\n",
    "y = pd.Categorical.from_codes(cancer.target, cancer.target_names)\n",
    "y = pd.get_dummies(y)\n",
    "print(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "tree_clf = DecisionTreeClassifier(max_depth=5)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "export_graphviz(\n",
    "    tree_clf,\n",
    "    out_file=\"DataFiles/cancer.dot\",\n",
    "    feature_names=cancer.feature_names,\n",
    "    class_names=cancer.target_names,\n",
    "    rounded=True,\n",
    "    filled=True\n",
    ")\n",
    "cmd = 'dot -Tpng DataFiles/cancer.dot -o DataFiles/cancer.png'\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976d6f54",
   "metadata": {},
   "source": [
    "## Visualizing the Tree, The Moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "245a63bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "from sklearn.model_selection import  train_test_split \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.tree import export_graphviz\n",
    "from pydot import graph_from_dot_data\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "np.random.seed(42)\n",
    "X, y = make_moons(n_samples=100, noise=0.25, random_state=53)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=0)\n",
    "tree_clf = DecisionTreeClassifier(max_depth=5)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "export_graphviz(\n",
    "    tree_clf,\n",
    "    out_file=\"DataFiles/moons.dot\",\n",
    "    rounded=True,\n",
    "    filled=True\n",
    ")\n",
    "cmd = 'dot -Tpng DataFiles/moons.dot -o DataFiles/moons.png'\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f4a124",
   "metadata": {},
   "source": [
    "## Other ways of visualizing the trees\n",
    "\n",
    "**Scikit-Learn** has also another way to visualize the trees which is very useful, here with the Iris data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89e96fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "X, y = load_iris(return_X_y=True)\n",
    "tree_clf = tree.DecisionTreeClassifier()\n",
    "tree_clf = tree_clf.fit(X, y)\n",
    "# and then plot the tree\n",
    "tree.plot_tree(tree_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f85a09f",
   "metadata": {},
   "source": [
    "## Printing out as text\n",
    "\n",
    "Alternatively, the tree can also be exported in textual format with the function exporttext.\n",
    "This method doesn’t require the installation of external libraries and is more compact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6c15b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_text\n",
    "iris = load_iris()\n",
    "decision_tree = DecisionTreeClassifier(random_state=0, max_depth=2)\n",
    "decision_tree = decision_tree.fit(iris.data, iris.target)\n",
    "r = export_text(decision_tree, feature_names=iris['feature_names'])\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b8d1dd",
   "metadata": {},
   "source": [
    "## Algorithms for Setting up Decision Trees\n",
    "\n",
    "Two algorithms stand out in the set up of decision trees:\n",
    "1. The CART (Classification And Regression Tree) algorithm for both classification and regression\n",
    "\n",
    "2. The ID3 algorithm based on the computation of the information gain for classification\n",
    "\n",
    "We discuss both algorithms with applications here. The popular library\n",
    "**Scikit-Learn** uses the CART algorithm. For classification problems\n",
    "you can use either the **gini** index or the **entropy** to split a tree\n",
    "in two branches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696574e0",
   "metadata": {},
   "source": [
    "## The CART algorithm for Classification\n",
    "\n",
    "For classification, the CART algorithm splits the data set in two subsets using a single feature $k$ and a threshold $t_k$.\n",
    "This could be for example a threshold set by a number below a certain circumference of a malign tumor.\n",
    "\n",
    "How do we find these two quantities?\n",
    "We search for the pair $(k,t_k)$ that produces the purest subset using for example the **gini** factor $G$.\n",
    "The cost function it tries to minimize is then"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cfbe7f",
   "metadata": {},
   "source": [
    "$$\n",
    "C(k,t_k) = \\frac{m_{\\mathrm{left}}}{m}G_{\\mathrm{left}}+ \\frac{m_{\\mathrm{right}}}{m}G_{\\mathrm{right}},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b5217a",
   "metadata": {},
   "source": [
    "where $G_{\\mathrm{left/right}}$ measures the impurity of the left/right subset  and $m_{\\mathrm{left/right}}$\n",
    " is the number of instances in the left/right subset\n",
    "\n",
    "Once it has successfully split the training set in two, it splits the subsets using the same logic, then the subsubsets\n",
    "and so on, recursively. It stops recursing once it reaches the maximum depth (defined by the\n",
    "$max\\_depth$ hyperparameter), or if it cannot find a split that will reduce impurity. A few other\n",
    "hyperparameters control additional stopping conditions such as the $min\\_samples\\_split$,\n",
    "$min\\_samples\\_leaf$, $min\\_weight\\_fraction\\_leaf$, and $max\\_leaf\\_nodes$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eeda5d",
   "metadata": {},
   "source": [
    "## The CART algorithm for Regression\n",
    "\n",
    "The CART algorithm for regression works is similar to the one for classification except that instead of trying to split the\n",
    "training set in a way that minimizes say the **gini** or **entropy** impurity, it now tries to split the training set in a way that minimizes our well-known mean-squared error (MSE). The cost function is now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0978df9",
   "metadata": {},
   "source": [
    "$$\n",
    "C(k,t_k) = \\frac{m_{\\mathrm{left}}}{m}\\mathrm{MSE}_{\\mathrm{left}}+ \\frac{m_{\\mathrm{right}}}{m}\\mathrm{MSE}_{\\mathrm{right}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40902e6c",
   "metadata": {},
   "source": [
    "Here the MSE for a specific node is defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091dc8c0",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{MSE}_{\\mathrm{node}}=\\frac{1}{m_\\mathrm{node}}\\sum_{i\\in \\mathrm{node}}(\\overline{y}_{\\mathrm{node}}-y_i)^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49903d3e",
   "metadata": {},
   "source": [
    "with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bff5f6",
   "metadata": {},
   "source": [
    "$$\n",
    "\\overline{y}_{\\mathrm{node}}=\\frac{1}{m_\\mathrm{node}}\\sum_{i\\in \\mathrm{node}}y_i,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd13ac16",
   "metadata": {},
   "source": [
    "the mean value of all observations in a specific node.\n",
    "\n",
    "Without any regularization, the regression task for decision trees, \n",
    "just like for classification tasks, is  prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec454ba0",
   "metadata": {},
   "source": [
    "## Why binary splits?\n",
    "\n",
    "It is custom to split to a tree uising binary splits. The reason is\n",
    "that multiway splits fragment the data too quickly, leaving\n",
    "insufficient data at the next level down.  Multiway splits can be\n",
    "achieved by a series of binary split and this is normally preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1419e8a",
   "metadata": {},
   "source": [
    "## Computing a Tree using the Gini Index\n",
    "\n",
    "Consider the following example with attributes/features and two\n",
    "possible outcomes (classes)  for each attribute.  Assume we wish to find some\n",
    "correlations between the average grade of a student as function of the\n",
    "number of hours studied and hours slept. We want also to correlate the\n",
    "grade in a given course with the general trend, whether the students\n",
    "recently has gotten grades below average or above.\n",
    "\n",
    "We have three features/attributes\n",
    "1. Trend of average grades before present course, classified as either below  or above  the average grade of the whole class  \n",
    "\n",
    "2. The number of hours studies, classified again as either higher (more than 3 hours per day) or lower . Here we have used a standard for one $ECTS$ which is scaled to 25-30 hours of work for a semester which lasts 18 weeks, with 15 weeks of lectures and 3 weeks for exams, assuming a total of 30 ECTS per semester. \n",
    "\n",
    "3. The number of hours slept as high for more than $8$ hours and below  for less than 8 hours of sleep, classified again as either high or low\n",
    "\n",
    "4. The final grade whether it is above or below average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf7278e",
   "metadata": {},
   "source": [
    "## The Table\n",
    "\n",
    "<table class=\"dotable\" border=\"1\">\n",
    "<thead>\n",
    "<tr><th align=\"center\">Grade Trend</th> <th align=\"center\">Hours slept</th> <th align=\"center\">Hours Studied</th> <th align=\"center\">Grade</th> </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td align=\"center\">   Above          </td> <td align=\"center\">   Low            </td> <td align=\"center\">   High             </td> <td align=\"center\">   Above    </td> </tr>\n",
    "<tr><td align=\"center\">   Below          </td> <td align=\"center\">   High           </td> <td align=\"center\">   Low              </td> <td align=\"center\">   Below    </td> </tr>\n",
    "<tr><td align=\"center\">   Above          </td> <td align=\"center\">   Low            </td> <td align=\"center\">   High             </td> <td align=\"center\">   Above    </td> </tr>\n",
    "<tr><td align=\"center\">   Above          </td> <td align=\"center\">   High           </td> <td align=\"center\">   High             </td> <td align=\"center\">   Above    </td> </tr>\n",
    "<tr><td align=\"center\">   Below          </td> <td align=\"center\">   Low            </td> <td align=\"center\">   High             </td> <td align=\"center\">   Below    </td> </tr>\n",
    "<tr><td align=\"center\">   Above          </td> <td align=\"center\">   Low            </td> <td align=\"center\">   Low              </td> <td align=\"center\">   Below    </td> </tr>\n",
    "<tr><td align=\"center\">   Below          </td> <td align=\"center\">   High           </td> <td align=\"center\">   High             </td> <td align=\"center\">   Below    </td> </tr>\n",
    "<tr><td align=\"center\">   Below          </td> <td align=\"center\">   Low            </td> <td align=\"center\">   High             </td> <td align=\"center\">   Below    </td> </tr>\n",
    "<tr><td align=\"center\">   Above          </td> <td align=\"center\">   Low            </td> <td align=\"center\">   Low              </td> <td align=\"center\">   Below    </td> </tr>\n",
    "<tr><td align=\"center\">   Above          </td> <td align=\"center\">   High           </td> <td align=\"center\">   High             </td> <td align=\"center\">   Above    </td> </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf09301a",
   "metadata": {},
   "source": [
    "## Computing the various Gini Indices\n",
    "\n",
    "In computations we will translate all classes into numbers. Being\n",
    "these binary classes, they can easily be split into ones and zeros.\n",
    "\n",
    "**Gini index for Average trend.**\n",
    "\n",
    "[See handwritten notes November 3](https://github.com/CompPhysics/MachineLearning/blob/master/doc/HandWrittenNotes/2022/NotesNov32022.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1071fd",
   "metadata": {},
   "source": [
    "## Computing the various Gini Indices, Hours slept\n",
    "\n",
    "**Gini index for hour slept.**\n",
    "\n",
    "[See handwritten notes November 3](https://github.com/CompPhysics/MachineLearning/blob/master/doc/HandWrittenNotes/2022/NotesNov32022.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae964258",
   "metadata": {},
   "source": [
    "## Computing the various Gini Indices, Hours studied\n",
    "\n",
    "**Gini index for hour studied.**\n",
    "\n",
    "[See handwritten notes November 3](https://github.com/CompPhysics/MachineLearning/blob/master/doc/HandWrittenNotes/2022/NotesNov32022.pdf)\n",
    "\n",
    "For final tree, see the above handwritten notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50f3fa1",
   "metadata": {},
   "source": [
    "## A possible code using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7550c1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from IPython.display import Image \n",
    "from pydot import graph_from_dot_data\n",
    "import os\n",
    "\n",
    "# Where to save the figures and data files\n",
    "PROJECT_ROOT_DIR = \"Results\"\n",
    "FIGURE_ID = \"Results/FigureFiles\"\n",
    "DATA_ID = \"DataFiles/\"\n",
    "\n",
    "if not os.path.exists(PROJECT_ROOT_DIR):\n",
    "    os.mkdir(PROJECT_ROOT_DIR)\n",
    "\n",
    "if not os.path.exists(FIGURE_ID):\n",
    "    os.makedirs(FIGURE_ID)\n",
    "\n",
    "if not os.path.exists(DATA_ID):\n",
    "    os.makedirs(DATA_ID)\n",
    "\n",
    "def image_path(fig_id):\n",
    "    return os.path.join(FIGURE_ID, fig_id)\n",
    "\n",
    "def data_path(dat_id):\n",
    "    return os.path.join(DATA_ID, dat_id)\n",
    "\n",
    "def save_fig(fig_id):\n",
    "    plt.savefig(image_path(fig_id) + \".png\", format='png')\n",
    "\n",
    "infile = open(data_path(\"grades.csv\"),'r')\n",
    "\n",
    "# Read the experimental data with Pandas\n",
    "from IPython.display import display\n",
    "grades = pd.read_csv(infile)\n",
    "grades = pd.DataFrame(grades)\n",
    "display(grades)\n",
    "# Features and targets\n",
    "X = grades.loc[:, grades.columns != 'Grade'].values\n",
    "y = grades.loc[:, grades.columns == 'Grade'].values\n",
    "print(X)\n",
    "# Then do a Classification tree\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X, y)\n",
    "print(\"Train set accuracy with Decision Tree: {:.2f}\".format(tree_clf.score(X,y)))\n",
    "#transfer to a decision tree graph\n",
    "export_graphviz(\n",
    "    tree_clf,\n",
    "    out_file=\"DataFiles/grade.dot\",\n",
    "    rounded=True,\n",
    "    filled=True\n",
    ")\n",
    "cmd = 'dot -Tpng DataFiles/grade.dot -o DataFiles/grades.png'\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73371791",
   "metadata": {},
   "source": [
    "## Further example: Computing the Gini index\n",
    "\n",
    "The next example we will look at is a classical one in many Machine\n",
    "Learning applications. Based on various meteorological features, we\n",
    "have several so-called attributes which decide whether we at the end\n",
    "will do some outdoor activity like skiing, going for a bike ride etc\n",
    "etc.  The table here contains the feautures **outlook**, **temperature**,\n",
    "**humidity** and **wind**.  The target or output is whether we ride\n",
    "(True=1) or whether we do something else that day (False=0). The\n",
    "attributes for each feature are then sunny, overcast and rain for the\n",
    "outlook, hot, cold and mild for temperature, high and normal for\n",
    "humidity and weak and strong for wind.\n",
    "\n",
    "The table here summarizes the various attributes and\n",
    "<table class=\"dotable\" border=\"1\">\n",
    "<thead>\n",
    "<tr><th align=\"center\">Day</th> <th align=\"center\">Outlook </th> <th align=\"center\">Temperature</th> <th align=\"center\">Humidity</th> <th align=\"center\"> Wind </th> <th align=\"center\">Ride</th> </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td align=\"center\">   1      </td> <td align=\"center\">   Sunny       </td> <td align=\"center\">   Hot            </td> <td align=\"center\">   High        </td> <td align=\"center\">   Weak      </td> <td align=\"center\">   0       </td> </tr>\n",
    "<tr><td align=\"center\">   2      </td> <td align=\"center\">   Sunny       </td> <td align=\"center\">   Hot            </td> <td align=\"center\">   High        </td> <td align=\"center\">   Strong    </td> <td align=\"center\">   1       </td> </tr>\n",
    "<tr><td align=\"center\">   3      </td> <td align=\"center\">   Overcast    </td> <td align=\"center\">   Hot            </td> <td align=\"center\">   High        </td> <td align=\"center\">   Weak      </td> <td align=\"center\">   1       </td> </tr>\n",
    "<tr><td align=\"center\">   4      </td> <td align=\"center\">   Rain        </td> <td align=\"center\">   Mild           </td> <td align=\"center\">   High        </td> <td align=\"center\">   Weak      </td> <td align=\"center\">   1       </td> </tr>\n",
    "<tr><td align=\"center\">   5      </td> <td align=\"center\">   Rain        </td> <td align=\"center\">   Cool           </td> <td align=\"center\">   Normal      </td> <td align=\"center\">   Weak      </td> <td align=\"center\">   1       </td> </tr>\n",
    "<tr><td align=\"center\">   6      </td> <td align=\"center\">   Rain        </td> <td align=\"center\">   Cool           </td> <td align=\"center\">   Normal      </td> <td align=\"center\">   Strong    </td> <td align=\"center\">   0       </td> </tr>\n",
    "<tr><td align=\"center\">   7      </td> <td align=\"center\">   Overcast    </td> <td align=\"center\">   Cool           </td> <td align=\"center\">   Normal      </td> <td align=\"center\">   Strong    </td> <td align=\"center\">   1       </td> </tr>\n",
    "<tr><td align=\"center\">   8      </td> <td align=\"center\">   Sunny       </td> <td align=\"center\">   Mild           </td> <td align=\"center\">   High        </td> <td align=\"center\">   Weak      </td> <td align=\"center\">   0       </td> </tr>\n",
    "<tr><td align=\"center\">   9      </td> <td align=\"center\">   Sunny       </td> <td align=\"center\">   Cool           </td> <td align=\"center\">   Normal      </td> <td align=\"center\">   Weak      </td> <td align=\"center\">   1       </td> </tr>\n",
    "<tr><td align=\"center\">   10     </td> <td align=\"center\">   Rain        </td> <td align=\"center\">   Mild           </td> <td align=\"center\">   Normal      </td> <td align=\"center\">   Weak      </td> <td align=\"center\">   1       </td> </tr>\n",
    "<tr><td align=\"center\">   11     </td> <td align=\"center\">   Sunny       </td> <td align=\"center\">   Mild           </td> <td align=\"center\">   Normal      </td> <td align=\"center\">   Strong    </td> <td align=\"center\">   1       </td> </tr>\n",
    "<tr><td align=\"center\">   12     </td> <td align=\"center\">   Overcast    </td> <td align=\"center\">   Mild           </td> <td align=\"center\">   High        </td> <td align=\"center\">   Strong    </td> <td align=\"center\">   1       </td> </tr>\n",
    "<tr><td align=\"center\">   13     </td> <td align=\"center\">   Overcast    </td> <td align=\"center\">   Hot            </td> <td align=\"center\">   Normal      </td> <td align=\"center\">   Weak      </td> <td align=\"center\">   1       </td> </tr>\n",
    "<tr><td align=\"center\">   14     </td> <td align=\"center\">   Rain        </td> <td align=\"center\">   Mild           </td> <td align=\"center\">   High        </td> <td align=\"center\">   Strong    </td> <td align=\"center\">   0       </td> </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b54f10",
   "metadata": {},
   "source": [
    "## Simple Python Code to read in Data and perform Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "727954e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from IPython.display import Image \n",
    "from pydot import graph_from_dot_data\n",
    "import os\n",
    "\n",
    "# Where to save the figures and data files\n",
    "PROJECT_ROOT_DIR = \"Results\"\n",
    "FIGURE_ID = \"Results/FigureFiles\"\n",
    "DATA_ID = \"DataFiles/\"\n",
    "\n",
    "if not os.path.exists(PROJECT_ROOT_DIR):\n",
    "    os.mkdir(PROJECT_ROOT_DIR)\n",
    "\n",
    "if not os.path.exists(FIGURE_ID):\n",
    "    os.makedirs(FIGURE_ID)\n",
    "\n",
    "if not os.path.exists(DATA_ID):\n",
    "    os.makedirs(DATA_ID)\n",
    "\n",
    "def image_path(fig_id):\n",
    "    return os.path.join(FIGURE_ID, fig_id)\n",
    "\n",
    "def data_path(dat_id):\n",
    "    return os.path.join(DATA_ID, dat_id)\n",
    "\n",
    "def save_fig(fig_id):\n",
    "    plt.savefig(image_path(fig_id) + \".png\", format='png')\n",
    "\n",
    "infile = open(data_path(\"rideclass.csv\"),'r')\n",
    "\n",
    "# Read the experimental data with Pandas\n",
    "from IPython.display import display\n",
    "ridedata = pd.read_csv(infile,names = ('Outlook','Temperature','Humidity','Wind','Ride'))\n",
    "ridedata = pd.DataFrame(ridedata)\n",
    "\n",
    "# Features and targets\n",
    "X = ridedata.loc[:, ridedata.columns != 'Ride'].values\n",
    "y = ridedata.loc[:, ridedata.columns == 'Ride'].values\n",
    "\n",
    "# Create the encoder.\n",
    "encoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "# Assume for simplicity all features are categorical.\n",
    "encoder.fit(X)    \n",
    "# Apply the encoder.\n",
    "X = encoder.transform(X)\n",
    "print(X)\n",
    "# Then do a Classification tree\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X, y)\n",
    "print(\"Train set accuracy with Decision Tree: {:.2f}\".format(tree_clf.score(X,y)))\n",
    "#transfer to a decision tree graph\n",
    "export_graphviz(\n",
    "    tree_clf,\n",
    "    out_file=\"DataFiles/ride.dot\",\n",
    "    rounded=True,\n",
    "    filled=True\n",
    ")\n",
    "cmd = 'dot -Tpng DataFiles/cancer.dot -o DataFiles/cancer.png'\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1867f9",
   "metadata": {},
   "source": [
    "## Computing the Gini Factor\n",
    "\n",
    "The above functions (gini, entropy and misclassification error) are\n",
    "important components of the so-called CART algorithm. We will discuss\n",
    "this algorithm below after we have discussed the information gain\n",
    "algorithm ID3.\n",
    "\n",
    "In the example here we have converted all our attributes into numerical values $0,1,2$ etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17d5a20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a dataset based on an attribute and an attribute value\n",
    "def test_split(index, value, dataset):\n",
    "\tleft, right = list(), list()\n",
    "\tfor row in dataset:\n",
    "\t\tif row[index] < value:\n",
    "\t\t\tleft.append(row)\n",
    "\t\telse:\n",
    "\t\t\tright.append(row)\n",
    "\treturn left, right\n",
    " \n",
    "# Calculate the Gini index for a split dataset\n",
    "def gini_index(groups, classes):\n",
    "\t# count all samples at split point\n",
    "\tn_instances = float(sum([len(group) for group in groups]))\n",
    "\t# sum weighted Gini index for each group\n",
    "\tgini = 0.0\n",
    "\tfor group in groups:\n",
    "\t\tsize = float(len(group))\n",
    "\t\t# avoid divide by zero\n",
    "\t\tif size == 0:\n",
    "\t\t\tcontinue\n",
    "\t\tscore = 0.0\n",
    "\t\t# score the group based on the score for each class\n",
    "\t\tfor class_val in classes:\n",
    "\t\t\tp = [row[-1] for row in group].count(class_val) / size\n",
    "\t\t\tscore += p * p\n",
    "\t\t# weight the group score by its relative size\n",
    "\t\tgini += (1.0 - score) * (size / n_instances)\n",
    "\treturn gini\n",
    "\n",
    "# Select the best split point for a dataset\n",
    "def get_split(dataset):\n",
    "\tclass_values = list(set(row[-1] for row in dataset))\n",
    "\tb_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "\tfor index in range(len(dataset[0])-1):\n",
    "\t\tfor row in dataset:\n",
    "\t\t\tgroups = test_split(index, row[index], dataset)\n",
    "\t\t\tgini = gini_index(groups, class_values)\n",
    "\t\t\tprint('X%d < %.3f Gini=%.3f' % ((index+1), row[index], gini))\n",
    "\t\t\tif gini < b_score:\n",
    "\t\t\t\tb_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "\treturn {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    " \n",
    "dataset = [[0,0,0,0,0],\n",
    "            [0,0,0,1,1],\n",
    "            [1,0,0,0,1],\n",
    "            [2,1,0,0,1],\n",
    "            [2,2,1,0,1],\n",
    "            [2,2,1,1,0],\n",
    "            [1,2,1,1,1],\n",
    "            [0,1,0,0,0],\n",
    "            [0,2,1,0,1],\n",
    "            [2,1,1,0,1],\n",
    "            [0,1,1,1,1],\n",
    "            [1,1,0,1,1],\n",
    "            [1,0,1,0,1],\n",
    "            [2,1,0,1,0]]\n",
    "\n",
    "split = get_split(dataset)\n",
    "print('Split: [X%d < %.3f]' % ((split['index']+1), split['value']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8400e1fe",
   "metadata": {},
   "source": [
    "## Regression trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f030fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadratic training set + noise\n",
    "np.random.seed(42)\n",
    "m = 200\n",
    "X = np.random.rand(m, 1)\n",
    "y = 4 * (X - 0.5) ** 2\n",
    "y = y + np.random.randn(m, 1) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7de07d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d114a7d",
   "metadata": {},
   "source": [
    "## Final regressor code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cf732d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(random_state=42, max_depth=2)\n",
    "tree_reg2 = DecisionTreeRegressor(random_state=42, max_depth=3)\n",
    "tree_reg1.fit(X, y)\n",
    "tree_reg2.fit(X, y)\n",
    "\n",
    "def plot_regression_predictions(tree_reg, X, y, axes=[0, 1, -0.2, 1], ylabel=\"$y$\"):\n",
    "    x1 = np.linspace(axes[0], axes[1], 500).reshape(-1, 1)\n",
    "    y_pred = tree_reg.predict(x1)\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "    if ylabel:\n",
    "        plt.ylabel(ylabel, fontsize=18, rotation=0)\n",
    "    plt.plot(X, y, \"b.\")\n",
    "    plt.plot(x1, y_pred, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "\n",
    "plt.figure(figsize=(11, 4))\n",
    "plt.subplot(121)\n",
    "plot_regression_predictions(tree_reg1, X, y)\n",
    "for split, style in ((0.1973, \"k-\"), (0.0917, \"k--\"), (0.7718, \"k--\")):\n",
    "    plt.plot([split, split], [-0.2, 1], style, linewidth=2)\n",
    "plt.text(0.21, 0.65, \"Depth=0\", fontsize=15)\n",
    "plt.text(0.01, 0.2, \"Depth=1\", fontsize=13)\n",
    "plt.text(0.65, 0.8, \"Depth=1\", fontsize=13)\n",
    "plt.legend(loc=\"upper center\", fontsize=18)\n",
    "plt.title(\"max_depth=2\", fontsize=14)\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_regression_predictions(tree_reg2, X, y, ylabel=None)\n",
    "for split, style in ((0.1973, \"k-\"), (0.0917, \"k--\"), (0.7718, \"k--\")):\n",
    "    plt.plot([split, split], [-0.2, 1], style, linewidth=2)\n",
    "for split in (0.0458, 0.1298, 0.2873, 0.9040):\n",
    "    plt.plot([split, split], [-0.2, 1], \"k:\", linewidth=1)\n",
    "plt.text(0.3, 0.5, \"Depth=2\", fontsize=13)\n",
    "plt.title(\"max_depth=3\", fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7713257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg1 = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg2 = DecisionTreeRegressor(random_state=42, min_samples_leaf=10)\n",
    "tree_reg1.fit(X, y)\n",
    "tree_reg2.fit(X, y)\n",
    "\n",
    "x1 = np.linspace(0, 1, 500).reshape(-1, 1)\n",
    "y_pred1 = tree_reg1.predict(x1)\n",
    "y_pred2 = tree_reg2.predict(x1)\n",
    "\n",
    "plt.figure(figsize=(11, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(x1, y_pred1, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "plt.axis([0, 1, -0.2, 1.1])\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", fontsize=18, rotation=0)\n",
    "plt.legend(loc=\"upper center\", fontsize=18)\n",
    "plt.title(\"No restrictions\", fontsize=14)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(x1, y_pred2, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "plt.axis([0, 1, -0.2, 1.1])\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.title(\"min_samples_leaf={}\".format(tree_reg2.min_samples_leaf), fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0b8e66",
   "metadata": {},
   "source": [
    "## Pros and cons of trees, pros\n",
    "\n",
    "* White box, easy to interpret model. Some people believe that decision trees more closely mirror human decision-making than do the regression and classification approaches discussed earlier (think of support vector machines)\n",
    "\n",
    "* Trees are very easy to explain to people. In fact, they are even easier to explain than linear regression!\n",
    "\n",
    "* No feature normalization needed\n",
    "\n",
    "* Tree models can handle both continuous and categorical data (Classification and Regression Trees)\n",
    "\n",
    "* Can model nonlinear relationships\n",
    "\n",
    "* Can model interactions between the different descriptive features\n",
    "\n",
    "* Trees can be displayed graphically, and are easily interpreted even by a non-expert (especially if they are small)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73d29d8",
   "metadata": {},
   "source": [
    "## Disadvantages\n",
    "\n",
    "* Unfortunately, trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches\n",
    "\n",
    "* If continuous features are used the tree may become quite large and hence less interpretable\n",
    "\n",
    "* Decision trees are prone to overfit the training data and hence do not well generalize the data if no stopping criteria or improvements like pruning, boosting or bagging are implemented\n",
    "\n",
    "* Small changes in the data may lead to a completely different tree. This issue can be addressed by using ensemble methods like bagging, boosting or random forests\n",
    "\n",
    "* Unbalanced datasets where some target feature values occur much more frequently than others may lead to biased trees since the frequently occurring feature values are preferred over the less frequently occurring ones. \n",
    "\n",
    "* If the number of features is relatively large (high dimensional) and the number of instances is relatively low, the tree might overfit the data\n",
    "\n",
    "* Features with many levels may be preferred over features with less levels since for them it is *more easy* to split the dataset such that the sub datasets only contain pure target feature values. This issue can be addressed by preferring for instance the information gain ratio as splitting criteria over information gain\n",
    "\n",
    "However, by aggregating many decision trees, using methods like\n",
    "bagging, random forests, and boosting, the predictive performance of\n",
    "trees can be substantially improved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86083d18",
   "metadata": {},
   "source": [
    "## Ensemble Methods: From a Single Tree to Many Trees and Extreme Boosting, Meet the Jungle of Methods\n",
    "\n",
    "As stated above and seen in many of the examples discussed here about\n",
    "a single decision tree, we often end up overfitting our training\n",
    "data. This normally means that we have a high variance. Can we reduce\n",
    "the variance of a statistical learning method?\n",
    "\n",
    "This leads us to a set of different methods that can combine different\n",
    "machine learning algorithms or just use one of them to construct\n",
    "forests and jungles of trees, homogeneous ones or heterogenous\n",
    "ones. These methods are recognized by different names which we will\n",
    "try to explain here. These are\n",
    "\n",
    "1. Voting classifiers\n",
    "\n",
    "2. Bagging and Pasting\n",
    "\n",
    "3. Random forests\n",
    "\n",
    "4. Boosting methods, from adaptive to Extreme Gradient Boosting (XGBoost)\n",
    "\n",
    "We discuss these methods here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d972ae",
   "metadata": {},
   "source": [
    "## An Overview of Ensemble Methods\n",
    "\n",
    "<!-- dom:FIGURE: [DataFiles/ensembleoverview.png, width=600 frac=0.8] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"DataFiles/ensembleoverview.png\" width=\"600\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6bf352",
   "metadata": {},
   "source": [
    "## Why Voting?\n",
    "\n",
    "The idea behind boosting, and voting as well can be phrased as follows:\n",
    "**Can a group of people somehow arrive at highly\n",
    "reasoned decisions, despite the weak judgement of the individual\n",
    "members?**\n",
    "\n",
    "The aim is to create a good classifier by combining several weak classifiers.\n",
    "**A weak classifier is a classifier which is able to produce results that are only slightly better than guessing at random.**\n",
    "\n",
    "The basic approach is to apply repeatedly (in boosting this is done in an iterative way) a weak classifier to modifications of the data.\n",
    "In voting we simply apply the law of large numbers while in boosting we give more weight to misclassified data in\n",
    "each iteration.  \n",
    "\n",
    "Decision trees play an important role as our weak classifier. They serve as the basic method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d89cf7",
   "metadata": {},
   "source": [
    "## Tossing coins\n",
    "\n",
    "The simplest case is a so-called voting ensemble. To illustrate this,\n",
    "think of yourself tossing coins with a biased outcome of 51 per cent\n",
    "for heads and 49% for tails. With only few tosses,\n",
    "you may not clearly see this distribution for heads and tails. However, after some\n",
    "thousands of tosses, there will be a clear majority of heads.  With 2000 tosses\n",
    "you should see approximately 1020 heads and 980 tails.\n",
    "\n",
    "We can then state that the outcome is a clear majority of heads. If\n",
    "you do this ten thousand times, it is easy to see that there is a 97%\n",
    "likelihood of a majority of heads.\n",
    "\n",
    "Another example would be to collect all polls before an\n",
    "election. Different polls may show different likelihoods for a\n",
    "candidate winning with say a majority  of the popular vote. The majority vote\n",
    "would then consist in many polls indicating that this candidate will\n",
    "actually win.\n",
    "\n",
    "The example here shows how we can implement the coin tossing case,\n",
    "clealry demostrating that after some tosses we see the [law of large](https://en.wikipedia.org/wiki/Law_of_large_numbers)\n",
    "numbers kicking in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec65ac92",
   "metadata": {},
   "source": [
    "## Standard imports first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6904dbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "from IPython.display import Image \n",
    "from pydot import graph_from_dot_data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from IPython.display import Image \n",
    "from pydot import graph_from_dot_data\n",
    "import os\n",
    "\n",
    "# Where to save the figures and data files\n",
    "PROJECT_ROOT_DIR = \"Results\"\n",
    "FIGURE_ID = \"Results/FigureFiles\"\n",
    "DATA_ID = \"DataFiles/\"\n",
    "\n",
    "if not os.path.exists(PROJECT_ROOT_DIR):\n",
    "    os.mkdir(PROJECT_ROOT_DIR)\n",
    "\n",
    "if not os.path.exists(FIGURE_ID):\n",
    "    os.makedirs(FIGURE_ID)\n",
    "\n",
    "if not os.path.exists(DATA_ID):\n",
    "    os.makedirs(DATA_ID)\n",
    "\n",
    "def image_path(fig_id):\n",
    "    return os.path.join(FIGURE_ID, fig_id)\n",
    "\n",
    "def data_path(dat_id):\n",
    "    return os.path.join(DATA_ID, dat_id)\n",
    "\n",
    "def save_fig(fig_id):\n",
    "    plt.savefig(image_path(fig_id) + \".png\", format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea8eea1",
   "metadata": {},
   "source": [
    "## Simple Voting Example, head or tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed3edd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "heads_proba = 0.51\n",
    "coin_tosses = (np.random.rand(10000, 10) < heads_proba).astype(np.int32)\n",
    "cumulative_heads_ratio = np.cumsum(coin_tosses, axis=0) / np.arange(1, 10001).reshape(-1, 1)\n",
    "plt.figure(figsize=(8,3.5))\n",
    "plt.plot(cumulative_heads_ratio)\n",
    "plt.plot([0, 10000], [0.51, 0.51], \"k--\", linewidth=2, label=\"51%\")\n",
    "plt.plot([0, 10000], [0.5, 0.5], \"k-\", label=\"50%\")\n",
    "plt.xlabel(\"Number of coin tosses\")\n",
    "plt.ylabel(\"Heads ratio\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.axis([0, 10000, 0.42, 0.58])\n",
    "save_fig(\"votingsimple\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913397e8",
   "metadata": {},
   "source": [
    "## Using the Voting Classifier\n",
    "\n",
    "We can use the voting classifier on other data sets, here the exciting binary case of two distinct objects using the make moons functionality of **Scikit-Learn**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d06e933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "svm_clf = SVC(gamma=\"auto\", random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard')\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "svm_clf = SVC(gamma=\"auto\", probability=True, random_state=42)\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='soft')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f392050",
   "metadata": {},
   "source": [
    "## Voting and Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f49c5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression(random_state=42)\n",
    "rnd_clf = RandomForestClassifier(random_state=42)\n",
    "svm_clf = SVC(random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fafb0ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58bfd587",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression(random_state=42)\n",
    "rnd_clf = RandomForestClassifier(random_state=42)\n",
    "svm_clf = SVC(probability=True, random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='soft')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5e2060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc47caa6",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "The **plain** decision trees suffer from high\n",
    "variance. This means that if we split the training data into two parts\n",
    "at random, and fit a decision tree to both halves, the results that we\n",
    "get could be quite different. In contrast, a procedure with low\n",
    "variance will yield similar results if applied repeatedly to distinct\n",
    "data sets; linear regression tends to have low variance, if the ratio\n",
    "of $n$ to $p$ is moderately large. \n",
    "\n",
    "**Bootstrap aggregation**, or just **bagging**, is a\n",
    "general-purpose procedure for reducing the variance of a statistical\n",
    "learning method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7ce88c",
   "metadata": {},
   "source": [
    "## More bagging\n",
    "\n",
    "Bagging typically results in improved accuracy\n",
    "over prediction using a single tree. Unfortunately, however, it can be\n",
    "difficult to interpret the resulting model. Recall that one of the\n",
    "advantages of decision trees is the attractive and easily interpreted\n",
    "diagram that results.\n",
    "\n",
    "However, when we bag a large number of trees, it is no longer\n",
    "possible to represent the resulting statistical learning procedure\n",
    "using a single tree, and it is no longer clear which variables are\n",
    "most important to the procedure. Thus, bagging improves prediction\n",
    "accuracy at the expense of interpretability.  Although the collection\n",
    "of bagged trees is much more difficult to interpret than a single\n",
    "tree, one can obtain an overall summary of the importance of each\n",
    "predictor using the MSE (for bagging regression trees) or the Gini\n",
    "index (for bagging classification trees). In the case of bagging\n",
    "regression trees, we can record the total amount that the MSE is\n",
    "decreased due to splits over a given predictor, averaged over all $B$ possible\n",
    "trees. A large value indicates an important predictor. Similarly, in\n",
    "the context of bagging classification trees, we can add up the total\n",
    "amount that the Gini index  is decreased by splits over a given\n",
    "predictor, averaged over all $B$ trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2bfa3b",
   "metadata": {},
   "source": [
    "## Making your own Bootstrap: Changing the Level of the Decision Tree\n",
    "\n",
    "Let us bring up our good old boostrap example from the linear regression lectures. We change the linerar regression algorithm with\n",
    "a decision tree wth different depths and perform a bootstrap aggregate (in this case we perform as many bootstraps as data points $n$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f5f20a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial degree: 1\n",
      "Error: 0.1077869092797025\n",
      "Bias^2: 0.09665219931376287\n",
      "Var: 0.01113470996593965\n",
      "0.1077869092797025 >= 0.09665219931376287 + 0.01113470996593965 = 0.10778690927970253\n",
      "Polynomial degree: 2\n",
      "Error: 0.07073328352158705\n",
      "Bias^2: 0.05660265596808508\n",
      "Var: 0.01413062755350194\n",
      "0.07073328352158705 >= 0.05660265596808508 + 0.01413062755350194 = 0.07073328352158702\n",
      "Polynomial degree: 3\n",
      "Error: 0.03491402051096134\n",
      "Bias^2: 0.025645913680458925\n",
      "Var: 0.009268106830502446\n",
      "0.03491402051096134 >= 0.025645913680458925 + 0.009268106830502446 = 0.034914020510961374\n",
      "Polynomial degree: 4\n",
      "Error: 0.03122805721680159\n",
      "Bias^2: 0.020459888416421317\n",
      "Var: 0.01076816880038027\n",
      "0.03122805721680159 >= 0.020459888416421317 + 0.01076816880038027 = 0.031228057216801587\n",
      "Polynomial degree: 5\n",
      "Error: 0.029360432630668315\n",
      "Bias^2: 0.01933417827965857\n",
      "Var: 0.010026254351009753\n",
      "0.029360432630668315 >= 0.01933417827965857 + 0.010026254351009753 = 0.029360432630668322\n",
      "Polynomial degree: 6\n",
      "Error: 0.030277866384743817\n",
      "Bias^2: 0.020595346780014352\n",
      "Var: 0.00968251960472946\n",
      "0.030277866384743817 >= 0.020595346780014352 + 0.00968251960472946 = 0.030277866384743814\n",
      "Polynomial degree: 7\n",
      "Error: 0.030892885889348087\n",
      "Bias^2: 0.02014767854486778\n",
      "Var: 0.010745207344480309\n",
      "0.030892885889348087 >= 0.02014767854486778 + 0.010745207344480309 = 0.03089288588934809\n",
      "Polynomial degree: 8\n",
      "Error: 0.03110939965212513\n",
      "Bias^2: 0.020817454714933955\n",
      "Var: 0.010291944937191167\n",
      "0.03110939965212513 >= 0.020817454714933955 + 0.010291944937191167 = 0.03110939965212512\n",
      "Polynomial degree: 9\n",
      "Error: 0.03129578341811742\n",
      "Bias^2: 0.020886306078419455\n",
      "Var: 0.01040947733969797\n",
      "0.03129578341811742 >= 0.020886306078419455 + 0.01040947733969797 = 0.03129578341811742\n",
      "Polynomial degree: 10\n",
      "Error: 0.031606699705781\n",
      "Bias^2: 0.021032809061206647\n",
      "Var: 0.010573890644574355\n",
      "0.031606699705781 >= 0.021032809061206647 + 0.010573890644574355 = 0.031606699705781005\n",
      "Polynomial degree: 11\n",
      "Error: 0.03148677989704399\n",
      "Bias^2: 0.021152336559874207\n",
      "Var: 0.010334443337169787\n",
      "0.03148677989704399 >= 0.021152336559874207 + 0.010334443337169787 = 0.031486779897043994\n",
      "Polynomial degree: 12\n",
      "Error: 0.032116346271443774\n",
      "Bias^2: 0.021860481703725315\n",
      "Var: 0.010255864567718462\n",
      "0.032116346271443774 >= 0.021860481703725315 + 0.010255864567718462 = 0.032116346271443774\n",
      "Polynomial degree: 13\n",
      "Error: 0.03209100097235015\n",
      "Bias^2: 0.02177267731801543\n",
      "Var: 0.01031832365433474\n",
      "0.03209100097235015 >= 0.02177267731801543 + 0.01031832365433474 = 0.03209100097235017\n",
      "Polynomial degree: 14\n",
      "Error: 0.031537537813106746\n",
      "Bias^2: 0.02082938849956035\n",
      "Var: 0.010708149313546408\n",
      "0.031537537813106746 >= 0.02082938849956035 + 0.010708149313546408 = 0.03153753781310675\n",
      "Polynomial degree: 15\n",
      "Error: 0.03145124683797013\n",
      "Bias^2: 0.021111527370668133\n",
      "Var: 0.010339719467301988\n",
      "0.03145124683797013 >= 0.021111527370668133 + 0.010339719467301988 = 0.03145124683797012\n",
      "Polynomial degree: 16\n",
      "Error: 0.03209105344712722\n",
      "Bias^2: 0.021338345379319522\n",
      "Var: 0.010752708067807702\n",
      "0.03209105344712722 >= 0.021338345379319522 + 0.010752708067807702 = 0.032091053447127225\n",
      "Polynomial degree: 17\n",
      "Error: 0.03196914233431665\n",
      "Bias^2: 0.02166629218762961\n",
      "Var: 0.010302850146687045\n",
      "0.03196914233431665 >= 0.02166629218762961 + 0.010302850146687045 = 0.03196914233431666\n",
      "Polynomial degree: 18\n",
      "Error: 0.03165248155309402\n",
      "Bias^2: 0.02132799291304245\n",
      "Var: 0.010324488640051558\n",
      "0.03165248155309402 >= 0.02132799291304245 + 0.010324488640051558 = 0.03165248155309401\n",
      "Polynomial degree: 19\n",
      "Error: 0.03235209768543048\n",
      "Bias^2: 0.022074472952423024\n",
      "Var: 0.010277624733007463\n",
      "0.03235209768543048 >= 0.022074472952423024 + 0.010277624733007463 = 0.032352097685430486\n",
      "Polynomial degree: 20\n",
      "Error: 0.03174154546980151\n",
      "Bias^2: 0.021139865993692355\n",
      "Var: 0.010601679476109165\n",
      "0.03174154546980151 >= 0.021139865993692355 + 0.010601679476109165 = 0.03174154546980152\n",
      "Polynomial degree: 21\n",
      "Error: 0.03249366272788966\n",
      "Bias^2: 0.022392964161879643\n",
      "Var: 0.010100698566010037\n",
      "0.03249366272788966 >= 0.022392964161879643 + 0.010100698566010037 = 0.03249366272788968\n",
      "Polynomial degree: 22\n",
      "Error: 0.03100387126675639\n",
      "Bias^2: 0.02122260145527001\n",
      "Var: 0.009781269811486361\n",
      "0.03100387126675639 >= 0.02122260145527001 + 0.009781269811486361 = 0.03100387126675637\n",
      "Polynomial degree: 23\n",
      "Error: 0.0320452032176362\n",
      "Bias^2: 0.021849700000696155\n",
      "Var: 0.010195503216940061\n",
      "0.0320452032176362 >= 0.021849700000696155 + 0.010195503216940061 = 0.032045203217636216\n",
      "Polynomial degree: 24\n",
      "Error: 0.0315431807559056\n",
      "Bias^2: 0.021311765078499363\n",
      "Var: 0.01023141567740624\n",
      "0.0315431807559056 >= 0.021311765078499363 + 0.01023141567740624 = 0.031543180755905606\n",
      "Polynomial degree: 25\n",
      "Error: 0.03167739763326645\n",
      "Bias^2: 0.020906889239215253\n",
      "Var: 0.010770508394051193\n",
      "0.03167739763326645 >= 0.020906889239215253 + 0.010770508394051193 = 0.03167739763326645\n",
      "Polynomial degree: 26\n",
      "Error: 0.03091217303915449\n",
      "Bias^2: 0.021114234118115666\n",
      "Var: 0.009797938921038828\n",
      "0.03091217303915449 >= 0.021114234118115666 + 0.009797938921038828 = 0.030912173039154493\n",
      "Polynomial degree: 27\n",
      "Error: 0.03181795943603612\n",
      "Bias^2: 0.0217612448640803\n",
      "Var: 0.01005671457195582\n",
      "0.03181795943603612 >= 0.0217612448640803 + 0.01005671457195582 = 0.03181795943603612\n",
      "Polynomial degree: 28\n",
      "Error: 0.031719847034916156\n",
      "Bias^2: 0.021338073424295095\n",
      "Var: 0.01038177361062107\n",
      "0.031719847034916156 >= 0.021338073424295095 + 0.01038177361062107 = 0.03171984703491616\n",
      "Polynomial degree: 29\n",
      "Error: 0.03138781693308297\n",
      "Bias^2: 0.021135473862578174\n",
      "Var: 0.010252343070504813\n",
      "0.03138781693308297 >= 0.021135473862578174 + 0.010252343070504813 = 0.031387816933082985\n",
      "Simple tree: 0.6586458006727302\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.utils import resample\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "n = 100\n",
    "n_boostraps = 100\n",
    "maxdepth = 30\n",
    "\n",
    "# Make data set.\n",
    "x = np.linspace(-3, 3, n).reshape(-1, 1)\n",
    "y = np.exp(-x**2) + 1.5 * np.exp(-(x-2)**2)+ np.random.normal(0, 0.1, x.shape)\n",
    "error = np.zeros(maxdepth)\n",
    "bias = np.zeros(maxdepth)\n",
    "variance = np.zeros(maxdepth)\n",
    "polydegree = np.zeros(maxdepth)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# we produce a simple tree first as benchmark\n",
    "simpletree = DecisionTreeRegressor(max_depth=3) \n",
    "simpletree.fit(X_train_scaled, y_train)\n",
    "simpleprediction = simpletree.predict(X_test_scaled)\n",
    "for degree in range(1,maxdepth):\n",
    "    model = DecisionTreeRegressor(max_depth=degree) \n",
    "    y_pred = np.empty((y_test.shape[0], n_boostraps))\n",
    "    for i in range(n_boostraps):\n",
    "        x_, y_ = resample(X_train_scaled, y_train)\n",
    "        model.fit(x_, y_)\n",
    "        y_pred[:, i] = model.predict(X_test_scaled)#.ravel()\n",
    "\n",
    "    polydegree[degree] = degree\n",
    "    error[degree] = np.mean( np.mean((y_test - y_pred)**2, axis=1, keepdims=True) )\n",
    "    bias[degree] = np.mean( (y_test - np.mean(y_pred, axis=1, keepdims=True))**2 )\n",
    "    variance[degree] = np.mean( np.var(y_pred, axis=1, keepdims=True) )\n",
    "    print('Polynomial degree:', degree)\n",
    "    print('Error:', error[degree])\n",
    "    print('Bias^2:', bias[degree])\n",
    "    print('Var:', variance[degree])\n",
    "    print('{} >= {} + {} = {}'.format(error[degree], bias[degree], variance[degree], bias[degree]+variance[degree]))\n",
    " \n",
    "mse_simpletree= np.mean( np.mean((y_test - simpleprediction)**2))\n",
    "print(\"Simple tree:\",mse_simpletree)\n",
    "plt.xlim(1,maxdepth)\n",
    "plt.plot(polydegree, error, label='MSE')\n",
    "plt.plot(polydegree, bias, label='bias')\n",
    "plt.plot(polydegree, variance, label='Variance')\n",
    "plt.legend()\n",
    "save_fig(\"baggingboot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f3b548",
   "metadata": {},
   "source": [
    "## Random forests\n",
    "\n",
    "Random forests provide an improvement over bagged trees by way of a\n",
    "small tweak that decorrelates the trees. \n",
    "\n",
    "As in bagging, we build a\n",
    "number of decision trees on bootstrapped training samples. But when\n",
    "building these decision trees, each time a split in a tree is\n",
    "considered, a random sample of $m$ predictors is chosen as split\n",
    "candidates from the full set of $p$ predictors. The split is allowed to\n",
    "use only one of those $m$ predictors. \n",
    "\n",
    "A fresh sample of $m$ predictors is\n",
    "taken at each split, and typically we choose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51ddbb5",
   "metadata": {},
   "source": [
    "$$\n",
    "m\\approx \\sqrt{p}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5307d349",
   "metadata": {},
   "source": [
    "In building a random forest, at\n",
    "each split in the tree, the algorithm is not even allowed to consider\n",
    "a majority of the available predictors. \n",
    "\n",
    "The reason for this is rather clever. Suppose that there is one very\n",
    "strong predictor in the data set, along with a number of other\n",
    "moderately strong predictors. Then in the collection of bagged\n",
    "variable importance random forest trees, most or all of the trees will\n",
    "use this strong predictor in the top split. Consequently, all of the\n",
    "bagged trees will look quite similar to each other. Hence the\n",
    "predictions from the bagged trees will be highly correlated.\n",
    "Unfortunately, averaging many highly correlated quantities does not\n",
    "lead to as large of a reduction in variance as averaging many\n",
    "uncorrelated quantities. In particular, this means that bagging will\n",
    "not lead to a substantial reduction in variance over a single tree in\n",
    "this setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5df6740",
   "metadata": {},
   "source": [
    "## Random Forest Algorithm\n",
    "The algorithm described here can be applied to both classification and regression problems.\n",
    "\n",
    "We will grow of forest of say $B$ trees.\n",
    "1. For $b=1:B$\n",
    "\n",
    "  * Draw a bootstrap sample from the training data organized in our $\\boldsymbol{X}$ matrix.\n",
    "\n",
    "  * We grow then a random forest tree $T_b$ based on the bootstrapped data by repeating the steps outlined till we reach the maximum node size is reached\n",
    "\n",
    "1. we select $m \\le p$ variables at random from the $p$ predictors/features\n",
    "\n",
    "2. pick the best split point among the $m$ features using for example the CART algorithm and create a new node\n",
    "\n",
    "3. split the node into daughter nodes\n",
    "\n",
    "4. Output then the ensemble of trees $\\{T_b\\}_1^{B}$ and make predictions for either a regression type of problem or a classification type of problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f7886c",
   "metadata": {},
   "source": [
    "## Random Forests Compared with other Methods on the Cancer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e31e8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import  train_test_split \n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Load the data\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data,cancer.target,random_state=0)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "#define methods\n",
    "# Logistic Regression\n",
    "logreg = LogisticRegression(solver='lbfgs')\n",
    "# Support vector machine\n",
    "svm = SVC(gamma='auto', C=100)\n",
    "# Decision Trees\n",
    "deep_tree_clf = DecisionTreeClassifier(max_depth=None)\n",
    "#Scale the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "# Logistic Regression\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "print(\"Test set accuracy Logistic Regression with scaled data: {:.2f}\".format(logreg.score(X_test_scaled,y_test)))\n",
    "# Support Vector Machine\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "print(\"Test set accuracy SVM with scaled data: {:.2f}\".format(logreg.score(X_test_scaled,y_test)))\n",
    "# Decision Trees\n",
    "deep_tree_clf.fit(X_train_scaled, y_train)\n",
    "print(\"Test set accuracy with Decision Trees and scaled data: {:.2f}\".format(deep_tree_clf.score(X_test_scaled,y_test)))\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_validate\n",
    "# Data set not specificied\n",
    "#Instantiate the model with 500 trees and entropy as splitting criteria\n",
    "Random_Forest_model = RandomForestClassifier(n_estimators=500,criterion=\"entropy\")\n",
    "Random_Forest_model.fit(X_train_scaled, y_train)\n",
    "#Cross validation\n",
    "accuracy = cross_validate(Random_Forest_model,X_test_scaled,y_test,cv=10)['test_score']\n",
    "print(accuracy)\n",
    "print(\"Test set accuracy with Random Forests and scaled data: {:.2f}\".format(Random_Forest_model.score(X_test_scaled,y_test)))\n",
    "\n",
    "\n",
    "import scikitplot as skplt\n",
    "y_pred = Random_Forest_model.predict(X_test_scaled)\n",
    "skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize=True)\n",
    "plt.show()\n",
    "y_probas = Random_Forest_model.predict_proba(X_test_scaled)\n",
    "skplt.metrics.plot_roc(y_test, y_probas)\n",
    "plt.show()\n",
    "skplt.metrics.plot_cumulative_gain(y_test, y_probas)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de40f94",
   "metadata": {},
   "source": [
    "Recall that the cumulative gains curve shows the percentage of the\n",
    "overall number of cases in a given category *gained* by targeting a\n",
    "percentage of the total number of cases.\n",
    "\n",
    "Similarly, the receiver operating characteristic curve, or ROC curve,\n",
    "displays the diagnostic ability of a binary classifier system as its\n",
    "discrimination threshold is varied. It plots the true positive rate against the false positive rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bff02d3",
   "metadata": {},
   "source": [
    "## Compare  Bagging on Trees with Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23f8734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16, random_state=42),\n",
    "    n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59bd532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=42)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "np.sum(y_pred == y_pred_rf) / len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24309a7",
   "metadata": {},
   "source": [
    "## Boosting, a Bird's Eye View\n",
    "\n",
    "The basic idea is to combine weak classifiers in order to create a good\n",
    "classifier. With a weak classifier we often intend a classifier which\n",
    "produces results which are only slightly better than we would get by\n",
    "random guesses.\n",
    "\n",
    "This is done by applying in an iterative way a weak (or a standard\n",
    "classifier like decision trees) to modify the data. In each iteration\n",
    "we emphasize those observations which are misclassified by weighting\n",
    "them with a factor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f84b7a8",
   "metadata": {},
   "source": [
    "## What is boosting? Additive Modelling/Iterative Fitting\n",
    "\n",
    "Boosting is a way of fitting an additive expansion in a set of\n",
    "elementary basis functions like for example some simple polynomials.\n",
    "Assume for example that we have a function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8144faa",
   "metadata": {},
   "source": [
    "$$\n",
    "f_M(x) = \\sum_{i=1}^M \\beta_m b(x;\\gamma_m),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f523032",
   "metadata": {},
   "source": [
    "where $\\beta_m$ are the expansion parameters to be determined in a\n",
    "minimization process and $b(x;\\gamma_m)$ are some simple functions of\n",
    "the multivariable parameter $x$ which is characterized by the\n",
    "parameters $\\gamma_m$.\n",
    "\n",
    "As an example, consider the Sigmoid function we used in logistic\n",
    "regression. In that case, we can translate the function\n",
    "$b(x;\\gamma_m)$ into the Sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d77841",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma(t) = \\frac{1}{1+\\exp{(-t)}},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e086177",
   "metadata": {},
   "source": [
    "where $t=\\gamma_0+\\gamma_1 x$ and the parameters $\\gamma_0$ and\n",
    "$\\gamma_1$ were determined by the Logistic Regression fitting\n",
    "algorithm.\n",
    "\n",
    "As another example, consider the cost function we defined for linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ad95dc",
   "metadata": {},
   "source": [
    "$$\n",
    "C(\\boldsymbol{y},\\boldsymbol{f}) = \\frac{1}{n} \\sum_{i=0}^{n-1}(y_i-f(x_i))^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6287183a",
   "metadata": {},
   "source": [
    "In this case the function $f(x)$ was replaced by the design matrix\n",
    "$\\boldsymbol{X}$ and the unknown linear regression parameters $\\boldsymbol{\\beta}$,\n",
    "that is $\\boldsymbol{f}=\\boldsymbol{X}\\boldsymbol{\\beta}$. In linear regression we can \n",
    "simply invert a matrix and obtain the parameters $\\beta$ by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27de8464",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{\\beta}=\\left(\\boldsymbol{X}^T\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8e41cb",
   "metadata": {},
   "source": [
    "In iterative fitting or additive modeling, we minimize the cost function with respect to the parameters $\\beta_m$ and $\\gamma_m$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45f6f2a",
   "metadata": {},
   "source": [
    "## Iterative Fitting, Regression and Squared-error Cost Function\n",
    "\n",
    "The way we proceed is as follows (here we specialize to the squared-error cost function)\n",
    "\n",
    "1. Establish a cost function, here ${\\cal C}(\\boldsymbol{y},\\boldsymbol{f}) = \\frac{1}{n} \\sum_{i=0}^{n-1}(y_i-f_M(x_i))^2$ with $f_M(x) = \\sum_{i=1}^M \\beta_m b(x;\\gamma_m)$.\n",
    "\n",
    "2. Initialize with a guess $f_0(x)$. It could be one or even zero or some random numbers.\n",
    "\n",
    "3. For $m=1:M$\n",
    "\n",
    "a. minimize $\\sum_{i=0}^{n-1}(y_i-f_{m-1}(x_i)-\\beta b(x;\\gamma))^2$ wrt $\\gamma$ and $\\beta$\n",
    "\n",
    "b. This gives the optimal values $\\beta_m$ and $\\gamma_m$\n",
    "\n",
    "c. Determine then the new values $f_m(x)=f_{m-1}(x) +\\beta_m b(x;\\gamma_m)$\n",
    "\n",
    "We could use any of the algorithms we have discussed till now. If we\n",
    "use trees, $\\gamma$ parameterizes the split variables and split points\n",
    "at the internal nodes, and the predictions at the terminal nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af260098",
   "metadata": {},
   "source": [
    "## Squared-Error Example and Iterative Fitting\n",
    "\n",
    "To better understand what happens, let us develop the steps for the iterative fitting using the above squared error function.\n",
    "\n",
    "For simplicity we assume also that our functions $b(x;\\gamma)=1+\\gamma x$. \n",
    "\n",
    "This means that for every iteration $m$, we need to optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a85e34",
   "metadata": {},
   "source": [
    "$$\n",
    "(\\beta_m,\\gamma_m) = \\mathrm{argmin}_{\\beta,\\lambda}\\hspace{0.1cm} \\sum_{i=0}^{n-1}(y_i-f_{m-1}(x_i)-\\beta b(x;\\gamma))^2=\\sum_{i=0}^{n-1}(y_i-f_{m-1}(x_i)-\\beta(1+\\gamma x_i))^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a809e54d",
   "metadata": {},
   "source": [
    "We start our iteration by simply setting $f_0(x)=0$. \n",
    "Taking the derivatives  with respect to $\\beta$ and $\\gamma$ we obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b8fb9c",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial {\\cal C}}{\\partial \\beta} = -2\\sum_{i}(1+\\gamma x_i)(y_i-\\beta(1+\\gamma x_i))=0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40d3542",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b388a2",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial {\\cal C}}{\\partial \\gamma} =-2\\sum_{i}\\beta x_i(y_i-\\beta(1+\\gamma x_i))=0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5986c5",
   "metadata": {},
   "source": [
    "We can then rewrite these equations as (defining $\\boldsymbol{w}=\\boldsymbol{e}+\\gamma \\boldsymbol{x})$ with $\\boldsymbol{e}$ being the unit vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605abac6",
   "metadata": {},
   "source": [
    "$$\n",
    "\\gamma \\boldsymbol{w}^T(\\boldsymbol{y}-\\beta\\gamma \\boldsymbol{w})=0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67ecd40",
   "metadata": {},
   "source": [
    "which gives us $\\beta = \\boldsymbol{w}^T\\boldsymbol{y}/(\\boldsymbol{w}^T\\boldsymbol{w})$. Similarly we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85880a97",
   "metadata": {},
   "source": [
    "$$\n",
    "\\beta\\gamma \\boldsymbol{x}^T(\\boldsymbol{y}-\\beta(1+\\gamma \\boldsymbol{x}))=0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e801e0c6",
   "metadata": {},
   "source": [
    "which leads to $\\gamma =(\\boldsymbol{x}^T\\boldsymbol{y}-\\beta\\boldsymbol{x}^T\\boldsymbol{e})/(\\beta\\boldsymbol{x}^T\\boldsymbol{x})$.  Inserting\n",
    "for $\\beta$ gives us an equation for $\\gamma$. This is a non-linear equation in the unknown $\\gamma$ and has to be solved numerically. \n",
    "\n",
    "The solution to these two equations gives us in turn $\\beta_1$ and $\\gamma_1$ leading to the new expression for $f_1(x)$ as\n",
    "$f_1(x) = \\beta_1(1+\\gamma_1x)$. Doing this $M$ times results in our final estimate for the function $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e0fc0e",
   "metadata": {},
   "source": [
    "## Iterative Fitting, Classification and AdaBoost\n",
    "\n",
    "Let us consider a binary classification problem with two outcomes $y_i \\in \\{-1,1\\}$ and $i=0,1,2,\\dots,n-1$ as our set of\n",
    "observations. We define a classification function $G(x)$ which produces a prediction taking one or the other of the two values \n",
    "$\\{-1,1\\}$.\n",
    "\n",
    "The error rate of the training sample is then"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c5a166",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{\\overline{err}}=\\frac{1}{n} \\sum_{i=0}^{n-1} I(y_i\\ne G(x_i)).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4476c4c7",
   "metadata": {},
   "source": [
    "The iterative procedure starts with defining a weak classifier whose\n",
    "error rate is barely better than random guessing.  The iterative\n",
    "procedure in boosting is to sequentially apply a  weak\n",
    "classification algorithm to repeatedly modified versions of the data\n",
    "producing a sequence of weak classifiers $G_m(x)$.\n",
    "\n",
    "Here we will express our  function $f(x)$ in terms of $G(x)$. That is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b35c8d2",
   "metadata": {},
   "source": [
    "$$\n",
    "f_M(x) = \\sum_{i=1}^M \\beta_m b(x;\\gamma_m),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a533e45a",
   "metadata": {},
   "source": [
    "will be a function of"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e33a99",
   "metadata": {},
   "source": [
    "$$\n",
    "G_M(x) = \\mathrm{sign} \\sum_{i=1}^M \\alpha_m G_m(x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4199e7a8",
   "metadata": {},
   "source": [
    "## Adaptive Boosting, AdaBoost\n",
    "\n",
    "In our iterative procedure we define thus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2fda6f",
   "metadata": {},
   "source": [
    "$$\n",
    "f_m(x) = f_{m-1}(x)+\\beta_mG_m(x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb7d266",
   "metadata": {},
   "source": [
    "The simplest possible cost function which leads (also simple from a computational point of view) to the AdaBoost algorithm is the\n",
    "exponential cost/loss function defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70431c63",
   "metadata": {},
   "source": [
    "$$\n",
    "C(\\boldsymbol{y},\\boldsymbol{f}) = \\sum_{i=0}^{n-1}\\exp{(-y_i(f_{m-1}(x_i)+\\beta G(x_i))}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e678e2c",
   "metadata": {},
   "source": [
    "We optimize $\\beta$ and $G$ for each value of $m=1:M$ as we did in the regression case.\n",
    "This is normally done in two steps. Let us however first rewrite the cost function as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2957ac5",
   "metadata": {},
   "source": [
    "$$\n",
    "C(\\boldsymbol{y},\\boldsymbol{f}) = \\sum_{i=0}^{n-1}w_i^{m}\\exp{(-y_i\\beta G(x_i))},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abebd5ea",
   "metadata": {},
   "source": [
    "where we have defined $w_i^m= \\exp{(-y_if_{m-1}(x_i))}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7d7c9f",
   "metadata": {},
   "source": [
    "## Building up AdaBoost\n",
    "\n",
    "First, for any $\\beta > 0$, we optimize $G$ by setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdaa6db",
   "metadata": {},
   "source": [
    "$$\n",
    "G_m(x) = \\mathrm{sign} \\sum_{i=0}^{n-1} w_i^m I(y_i \\ne G_(x_i)),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cbd741",
   "metadata": {},
   "source": [
    "which is the classifier that minimizes the weighted error rate in predicting $y$.\n",
    "\n",
    "We can do this by rewriting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8310198c",
   "metadata": {},
   "source": [
    "$$\n",
    "\\exp{-(\\beta)}\\sum_{y_i=G(x_i)}w_i^m+\\exp{(\\beta)}\\sum_{y_i\\ne G(x_i)}w_i^m,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37fe48c",
   "metadata": {},
   "source": [
    "which can be rewritten as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e909739",
   "metadata": {},
   "source": [
    "$$\n",
    "(\\exp{(\\beta)}-\\exp{-(\\beta)})\\sum_{i=0}^{n-1}w_i^mI(y_i\\ne G(x_i))+\\exp{(-\\beta)}\\sum_{i=0}^{n-1}w_i^m=0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ad59f0",
   "metadata": {},
   "source": [
    "which leads to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2d48e0",
   "metadata": {},
   "source": [
    "$$\n",
    "\\beta_m = \\frac{1}{2}\\log{\\frac{1-\\mathrm{\\overline{err}}}{\\mathrm{\\overline{err}}}},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1dcb00",
   "metadata": {},
   "source": [
    "where we have redefined the error as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c824f3",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{\\overline{err}}_m=\\frac{1}{n}\\frac{\\sum_{i=0}^{n-1}w_i^mI(y_i\\ne G(x_i)}{\\sum_{i=0}^{n-1}w_i^m},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c916fad2",
   "metadata": {},
   "source": [
    "which leads to an update of"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070d7540",
   "metadata": {},
   "source": [
    "$$\n",
    "f_m(x) = f_{m-1}(x) +\\beta_m G_m(x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1b22a8",
   "metadata": {},
   "source": [
    "This leads to the new weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2bf04a",
   "metadata": {},
   "source": [
    "$$\n",
    "w_i^{m+1} = w_i^m \\exp{(-y_i\\beta_m G_m(x_i))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea775ce7",
   "metadata": {},
   "source": [
    "## Adaptive boosting: AdaBoost, Basic Algorithm\n",
    "\n",
    "The algorithm here is rather straightforward. Assume that our weak\n",
    "classifier is a decision tree and we consider a binary set of outputs\n",
    "with $y_i \\in \\{-1,1\\}$ and $i=0,1,2,\\dots,n-1$ as our set of\n",
    "observations. Our design matrix is given in terms of the\n",
    "feature/predictor vectors\n",
    "$\\boldsymbol{X}=[\\boldsymbol{x}_0\\boldsymbol{x}_1\\dots\\boldsymbol{x}_{p-1}]$. Finally, we define also a\n",
    "classifier determined by our data via a function $G(x)$. This function tells us how well we are able to classify our outputs/targets $\\boldsymbol{y}$. \n",
    "\n",
    "We have already defined the misclassification error $\\mathrm{err}$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae10c0a4",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{err}=\\frac{1}{n}\\sum_{i=0}^{n-1}I(y_i\\ne G(x_i)),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe87ab2",
   "metadata": {},
   "source": [
    "where the function $I()$ is one if we misclassify and zero if we classify correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875649dd",
   "metadata": {},
   "source": [
    "## Basic Steps of AdaBoost\n",
    "\n",
    "With the above definitions we are now ready to set up the algorithm for AdaBoost.\n",
    "The basic idea is to set up weights which will be used to scale the correctly classified and the misclassified cases.\n",
    "1. We start by initializing all weights to $w_i = 1/n$, with $i=0,1,2,\\dots n-1$. It is easy to see that we must have $\\sum_{i=0}^{n-1}w_i = 1$.\n",
    "\n",
    "2. We rewrite the misclassification error as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aac54e6",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{\\overline{err}}_m=\\frac{\\sum_{i=0}^{n-1}w_i^m I(y_i\\ne G(x_i))}{\\sum_{i=0}^{n-1}w_i},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e84a60",
   "metadata": {},
   "source": [
    "1. Then we start looping over all attempts at classifying, namely we start an iterative process for $m=1:M$, where $M$ is the final number of classifications. Our given classifier could for example be a plain decision tree.\n",
    "\n",
    "a. Fit then a given classifier to the training set using the weights $w_i$.\n",
    "\n",
    "b. Compute then $\\mathrm{err}$ and figure out which events are classified properly and which are classified wrongly.\n",
    "\n",
    "c. Define a quantity $\\alpha_{m} = \\log{(1-\\mathrm{\\overline{err}}_m)/\\mathrm{\\overline{err}}_m}$\n",
    "\n",
    "d. Set the new weights to $w_i = w_i\\times \\exp{(\\alpha_m I(y_i\\ne G(x_i)}$.\n",
    "\n",
    "5. Compute the new classifier $G(x)= \\sum_{i=0}^{n-1}\\alpha_m I(y_i\\ne G(x_i)$.\n",
    "\n",
    "For the iterations with $m \\le 2$ the weights are modified\n",
    "individually at each steps. The observations which were misclassified\n",
    "at iteration $m-1$ have a weight which is larger than those which were\n",
    "classified properly. As this proceeds, the observations which were\n",
    "difficult to classifiy correctly are given a larger influence. Each\n",
    "new classification step $m$ is then forced to concentrate on those\n",
    "observations that are missed in the previous iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80edac5",
   "metadata": {},
   "source": [
    "## AdaBoost Examples\n",
    "\n",
    "Using **Scikit-Learn** it is easy to apply the adaptive boosting algorithm, as done here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04a2ab5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5, random_state=42)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5, random_state=42)\n",
    "ada_clf.fit(X_train_scaled, y_train)\n",
    "y_pred = ada_clf.predict(X_test_scaled)\n",
    "skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize=True)\n",
    "plt.show()\n",
    "y_probas = ada_clf.predict_proba(X_test_scaled)\n",
    "skplt.metrics.plot_roc(y_test, y_probas)\n",
    "plt.show()\n",
    "skplt.metrics.plot_cumulative_gain(y_test, y_probas)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

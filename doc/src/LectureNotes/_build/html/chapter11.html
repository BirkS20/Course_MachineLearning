

<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>11. Data Analysis and Machine Learning: &#8212; Applied Machine Learning and Data Analysis</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="9. Recurrent Neural Networks" href="chapter10.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  <img src="_static/Picture.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Applied Machine Learning and Data Analysis</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="chapter1.html">
   Introduction to Applied Data Analysis and Machine Learning
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Supervised Learning
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="chapter2.html">
   1. Elements of Probability Theory and Statistical Data Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter3.html">
   2. Getting started, our first data and Machine Learning encounters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter4.html">
   3. Linear Regression and more Advanced Regression Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter5.html">
   4. Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter6.html">
   5. Neural networks, from the simple perceptron to deep learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter7.html">
   6. Support Vector Machines, overarching aims
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter8.html">
   7. Dimensionality Reduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter9.html">
   8. Convolutional Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter10.html">
   9. Recurrent Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter10.html#solving-odes-with-deep-learning">
   10. Solving ODEs with Deep Learning
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   11. Data Analysis and Machine Learning:
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="#elements-of-bayesian-theory-and-bayesian-neural-networks">
   12. Elements of Bayesian theory and Bayesian Neural Networks
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/chapter11.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            
        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <!-- dom:TITLE: Data Analysis and Machine Learning:  -->
<div class="section" id="data-analysis-and-machine-learning">
<h1><span class="section-number">11. </span>Data Analysis and Machine Learning:<a class="headerlink" href="#data-analysis-and-machine-learning" title="Permalink to this headline">¶</a></h1>
<!-- dom:AUTHOR: Christian Forssén at Department of Physics, Chalmers University of Technology, Sweden -->
<!-- Author: -->  
<p><strong>Christian Forssén</strong>, Department of Physics, Chalmers University of Technology, Sweden</p>
<!-- dom:AUTHOR: Morten Hjorth-Jensen at Department of Physics, University of Oslo & Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University -->
<!-- Author: --> **Morten Hjorth-Jensen**, Department of Physics, University of Oslo and Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University
<p>Date: <strong>Dec 23, 2020</strong></p>
<p>Copyright 1999-2020, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license</p>
</div>
<div class="section" id="elements-of-bayesian-theory-and-bayesian-neural-networks">
<h1><span class="section-number">12. </span>Elements of Bayesian theory and Bayesian Neural Networks<a class="headerlink" href="#elements-of-bayesian-theory-and-bayesian-neural-networks" title="Permalink to this headline">¶</a></h1>
<div class="section" id="why-bayesian-statistics">
<h2><span class="section-number">12.1. </span>Why Bayesian Statistics?<a class="headerlink" href="#why-bayesian-statistics" title="Permalink to this headline">¶</a></h2>
<p>We have already made ourselves familiar with elements of a statistical
data analysis via quantities like the bias-variance tradeoff as well
as some central distribution functions such as the Normal
distribution, the binomial distribution and other probability
distribution functions.</p>
<p>In essentially all the Machine Learning algorithms we have studied,
our focus has been on a so-called <strong>frequentist approach</strong>, where
knowledge of an underlying likelihood function has not been
emphasized. Our data, whether we had a classification or a regression
problem, have been our central points of departure.</p>
<p>Here we wish to merge this approach with the derivation of a
likelihood function which can be used to make prediction on how our
system under study evolves.  We will venture into the realm of what is
called Bayesian Neural Networks. To get an overarching view on what
this entails, the following figure conveys the essential differences
between a standard Neural network that we have met earlier and a
Bayesian Neural Network. In order to get there, we need to present
some of the basic elements of Bayesian statistics, starting with the
product rule and Bayes’ theorem.</p>
</div>
<div class="section" id="inference">
<h2><span class="section-number">12.2. </span>Inference<a class="headerlink" href="#inference" title="Permalink to this headline">¶</a></h2>
<p>Inference:
:<br />
“the act of passing from one proposition, statement or judgment considered as true to another whose truth is believed to follow from that of the former” (Webster)
Do premises <span class="math notranslate nohighlight">\(A, B, \ldots \to\)</span> hypothesis, <span class="math notranslate nohighlight">\(H\)</span>?</p>
<p>Deductive inference:
:<br />
Premises allow definite determination of truth/falsity of H (syllogisms, symbolic logic, Boolean algebra)
<span class="math notranslate nohighlight">\(B(H|A,B,...) = 0\)</span> or <span class="math notranslate nohighlight">\(1\)</span></p>
<p>Inductive inference:
:<br />
Premises bear on truth/falsity of H, but don’t allow its definite determination (weak syllogisms, analogies)
<span class="math notranslate nohighlight">\(A, B, C, D\)</span> share properties <span class="math notranslate nohighlight">\(x, y, z\)</span>; <span class="math notranslate nohighlight">\(E\)</span> has properties <span class="math notranslate nohighlight">\(x, y\)</span>
<span class="math notranslate nohighlight">\(\to\)</span> <span class="math notranslate nohighlight">\(E\)</span> probably has property <span class="math notranslate nohighlight">\(z\)</span>.</p>
</div>
<div class="section" id="statistical-inference">
<h2><span class="section-number">12.3. </span>Statistical Inference<a class="headerlink" href="#statistical-inference" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Quantify the strength of inductive inferences from facts, in the form of data (<span class="math notranslate nohighlight">\(D\)</span>), and other premises, e.g. models, to hypotheses about the phenomena producing the data.</p></li>
<li><p>Quantify via probabilities, or averages calculated using probabilities. Frequentists (<span class="math notranslate nohighlight">\(\mathcal{F}\)</span>) and Bayesians (<span class="math notranslate nohighlight">\(\mathcal{B}\)</span>) use probabilities very differently for this.</p></li>
<li><p>To the pioneers such as Bernoulli, Bayes and Laplace, a probability represented a <em>degree-of-belief</em> or plausability: how much they thought that something as true based on the evidence at hand. This is the Bayesian approach.</p></li>
<li><p>To the 19th century scholars, this seemed too vague and subjective. They redefined probability as the <em>long run relative frequency</em> with which an event occurred, given (infinitely) many repeated (experimental) trials.</p></li>
</ul>
</div>
<div class="section" id="some-history">
<h2><span class="section-number">12.4. </span>Some history<a class="headerlink" href="#some-history" title="Permalink to this headline">¶</a></h2>
<p>Adapted from D.S. Sivia<a class="footnote-reference brackets" href="#sivia" id="id1">1</a>:</p>
<blockquote>
<div><p>Although the frequency definition appears to be more objective, its range of validity is also far more limited. For example, Laplace used (his) probability theory to estimate the mass of Saturn, given orbital data that were available to him from various astronomical observatories. In essence, he computed the posterior pdf for the mass M , given the data and all the relevant background information I (such as a knowledge of the laws of classical mechanics): prob(M|{data},I); this is shown schematically in the figure [Fig. 1.2].</p>
</div></blockquote>
<!-- dom:FIGURE: [fig/sivia_fig_1_2.png, width=700 frac=0.9] -->
<!-- begin figure -->
<p></p>
<img src="fig/sivia_fig_1_2.png" width=700>
<!-- end figure -->
<blockquote>
<div><p>To Laplace, the (shaded) area under the posterior pdf curve between <span class="math notranslate nohighlight">\(m_1\)</span> and <span class="math notranslate nohighlight">\(m_2\)</span> was a measure of how much he believed that the mass of Saturn lay in the range <span class="math notranslate nohighlight">\(m_1 \le M \le m_2\)</span>. As such, the position of the maximum of the posterior pdf represents a best estimate of the mass; its width, or spread, about this optimal value gives an indication of the uncertainty in the estimate. Laplace stated that: ‘ … it is a bet of 11,000 to 1 that the error of this result is not 1/100th of its value.’ He would have won the bet, as another 150 years’ accumulation of data has changed the estimate by only 0.63%!</p>
</div></blockquote>
<blockquote>
<div><p>According to the frequency definition, however, we are not permitted to use probability theory to tackle this problem. This is because the mass of Saturn is a constant and not a random variable; therefore, it has no frequency distribution and so probability theory cannot be used.</p>
<p>If the pdf [of Fig. 1.2] had to be interpreted in terms of the frequency definition, we would have to imagine a large ensemble of universes in which everything remains constant apart from the mass of Saturn.</p>
</div></blockquote>
<blockquote>
<div><p>As this scenario appears quite far-fetched, we might be inclined to think of [Fig. 1.2] in terms of the distribution of the measurements of the mass in many repetitions of the experiment. Although we are at liberty to think about a problem in any way that facilitates its solution, or our understanding of it, having to seek a frequency interpretation for every data analysis problem seems rather perverse.
For example, what do we mean by the ‘measurement of the mass’ when the data consist of orbital periods? Besides, why should we have to think about many repetitions of an experiment that never happened? What we really want to do is to make the best inference of the mass given the (few) data that we actually have; this is precisely the Bayes and Laplace view of probability.</p>
</div></blockquote>
<blockquote>
<div><p>Faced with the realization that the frequency definition of probability theory did not permit most real-life scientific problems to be addressed, a new subject was invented — statistics! To estimate the mass of Saturn, for example, one has to relate the mass to the data through some function called the statistic; since the data are subject to ‘random’ noise, the statistic becomes the random variable to which the rules of probability the- ory can be applied. But now the question arises: How should we choose the statistic? The frequentist approach does not yield a natural way of doing this and has, therefore, led to the development of several alternative schools of orthodox or conventional statis- tics. The masters, such as Fisher, Neyman and Pearson, provided a variety of different principles, which has merely resulted in a plethora of tests and procedures without any clear underlying rationale. This lack of unifying principles is, perhaps, at the heart of the shortcomings of the cook-book approach to statistics that students are often taught even today.</p>
</div></blockquote>
</div>
<div class="section" id="the-bayesian-recipe">
<h2><span class="section-number">12.5. </span>The Bayesian recipe<a class="headerlink" href="#the-bayesian-recipe" title="Permalink to this headline">¶</a></h2>
<p>Assess hypotheses by calculating their probabilities <span class="math notranslate nohighlight">\(p(H_i | \ldots)\)</span> conditional on known and/or presumed information using the rules of probability theory.</p>
<p>Probability Theory Axioms:
Product (AND) rule :
:<br />
<span class="math notranslate nohighlight">\(p(A, B | I) = p(A|I) p(B|A, I) = p(B|I)p(A|B,I)\)</span>
Should read <span class="math notranslate nohighlight">\(p(A,B|I)\)</span> as the probability for propositions <span class="math notranslate nohighlight">\(A\)</span> AND <span class="math notranslate nohighlight">\(B\)</span> being true given that <span class="math notranslate nohighlight">\(I\)</span> is true.</p>
<p>Sum (OR) rule:
:<br />
<span class="math notranslate nohighlight">\(p(A + B | I) = p(A | I) + p(B | I) - p(A, B | I)\)</span>
<span class="math notranslate nohighlight">\(p(A+B|I)\)</span> is the probability that proposition <span class="math notranslate nohighlight">\(A\)</span> OR <span class="math notranslate nohighlight">\(B\)</span> is true given that <span class="math notranslate nohighlight">\(I\)</span> is true.</p>
<p>Normalization:
:<br />
<span class="math notranslate nohighlight">\(p(A|I) + p(\bar{A}|I) = 1\)</span>
<span class="math notranslate nohighlight">\(\bar{A}\)</span> denotes the proposition that <span class="math notranslate nohighlight">\(A\)</span> is false.</p>
</div>
<div class="section" id="bayes-theorem">
<h2><span class="section-number">12.6. </span>Bayes’ theorem<a class="headerlink" href="#bayes-theorem" title="Permalink to this headline">¶</a></h2>
<p>Bayes’ theorem follows directly from the product rule</p>
<div class="math notranslate nohighlight">
\[\]</div>
<p>p(A|B,I) = \frac{p(B|A,I) p(A|I)}{p(B|I)}.
$<span class="math notranslate nohighlight">\(
\)</span>$</p>
<p>The importance of this property to data analysis becomes apparent if we replace <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> by hypothesis(<span class="math notranslate nohighlight">\(H\)</span>) and data(<span class="math notranslate nohighlight">\(D\)</span>):</p>
<!-- Equation labels as ordinary links -->
<div id="eq:bayes"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation}
p(H|D,I) = \frac{p(D|H,I) p(H|I)}{p(D|I)}.
\label{eq:bayes} \tag{1}
\end{equation}
\]</div>
<p>The power of Bayes’ theorem lies in the fact that it relates the quantity of interest, the probability that the hypothesis is true given the data, to the term we have a better chance of being able to assign, the probability that we would have observed the measured data if the hypothesis was true.</p>
<p>The various terms in Bayes’ theorem have formal names.</p>
<ul class="simple">
<li><p>The quantity on the far right, <span class="math notranslate nohighlight">\(p(H|I)\)</span>, is called the <em>prior</em> probability; it represents our state of knowledge (or ignorance) about the truth of the hypothesis before we have analysed the current data.</p></li>
<li><p>This is modified by the experimental measurements through <span class="math notranslate nohighlight">\(p(D|H,I)\)</span>, the <em>likelihood</em> function,</p></li>
<li><p>The denominator <span class="math notranslate nohighlight">\(p(D|I)\)</span> is called the <em>evidence</em>. It does not depend on the hypothesis and can be regarded as a normalization constant.</p></li>
<li><p>Together, these yield the <em>posterior</em> probability, <span class="math notranslate nohighlight">\(p(H|D, I )\)</span>, representing our state of knowledge about the truth of the hypothesis in the light of the data.</p></li>
</ul>
<p>In a sense, Bayes’ theorem encapsulates the process of learning.</p>
</div>
<div class="section" id="the-friends-of-bayes-theorem">
<h2><span class="section-number">12.7. </span>The friends of Bayes’ theorem<a class="headerlink" href="#the-friends-of-bayes-theorem" title="Permalink to this headline">¶</a></h2>
<p>Normalization:
:<br />
<span class="math notranslate nohighlight">\(\sum_i p(H_i|\ldots) = 1\)</span>.</p>
<p>Marginalization:
:<br />
<span class="math notranslate nohighlight">\(\sum_i p(A,H_i|I) = \sum_i p(H_i|A,I) p(A|I) = p(A|I)\)</span>.</p>
<p>Marginalization (continuum limit):
:<br />
<span class="math notranslate nohighlight">\(\int dx p(A,H(x)|I) = p(A|I)\)</span>.</p>
<p>In the above, <span class="math notranslate nohighlight">\(H_i\)</span> is an exclusive and exhaustive list of hypotheses. For example,let’s imagine that there are five candidates in a presidential election; then <span class="math notranslate nohighlight">\(H_1\)</span> could be the proposition that the first candidate will win, and so on. The probability that <span class="math notranslate nohighlight">\(A\)</span> is true, for example that unemployment will be lower in a year’s time (given all relevant information <span class="math notranslate nohighlight">\(I\)</span>, but irrespective of whoever becomes president) is then given by <span class="math notranslate nohighlight">\(\sum_i p(A,H_i|I)\)</span>.</p>
<p>In the continuum limit of propositions we must understand <span class="math notranslate nohighlight">\(p(\ldots)\)</span> as a pdf (probability density function).</p>
<p>Marginalization is a very powerful device in data analysis because it enables us to deal with nuisance parameters; that is, quantities which necessarily enter the analysis but are of no intrinsic interest. The unwanted background signal present in many experimental measurements are examples of nuisance parameters.</p>
</div>
<div class="section" id="inference-with-parametric-models">
<h2><span class="section-number">12.8. </span>Inference With Parametric Models<a class="headerlink" href="#inference-with-parametric-models" title="Permalink to this headline">¶</a></h2>
<p>Inductive inference with parametric models is a very important tool in the natural sciences.</p>
<ul class="simple">
<li><p>Consider <span class="math notranslate nohighlight">\(N\)</span> different models <span class="math notranslate nohighlight">\(M_i\)</span> (<span class="math notranslate nohighlight">\(i = 1, \ldots, N\)</span>), each with parameters <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}_i\)</span>. Each of them implies a sampling distribution (conditional predictive distribution for possible data)</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\]</div>
<p>p(D|\boldsymbol{\alpha}_i, M_i)
$<span class="math notranslate nohighlight">\(
\)</span>$</p>
<ul class="simple">
<li><p>The <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}_i\)</span> dependence when we fix attention on the actual, observed data (<span class="math notranslate nohighlight">\(D_\mathrm{obs}\)</span>) is the likelihood function:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\]</div>
<p>\mathcal{L}_i (\boldsymbol{\alpha}<em>i) \equiv p(D</em>\mathrm{obs}|\boldsymbol{\alpha}_i, M_i)
$<span class="math notranslate nohighlight">\(
\)</span>$</p>
<ul class="simple">
<li><p>We may be uncertain about <span class="math notranslate nohighlight">\(i\)</span> (model uncertainty),</p></li>
<li><p>or uncertain about <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}_i\)</span> (parameter uncertainty).</p></li>
</ul>
<p>Parameter Estimation:
:<br />
Premise = choice of model (pick specific <span class="math notranslate nohighlight">\(i\)</span>)
<span class="math notranslate nohighlight">\(\Rightarrow\)</span> What can we say about <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}_i\)</span>?</p>
<p>Model comparison:
:<br />
Premise = <span class="math notranslate nohighlight">\(\{M_i\}\)</span>
<span class="math notranslate nohighlight">\(\Rightarrow\)</span> What can we say about <span class="math notranslate nohighlight">\(i\)</span>?</p>
<p>Model adequacy:
:<br />
Premise = <span class="math notranslate nohighlight">\(M_1\)</span>
<span class="math notranslate nohighlight">\(\Rightarrow\)</span> Is <span class="math notranslate nohighlight">\(M_1\)</span> adequate?</p>
<p>Hybrid Uncertainty:
:<br />
Models share some common params: <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}_1 = \{ \boldsymbol{\varphi}, \boldsymbol{\eta}_i\}\)</span>
<span class="math notranslate nohighlight">\(\Rightarrow\)</span> What can we say about <span class="math notranslate nohighlight">\(\boldsymbol{\varphi}\)</span>? (Systematic error is an example)</p>
</div>
<div class="section" id="illustrative-examples-with-python-code">
<h2><span class="section-number">12.9. </span>Illustrative examples with python code<a class="headerlink" href="#illustrative-examples-with-python-code" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Is this a fair coin? (analytical)</p></li>
<li><p>Flux from a star (single parameter, MCMC)</p></li>
<li><p>The lighthouse problem (two parameters, MCMC)</p></li>
<li><p>Linear fit with outliers (nuisance parameters)</p></li>
<li><p>…</p></li>
</ul>
</div>
<div class="section" id="example-is-this-a-fair-coin">
<h2><span class="section-number">12.10. </span>Example: Is this a fair coin?<a class="headerlink" href="#example-is-this-a-fair-coin" title="Permalink to this headline">¶</a></h2>
<p>Let us begin with the analysis of data from a simple coin-tossing experiment.
Given that we had observed 6 heads in 8 flips, would you think it was a fair coin? By fair, we mean that we would be prepared to lay an even 1 : 1 bet on the outcome of a flip being a head or a tail. If we decide that the coin was fair, the question which follows naturally is how sure are we that this was so; if it was not fair, how unfair do we think it was? Furthermore, if we were to continue collecting data for this particular coin, observing the outcomes of additional flips, how would we update our belief on the fairness of the coin?</p>
<p>A sensible way of formulating this problem is to consider a large number of hypotheses about the range in which the bias-weighting of the coin might lie. If we denote the bias-weighting by <span class="math notranslate nohighlight">\(H\)</span>, then <span class="math notranslate nohighlight">\(H = 0\)</span> and <span class="math notranslate nohighlight">\(H = 1\)</span> can represent a coin which produces a tail or a head on every flip, respectively. There is a continuum of possibilities for the value of H between these limits, with <span class="math notranslate nohighlight">\(H = 0.5\)</span> indicating a fair coin. Our state of knowledge about the fairness, or the degree of unfairness, of the coin is then completely summarized by specifying how much we believe these various propositions to be true.</p>
<p>Let us perform a computer simulation of a coin-tossing experiment. This provides the data that we will be analysing.</p>
<p>0</p>
<p>&lt;
&lt;
&lt;
!
!
C
O
D
E
_
B
L
O
C
K</p>
<p>p
y
c
o
d</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">999</span><span class="p">)</span>         <span class="c1"># for reproducibility</span>
<span class="n">a</span><span class="o">=</span><span class="mf">0.6</span>                       <span class="c1"># biased coin</span>
<span class="n">flips</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">12</span><span class="p">)</span> <span class="c1"># simulates 4096 coin flips</span>
<span class="n">heads</span><span class="o">=</span><span class="n">flips</span><span class="o">&lt;</span><span class="n">a</span>               <span class="c1"># boolean array, heads[i]=True if flip i is heads</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="o">&lt;</span><span class="n">ipython</span><span class="o">-</span><span class="nb">input</span><span class="o">-</span><span class="mi">1</span><span class="o">-</span><span class="mi">3</span><span class="n">f1158901148</span><span class="o">&gt;</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">999</span><span class="p">)</span>         <span class="c1"># for reproducibility</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="n">a</span><span class="o">=</span><span class="mf">0.6</span>                       <span class="c1"># biased coin</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="n">flips</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">12</span><span class="p">)</span> <span class="c1"># simulates 4096 coin flips</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="n">heads</span><span class="o">=</span><span class="n">flips</span><span class="o">&lt;</span><span class="n">a</span>               <span class="c1"># boolean array, heads[i]=True if flip i is heads</span>

<span class="ne">NameError</span>: name &#39;np&#39; is not defined
</pre></div>
</div>
</div>
</div>
<p>In the light of this data, our inference about the fairness of this coin is summarized by the conditional pdf: <span class="math notranslate nohighlight">\(p(H|D,I)\)</span>. This is, of course, shorthand for the limiting case of a continuum of propositions for the value of <span class="math notranslate nohighlight">\(H\)</span>; that is to say, the probability that <span class="math notranslate nohighlight">\(H\)</span> lies in an infinitesimally narrow range is given by <span class="math notranslate nohighlight">\(p(H|D,I) dH\)</span>.</p>
<p>To estimate this posterior pdf, we need to use Bayes’ theorem (<a class="reference external" href="#eq:bayes">1</a>). We will ignore the denominator <span class="math notranslate nohighlight">\(p(D|I)\)</span> as it does not involve bias-weighting explicitly, and it will therefore not affect the shape of the desired pdf. At the end we can evaluate the missing constant subsequently from the normalization condition</p>
<!-- Equation labels as ordinary links -->
<div id="eq:coin_posterior_norm"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation}
\int_0^1 p(H|D,I) dH = 1.
\label{eq:coin_posterior_norm} \tag{2}
\end{equation}
\]</div>
<p>The prior pdf, <span class="math notranslate nohighlight">\(p(H|I)\)</span>, represents what we know about the coin given only the information <span class="math notranslate nohighlight">\(I\)</span> that we are dealing with a ‘strange coin’. We could keep a very open mind about the nature of the coin; a simple probability assignment which reflects this is a uniform, or flat, prior</p>
<!-- Equation labels as ordinary links -->
<div id="eq:coin_prior_uniform"></div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{equation}
p(H|I) = \left\{ \begin{array}{ll}
1 &amp; 0 \le H \le 1, \\
0 &amp; \mathrm{otherwise}.
\end{array} \right.
\label{eq:coin_prior_uniform} \tag{3}
\end{equation}
\end{split}\]</div>
<p>We will get back later to the choice of prior and its effect on the analysis.</p>
<p>This prior state of knowledge, or ignorance, is modified by the data through the likelihood function <span class="math notranslate nohighlight">\(p(D|H,I)\)</span>. It is a measure of the chance that we would have obtained the data that we actually observed, if the value of the bias-weighting was given (as known). If, in the conditioning information <span class="math notranslate nohighlight">\(I\)</span>, we assume that the flips of the coin were independent events, so that the outcome of one did not influence that of another, then the probability of obtaining the data `R heads in N tosses’ is given by the binomial distribution (we leave a formal definition of this to a statistics textbook)</p>
<!-- Equation labels as ordinary links -->
<div id="_auto1"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation}
p(D|H,I) \propto H^R (1-H)^{N-R}.
\label{_auto1} \tag{4}
\end{equation}
\]</div>
<p>It seems reasonable because <span class="math notranslate nohighlight">\(H\)</span> is the chance of obtaining a head on any flip, and there were <span class="math notranslate nohighlight">\(R\)</span> of them, and <span class="math notranslate nohighlight">\(1-H\)</span> is the corresponding probability for a tail, of which there were <span class="math notranslate nohighlight">\(N-R\)</span>. We note that this binomial distribution also contains a normalization factor, but we will ignore it since it does not depend explicitly on <span class="math notranslate nohighlight">\(H\)</span>, the quantity of interest. It will be absorbed by the normalization condition (<a class="reference external" href="#eq:coin_posterior_norm">2</a>).</p>
<p>We perform the setup of this Bayesian framework on the computer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">prior</span><span class="p">(</span><span class="n">H</span><span class="p">):</span>
    <span class="n">p</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">H</span><span class="p">)</span>
    <span class="n">p</span><span class="p">[(</span><span class="mi">0</span><span class="o">&lt;=</span><span class="n">x</span><span class="p">)</span><span class="o">&amp;</span><span class="p">(</span><span class="n">x</span><span class="o">&lt;=</span><span class="mi">1</span><span class="p">)]</span><span class="o">=</span><span class="mi">1</span>      <span class="c1"># allowed range: 0&lt;=H&lt;=1</span>
    <span class="k">return</span> <span class="n">p</span>                <span class="c1"># uniform prior</span>
<span class="k">def</span> <span class="nf">likelihood</span><span class="p">(</span><span class="n">H</span><span class="p">,</span><span class="n">data</span><span class="p">):</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">no_of_heads</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">no_of_tails</span> <span class="o">=</span> <span class="n">N</span> <span class="o">-</span> <span class="n">no_of_heads</span>
    <span class="k">return</span> <span class="n">H</span><span class="o">**</span><span class="n">no_of_heads</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">H</span><span class="p">)</span><span class="o">**</span><span class="n">no_of_tails</span>
<span class="k">def</span> <span class="nf">posterior</span><span class="p">(</span><span class="n">H</span><span class="p">,</span><span class="n">data</span><span class="p">):</span>
    <span class="n">p</span><span class="o">=</span><span class="n">prior</span><span class="p">(</span><span class="n">H</span><span class="p">)</span><span class="o">*</span><span class="n">likelihood</span><span class="p">(</span><span class="n">H</span><span class="p">,</span><span class="n">data</span><span class="p">)</span>
    <span class="n">norm</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">trapz</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">H</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span><span class="o">/</span><span class="n">norm</span>
</pre></div>
</div>
</div>
</div>
<p>The next step is to confront this setup with the simulated data. To get a feel for the result, it is instructive to see how the posterior pdf evolves as we obtain more and more data pertaining to the coin. The results of such an analyses is shown in Fig. <a class="reference external" href="#fig:coinflipping">fig:coinflipping</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span><span class="n">ncols</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">sharey</span><span class="o">=</span><span class="s1">&#39;row&#39;</span><span class="p">)</span>
<span class="n">axs_vec</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">axs</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">axs_vec</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">prior</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="k">for</span> <span class="n">ndouble</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">11</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">axs_vec</span><span class="p">[</span><span class="mi">1</span><span class="o">+</span><span class="n">ndouble</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">posterior</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">heads</span><span class="p">[:</span><span class="mi">2</span><span class="o">**</span><span class="n">ndouble</span><span class="p">]))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="s1">&#39;$N=</span><span class="si">{0}</span><span class="s1">$&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="n">ndouble</span><span class="p">),</span> <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">transAxes</span><span class="p">)</span>
<span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span> <span class="n">axs</span><span class="p">[</span><span class="n">row</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$p(H|D_\mathrm</span><span class="si">{obs}</span><span class="s1">,I)$&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span> <span class="n">axs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$H$&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<!-- dom:FIGURE:[fig/coinflipping_fig_1.png, width=500 frac=0.95] The evolution of the posterior pdf for the bias-weighting of a coin, as the number of data available increases. The figure on the top left-hand corner of each panel shows the number of data included in the analysis. <div id="fig:coinflipping"></div> -->
<!-- begin figure -->
<div id="fig:coinflipping"></div>
<p>The evolution of the posterior pdf for the bias-weighting of a coin, as the number of data available increases. The figure on the top left-hand corner of each panel shows the number of data included in the analysis.</p>
<img src="fig/coinflipping_fig_1.png" width=500>
<!-- end figure -->
<p>The panel in the top left-hand corner shows the posterior pdf for <span class="math notranslate nohighlight">\(H\)</span> given no data, i.e., it is the same as the prior pdf of Eq. (<a class="reference external" href="#eq:coin_prior_uniform">3</a>). It indicates that we have no more reason to believe that the coin is fair than we have to think that it is double-headed, double-tailed, or of any other intermediate bias-weighting.</p>
<p>The first flip is obviously tails. At this point we have no evidence that the coin has a side with heads, as indicated by the pdf going to zero as <span class="math notranslate nohighlight">\(H \to 1\)</span>. The second flip is obviously heads and we have now excluded both extreme options <span class="math notranslate nohighlight">\(H=0\)</span> (double-tailed) and <span class="math notranslate nohighlight">\(H=1\)</span> (double-headed). We can note that the posterior at this point has the simple form <span class="math notranslate nohighlight">\(p(H|D,I) = H(1-H)\)</span> for <span class="math notranslate nohighlight">\(0 \le H \le 1\)</span>.</p>
<p>The remainder of Fig. <a class="reference external" href="#fig:coinflipping">fig:coinflipping</a> shows how the posterior pdf evolves as the number of data analysed becomes larger and larger. We see that the position of the maximum moves around, but that the amount by which it does so decreases with the increasing number of observations. The width of the posterior pdf also becomes narrower with more data, indicating that we are becoming increasingly confident in our estimate of the bias-weighting. For the coin in this example, the best estimate of <span class="math notranslate nohighlight">\(H\)</span> eventually converges to 0.6, which, of course, was the value chosen to simulate the flips.</p>
</div>
<div class="section" id="a-few-words-on-different-priors">
<h2><span class="section-number">12.11. </span>A few words on different priors<a class="headerlink" href="#a-few-words-on-different-priors" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>uniform</p></li>
<li><p>Gaussian</p></li>
<li><p>Jeffrey’s prior</p></li>
</ul>
<p>Repeat the coin flipping experiment with other priors.</p>
</div>
<div class="section" id="bayesian-parameter-estimation-single-parameter">
<h2><span class="section-number">12.12. </span>Bayesian parameter estimation (single parameter)<a class="headerlink" href="#bayesian-parameter-estimation-single-parameter" title="Permalink to this headline">¶</a></h2>
<p>We will now consider the very important task of model parameter estimation using statistical inference.
[CF 1: maybe stress that model parameters are not random variables, and the meaning of parameter estimation is therefore very different between frequentist and bayesian approaches.]</p>
<p>Throughout this section we will consider a specific example that involves a model with a single parameter: “Measured flux from a star”.</p>
<div class="section" id="example-measured-flux-from-a-star">
<h3><span class="section-number">12.12.1. </span>Example: Measured flux from a star<a class="headerlink" href="#example-measured-flux-from-a-star" title="Permalink to this headline">¶</a></h3>
<p>Adapted from the blog <a class="reference external" href="http://jakevdp.github.io">Pythonic Perambulations</a> by Jake VanderPlas.</p>
<p>Imagine that we point our telescope to the sky, and observe the light coming from a single star. For the time being, we’ll assume that the star’s true flux is constant with time, i.e. that is it has a fixed value <span class="math notranslate nohighlight">\(F_\mathrm{true}\)</span> (we’ll also ignore effects like sky noise and other sources of systematic error). We’ll assume that we perform a series of <span class="math notranslate nohighlight">\(N\)</span> measurements with our telescope, where the ith measurement reports the observed photon flux <span class="math notranslate nohighlight">\(F_i\)</span> and error <span class="math notranslate nohighlight">\(e_i\)</span><a class="footnote-reference brackets" href="#errors" id="id2">2</a>.
The question is, given this set of measurements <span class="math notranslate nohighlight">\(D = \{F_i, e_i\}\)</span>, what is our best estimate of the true flux <span class="math notranslate nohighlight">\(F_\mathrm{true}\)</span>?</p>
<p>Because the measurements are number counts, a Poisson distribution is a good approximation to the measurement process:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>      <span class="c1"># for repeatability</span>
<span class="n">F_true</span> <span class="o">=</span> <span class="mi">1000</span>          <span class="c1"># true flux, say number of photons measured in 1 second</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">50</span>                 <span class="c1"># number of measurements</span>
<span class="n">F</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span><span class="n">F_true</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
                       <span class="c1"># N measurements of the flux</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">F</span><span class="p">)</span>         <span class="c1"># errors on Poisson counts estimated via square root</span>
</pre></div>
</div>
</div>
</div>
<p>Now let’s make a simple visualization of the “observed” data, see Fig. <a class="reference external" href="#fig:flux">fig:flux</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">N</span><span class="p">),</span> <span class="n">xerr</span><span class="o">=</span><span class="n">e</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;ok&#39;</span><span class="p">,</span> <span class="n">ecolor</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">vlines</span><span class="p">([</span><span class="n">F_true</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Flux&quot;</span><span class="p">);</span><span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;measurement number&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<!-- dom:FIGURE:[fig/singlephotoncount_fig_1.png, width=400 frac=0.8] Single photon counts (flux measurements). <div id="fig:flux"></div> -->
<!-- begin figure -->
<div id="fig:flux"></div>
<p>Single photon counts (flux measurements).</p>
<img src="fig/singlephotoncount_fig_1.png" width=400>
<!-- end figure -->
<p>These measurements each have a different error <span class="math notranslate nohighlight">\(e_i\)</span> which is estimated from Poisson statistics using the standard square-root rule. In this toy example we already know the true flux <span class="math notranslate nohighlight">\(F_\mathrm{true}\)</span>, but the question is this: given our measurements and errors, what is our best estimate of the true flux?</p>
<p>Let’s take a look at the frequentist and Bayesian approaches to solving this.</p>
</div>
<div class="section" id="simple-photon-counts-frequentist-approach">
<h3><span class="section-number">12.12.2. </span>Simple Photon Counts: Frequentist Approach<a class="headerlink" href="#simple-photon-counts-frequentist-approach" title="Permalink to this headline">¶</a></h3>
<p>We’ll start with the classical frequentist maximum likelihood approach. Given a single observation <span class="math notranslate nohighlight">\(D_i = (F_i, e_i)\)</span>, we can compute the probability distribution of the measurement given the true flux Ftrue given our assumption of Gaussian errors</p>
<!-- Equation labels as ordinary links -->
<div id="_auto2"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation}
p(D_i | F_\mathrm{true}, I) = \frac{1}{\sqrt{2\pi e_i^2}} \exp \left( \frac{-(F_i-F_\mathrm{true})^2}{2e_i^2} \right).
\label{_auto2} \tag{5}
\end{equation}
\]</div>
<p>This should be read “the probability of <span class="math notranslate nohighlight">\(D_i\)</span> given <span class="math notranslate nohighlight">\(F_\mathrm{true}\)</span>
equals …”. You should recognize this as a normal distribution with mean <span class="math notranslate nohighlight">\(F_\mathrm{true}\)</span> and standard deviation <span class="math notranslate nohighlight">\(e_i\)</span>.</p>
<p>We construct the <em>likelihood function</em> by computing the product of the probabilities for each data point</p>
<!-- Equation labels as ordinary links -->
<div id="_auto3"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation}
\mathcal{L}(D | F_\mathrm{true}, I) = \prod_{i=1}^N p(D_i | F_\mathrm{true}, I),
\label{_auto3} \tag{6}
\end{equation}
\]</div>
<p>here <span class="math notranslate nohighlight">\(D = \{D_i\}\)</span> represents the entire set of measurements. Because the value of the likelihood can become very small, it is often more convenient to instead compute the log-likelihood. Combining the previous two equations and computing the log, we have</p>
<!-- Equation labels as ordinary links -->
<div id="_auto4"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation}
\log\mathcal{L} = -\frac{1}{2} \sum_{i=1}^N \left[ \log(2\pi e_i^2) +  \frac{(F_i-F_\mathrm{true})^2}{e_i^2} \right].
\label{_auto4} \tag{7}
\end{equation}
\]</div>
<p>What we’d like to do is determine <span class="math notranslate nohighlight">\(F_\mathrm{true}\)</span> such that the likelihood is maximized. For this simple problem, the maximization can be computed analytically (i.e. by setting <span class="math notranslate nohighlight">\(d\log\mathcal{L}/d F_\mathrm{true} = 0\)</span>). This results in the following observed estimate of <span class="math notranslate nohighlight">\(F_\mathrm{true}\)</span></p>
<!-- Equation labels as ordinary links -->
<div id="_auto5"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation}
F_\mathrm{est} = \sum_{i=1}^N w_i F_i; \quad w_i = 1/e_i^2.
\label{_auto5} \tag{8}
\end{equation}
\]</div>
<p>Notice that in the special case of all errors <span class="math notranslate nohighlight">\(e_i\)</span> being equal, this reduces to</p>
<!-- Equation labels as ordinary links -->
<div id="_auto6"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation}
F_\mathrm{est} = \frac{1}{N} \sum_{i=1} F_i.
\label{_auto6} \tag{9}
\end{equation}
\]</div>
<p>That is, in agreement with intuition, <span class="math notranslate nohighlight">\(F_\mathrm{est}\)</span> is simply the mean of the observed data when errors are equal.</p>
<p>We can go further and ask what the error of our estimate is. In the frequentist approach, this can be accomplished by fitting a Gaussian approximation to the likelihood curve at maximum; in this simple case this can also be solved analytically (the sum of Gaussians is also a Gaussian). It can be shown that the standard deviation of this Gaussian approximation is</p>
<!-- Equation labels as ordinary links -->
<div id="_auto7"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation}
\sigma_\mathrm{est} = \sum_{i=1}^N w_i.
\label{_auto7} \tag{10}
\end{equation}
\]</div>
<p>These results are fairly simple calculations; let’s evaluate them for our toy dataset:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span><span class="o">=</span><span class="mf">1.</span><span class="o">/</span><span class="n">e</span><span class="o">**</span><span class="mi">2</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">F_true = </span><span class="si">{0}</span><span class="s2"></span>
<span class="s2">F_est = </span><span class="si">{1:.0f}</span><span class="s2"> +/- </span><span class="si">{2:.0f}</span><span class="s2"> (based on </span><span class="si">{3}</span><span class="s2"> measurements) &quot;&quot;&quot;</span>\
          <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">F_true</span><span class="p">,</span> <span class="p">(</span><span class="n">w</span> <span class="o">*</span> <span class="n">F</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">w</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="n">w</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">N</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">F_true</span> <span class="pre">=</span> <span class="pre">1000</span></code>
<code class="docutils literal notranslate"><span class="pre">F_est</span> <span class="pre">=</span> <span class="pre">998</span> <span class="pre">+/-</span> <span class="pre">4</span> <span class="pre">(based</span> <span class="pre">on</span> <span class="pre">50</span> <span class="pre">measurements)</span></code></p>
<p>We find that for 50 measurements of the flux, our estimate has an error of about 0.4% and is consistent with the input value.</p>
</div>
<div class="section" id="simple-photon-counts-bayesian-approach">
<h3><span class="section-number">12.12.3. </span>Simple Photon Counts: Bayesian Approach<a class="headerlink" href="#simple-photon-counts-bayesian-approach" title="Permalink to this headline">¶</a></h3>
<p>The Bayesian approach, as you might expect, begins and ends with probabilities. Our hypothesis is that the star has a constant flux <span class="math notranslate nohighlight">\(F_\mathrm{true}\)</span>. It recognizes that what we fundamentally want to compute is our knowledge of the parameters in question given the data and other information (such as our knowledge of uncertainties for the observed values), i.e. in this case, <span class="math notranslate nohighlight">\(p(F_\mathrm{true} | D,I)\)</span>.
Note that this formulation of the problem is fundamentally contrary to the frequentist philosophy, which says that probabilities have no meaning for model parameters like <span class="math notranslate nohighlight">\(F_\mathrm{true}\)</span>. Nevertheless, within the Bayesian philosophy this is perfectly acceptable.</p>
<p>To compute this result, Bayesians next apply Bayes’ Theorem (<a class="reference external" href="#eq:bayes">1</a>).
If we set the prior <span class="math notranslate nohighlight">\(p(F_\mathrm{true}|I) \propto 1\)</span> (a flat prior), we find
<span class="math notranslate nohighlight">\(p(F_\mathrm{true}|D,I) \propto p(D | F_\mathrm{true},I) \equiv \mathcal{L}(D | F_\mathrm{true},I)\)</span>
and the Bayesian probability is maximized at precisely the same value as the frequentist result! So despite the philosophical differences, we see that (for this simple problem at least) the Bayesian and frequentist point estimates are equivalent.</p>
</div>
<div class="section" id="a-note-about-priors">
<h3><span class="section-number">12.12.4. </span>A note about priors<a class="headerlink" href="#a-note-about-priors" title="Permalink to this headline">¶</a></h3>
<p>The prior allows inclusion of other information into the computation, which becomes very useful in cases where multiple measurement strategies are being combined to constrain a single model. The necessity to specify a prior, however, is one of the more controversial pieces of Bayesian analysis.
A frequentist will point out that the prior is problematic when no true prior information is available. Though it might seem straightforward to use a noninformative prior like the flat prior mentioned above, there are some [surprisingly subtleties](http://normaldeviate.wordpress.com/2013/07/13/lost-causes-in-statistics-ii-noninformative- priors/comment-page-1/) involved. It turns out that in many situations, a truly noninformative prior does not exist! Frequentists point out that the subjective choice of a prior which necessarily biases your result has no place in statistical data analysis.
A Bayesian would counter that frequentism doesn’t solve this problem, but simply skirts the question. Frequentism can often be viewed as simply a special case of the Bayesian approach for some (implicit) choice of the prior: a Bayesian would say that it’s better to make this implicit choice explicit, even if the choice might include some subjectivity.</p>
</div>
<div class="section" id="simple-photon-counts-bayesian-approach-in-practice">
<h3><span class="section-number">12.12.5. </span>Simple Photon Counts: Bayesian approach in practice<a class="headerlink" href="#simple-photon-counts-bayesian-approach-in-practice" title="Permalink to this headline">¶</a></h3>
<p>Leaving these philosophical debates aside for the time being, let’s address how Bayesian results are generally computed in practice. For a one parameter problem like the one considered here, it’s as simple as computing the posterior probability <span class="math notranslate nohighlight">\(p(F_\mathrm{true} | D,I)\)</span> as a function of <span class="math notranslate nohighlight">\(F_\mathrm{true}\)</span>: this is the distribution reflecting our knowledge of the parameter <span class="math notranslate nohighlight">\(F_\mathrm{true}\)</span>.
But as the dimension of the model grows, this direct approach becomes increasingly intractable. For this reason, Bayesian calculations often depend on sampling methods such as Markov Chain Monte Carlo (MCMC). For this practical example, let us apply an MCMC approach using Dan Foreman-Mackey’s <a class="reference external" href="http://dan.iel.fm/emcee/current/">emcee</a> package. Keep in mind here that the goal is to generate a set of points drawn from the posterior probability distribution, and to use those points to determine the answer we seek.
To perform this MCMC, we start by defining Python functions for the prior <span class="math notranslate nohighlight">\(p(F_\mathrm{true} | I)\)</span>, the likelihood <span class="math notranslate nohighlight">\(p(D | F_\mathrm{true},I)\)</span>, and the posterior <span class="math notranslate nohighlight">\(p(F_\mathrm{true} | D,I)\)</span>, noting that none of these need be properly normalized. Our model here is one-dimensional, but to handle multi-dimensional models we’ll define the model in terms of an array of parameters <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}\)</span>, which in this case is <span class="math notranslate nohighlight">\(\boldsymbol{\alpha} = [F_\mathrm{true}]\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">log_prior</span><span class="p">(</span><span class="n">alpha</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">0</span> <span class="c1"># flat prior</span>

<span class="k">def</span> <span class="nf">log_likelihood</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">e</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">e</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> \
                             <span class="o">+</span> <span class="p">(</span><span class="n">F</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">e</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
                             
<span class="k">def</span> <span class="nf">log_posterior</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">e</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">log_prior</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span> <span class="o">+</span> <span class="n">log_likelihood</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now we set up the problem, including generating some random starting guesses for the multiple chains of points.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ndim</span> <span class="o">=</span> <span class="mi">1</span>      <span class="c1"># number of parameters in the model</span>
<span class="n">nwalkers</span> <span class="o">=</span> <span class="mi">50</span> <span class="c1"># number of MCMC walkers</span>
<span class="n">nburn</span> <span class="o">=</span> <span class="mi">1000</span>  <span class="c1"># &quot;burn-in&quot; period to let chains stabilize</span>
<span class="n">nsteps</span> <span class="o">=</span> <span class="mi">2000</span> <span class="c1"># number of MCMC steps to take</span>
<span class="c1"># we&#39;ll start at random locations between 0 and 2000</span>
<span class="n">starting_guesses</span> <span class="o">=</span> <span class="mi">2000</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">nwalkers</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
<span class="n">sampler</span> <span class="o">=</span> <span class="n">emcee</span><span class="o">.</span><span class="n">EnsembleSampler</span><span class="p">(</span><span class="n">nwalkers</span><span class="p">,</span> <span class="n">ndim</span><span class="p">,</span> <span class="n">log_posterior</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="n">F</span><span class="p">,</span><span class="n">e</span><span class="p">])</span>
<span class="n">sampler</span><span class="o">.</span><span class="n">run_mcmc</span><span class="p">(</span><span class="n">starting_guesses</span><span class="p">,</span> <span class="n">nsteps</span><span class="p">)</span>
<span class="c1"># Shape of sampler.chain  = (nwalkers, nsteps, ndim)</span>
<span class="c1"># Flatten the sampler chain and discard burn-in points:</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">chain</span><span class="p">[:,</span> <span class="n">nburn</span><span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">ndim</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>If this all worked correctly, the array sample should contain a series of 50,000 points drawn from the posterior. Let’s plot them and check. See results in Fig. <a class="reference external" href="#fig:flux-bayesian">fig:flux-bayesian</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">histtype</span><span class="o">=</span><span class="s2">&quot;stepfilled&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">normed</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$F_\mathrm</span><span class="si">{est}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$p(F_\mathrm</span><span class="si">{est}</span><span class="s1">|D,I)$&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<!-- dom:FIGURE:[fig/singlephotoncount_fig_2.png, width=400 frac=0.8] Bayesian posterior pdf (represented by a histogram of MCMC samples) from flux measurements. <div id="fig:flux-bayesian"></div> -->
<!-- begin figure -->
<div id="fig:flux-bayesian"></div>
<p>Bayesian posterior pdf (represented by a histogram of MCMC samples) from flux measurements.</p>
<img src="fig/singlephotoncount_fig_2.png" width=400>
<!-- end figure -->
</div>
<div class="section" id="best-estimates-and-confidence-intervals">
<h3><span class="section-number">12.12.6. </span>Best estimates and confidence intervals<a class="headerlink" href="#best-estimates-and-confidence-intervals" title="Permalink to this headline">¶</a></h3>
<p>The posterior distribution from our Bayesian data analysis is the key quantity that encodes our inference about the values of the model parameters, given the data and the relevant background information. Often, however, we wish to summarize this result with just a few numbers: the best estimate and a measure of its reliability.</p>
<p>There are a few different options for this. The choice of the most appropriate one depends mainly on the shape of the posterior distribution:</p>
<p><em>Symmetric posterior pdfs</em>: Since the probability (density) associated with any particular value of the parameter is a measure of how much we believe that it lies in the neighbourhood of that point, our best estimate is given by the maximum of the posterior pdf. If we denote the quantity of interest by <span class="math notranslate nohighlight">\(X\)</span>, with a posterior pdf <span class="math notranslate nohighlight">\(P =p(X|D,I)\)</span>, then the best estimate of its value <span class="math notranslate nohighlight">\(X_0\)</span> is given by the condition <span class="math notranslate nohighlight">\(dP/dX|_{X=X_0}=0\)</span>. Strictly speaking, we should also check the sign of the second derivative to ensure that <span class="math notranslate nohighlight">\(X_0\)</span> represents a maximum.</p>
<p>To obtain a measure of the reliability of this best estimate, we need to look at the width or spread of the posterior pdf about <span class="math notranslate nohighlight">\(X_0\)</span>. When considering the behaviour of any function in the neighbourhood of a particular point, it is often helpful to carry out a Taylor series expansion; this is simply a standard tool for (locally) approximating a complicated function by a low-order polynomial. The linear term is zero at the maximum and the quadratic term is often the dominating one determining the width of the posterior pdf. Ignoring all the higher-order terms we arrive at the Gaussian approximation</p>
<!-- Equation labels as ordinary links -->
<div id="_auto8"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation}
p(X|D,I) \approx \frac{1}{\sigma\sqrt{2\pi}} \exp \left[ -\frac{(x-\mu)^2}{2\sigma^2} \right],
\label{_auto8} \tag{11}
\end{equation}
\]</div>
<p>where the mean <span class="math notranslate nohighlight">\(\mu = X_0\)</span> and the variance <span class="math notranslate nohighlight">\(\sigma = \left( - \left. \frac{d^2L}{dX^2} \right|_{X_0} \right)^{-1/2}\)</span>, where <span class="math notranslate nohighlight">\(L\)</span> is the logarithm of the posterior <span class="math notranslate nohighlight">\(P\)</span>. Our inference about the quantity of interest is conveyed very concisely, therefore, by the statement <span class="math notranslate nohighlight">\(X = X_0 \pm \sigma\)</span>, and</p>
<div class="math notranslate nohighlight">
\[\]</div>
<p>p(X_0-\sigma &lt; X &lt; X_0+\sigma | D,I) = \int_{X_0-\sigma}^{X_0+\sigma} p(X|D,I) dX \approx 0.67.
$<span class="math notranslate nohighlight">\(
\)</span>$</p>
<p><em>Asymmetric posterior pdfs</em>: While the maximum of the posterior (<span class="math notranslate nohighlight">\(X_0\)</span>) can still be regarded as giving the best estimate, the true value is now more likely to be on one side of this rather than the other. Alternatively one can compute the mean value, <span class="math notranslate nohighlight">\(\langle X \rangle = \int X p(X|D,I) dX\)</span>, although this tends to overemphasise very long tails. The best option is probably a compromise that can be employed when having access to a large sample from the posterior (as provided by an MCMC), namely to give the median of this ensamble.</p>
<p>Furthermore, the concept of an error-bar does not seem appropriate in this case, as it implicitly entails the idea of symmetry. A good way of expressing the reliability with which a parameter can be inferred, for an asymmetric posterior pdf, is rather through a <em>confidence interval</em>. Since the area under the posterior pdf between <span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span> is proportional to how much we believe that <span class="math notranslate nohighlight">\(X\)</span> lies in that range, the shortest interval that encloses 67% of the area represents a sensible measure of the uncertainty of the estimate. Obviously we can choose to provide some other degree-of-belief that we think is relevant for the case at hand. Assuming that the posterior pdf has been normalized, to have unit area, we need to find <span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span> such that:</p>
<div class="math notranslate nohighlight">
\[\]</div>
<p>p(X_1 &lt; X &lt; X_2 | D,I) = \int_{X_1}^{X_2} p(X|D,I) dX \approx 0.67,
$<span class="math notranslate nohighlight">\(
\)</span>$</p>
<p>where the difference <span class="math notranslate nohighlight">\(X_2 - X_1\)</span> is as small as possible. The region <span class="math notranslate nohighlight">\(X_1 &lt; X &lt; X_2\)</span> is then called the shortest 67% confidence interval.</p>
<p><em>Multimodal posterior pdfs</em>: We can sometimes obtain posteriors which are multimodal; i.e. contains several disconnected regions with large probabilities. There is no difficulty when one of the maxima is very much larger than the others: we can simply ignore the subsidiary solutions, to a good approximation, and concentrate on the global maximum. The problem arises when there are several maxima of comparable magnitude. What do we now mean by a best estimate, and how should we quantify its reliability? The idea of a best estimate and an error-bar, or even a confidence interval, is merely an attempt to summarize the posterior with just two or three numbers; sometimes this just can’t be done, and so these concepts are not valid. For the bimodal case we might be able to characterize the posterior in terms of a few numbers: two best estimates and their associated error-bars, or disjoint confidence intervals. For a general multimodal pdf, the most honest thing we can do is just display the posterior itself.</p>
</div>
<div class="section" id="simple-photon-counts-best-estimates-and-confidence-intervals">
<h3><span class="section-number">12.12.7. </span>Simple Photon Counts: Best estimates and confidence intervals<a class="headerlink" href="#simple-photon-counts-best-estimates-and-confidence-intervals" title="Permalink to this headline">¶</a></h3>
<p>To compute these numbers for our example, you would run:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sampper</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">16.5</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mf">83.5</span><span class="p">,</span> <span class="mf">97.5</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">F_true = </span><span class="si">{0}</span><span class="s2"></span>
<span class="s2">Based on </span><span class="si">{1}</span><span class="s2"> measurements the posterior point estimates are:</span>
<span class="s2">...F_est = </span><span class="si">{2:.0f}</span><span class="s2"> +/- </span><span class="si">{3:.0f}</span><span class="s2"></span>
<span class="s2">or using credible intervals:</span>
<span class="s2">...F_est = </span><span class="si">{4:.0f}</span><span class="s2">          (posterior median) </span>
<span class="s2">...F_est in [</span><span class="si">{5:.0f}</span><span class="s2">, </span><span class="si">{6:.0f}</span><span class="s2">] (67</span><span class="si">% c</span><span class="s2">redible interval) </span>
<span class="s2">...F_est in [</span><span class="si">{7:.0f}</span><span class="s2">, </span><span class="si">{8:.0f}</span><span class="s2">] (95</span><span class="si">% c</span><span class="s2">redible interval) &quot;&quot;&quot;</span>\
          <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">F_true</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">samples</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">samples</span><span class="p">),</span> \
                      <span class="n">sampper</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">sampper</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">sampper</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">sampper</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">sampper</span><span class="p">[</span><span class="mi">4</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">F_true</span> <span class="pre">=</span> <span class="pre">1000</span></code>
<code class="docutils literal notranslate"><span class="pre">Based</span> <span class="pre">on</span> <span class="pre">50</span> <span class="pre">measurements</span> <span class="pre">the</span> <span class="pre">posterior</span> <span class="pre">point</span> <span class="pre">estimates</span> <span class="pre">are:</span></code>
<code class="docutils literal notranslate"><span class="pre">...F_est</span> <span class="pre">=</span> <span class="pre">998</span> <span class="pre">+/-</span> <span class="pre">4</span></code>
<code class="docutils literal notranslate"><span class="pre">or</span> <span class="pre">using</span> <span class="pre">credible</span> <span class="pre">intervals:</span></code>
<code class="docutils literal notranslate"><span class="pre">...F_est</span> <span class="pre">=</span> <span class="pre">998</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">(posterior</span> <span class="pre">median)</span></code><br />
<code class="docutils literal notranslate"><span class="pre">...F_est</span> <span class="pre">in</span> <span class="pre">[993,</span> <span class="pre">1002]</span> <span class="pre">(67%</span> <span class="pre">credible</span> <span class="pre">interval)</span></code><br />
<code class="docutils literal notranslate"><span class="pre">...F_est</span> <span class="pre">in</span> <span class="pre">[989,</span> <span class="pre">1006]</span> <span class="pre">(95%</span> <span class="pre">credible</span> <span class="pre">interval)</span></code></p>
<p>In this particular example, the posterior pdf is actually a Gaussian (since it is constructed as a product of Gaussians), and the mean and variance from the quadratic approximation will agree exactly with the frequentist approach.</p>
<p>From this final result you might come away with the impression that the Bayesian method is unnecessarily complicated, and in this case it certainly is. Using an MCMC sampler to characterize a one-dimensional normal distribution is a bit like using the Death Star to destroy a beach ball, but we did this here because it demonstrates an approach that can scale to complicated posteriors in many, many dimensions, and can provide nice results in more complicated situations where an analytic likelihood approach is not possible.</p>
<p>Furthermore, as data and models grow in complexity, the two approaches can diverge greatly.</p>
</div>
</div>
<div class="section" id="bayesian-parameter-estimation-multiple-parameters-covariance">
<h2><span class="section-number">12.13. </span>Bayesian parameter estimation (multiple parameters, covariance)<a class="headerlink" href="#bayesian-parameter-estimation-multiple-parameters-covariance" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>multidimensional posterior pdf:s</p></li>
<li><p>nuisance parameters (e.g. background subtraction?)</p></li>
<li><p>corner plots, covariance, correlations</p></li>
<li><p>best example?</p></li>
</ul>
</div>
<div class="section" id="bayesian-model-selection">
<h2><span class="section-number">12.14. </span>Bayesian model selection<a class="headerlink" href="#bayesian-model-selection" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Bayesian evidence</p></li>
<li><p>Occam’s razor</p></li>
<li><p>Best example? How many spectral lines are there?</p></li>
</ul>
<hr class="docutils" />
<dl class="footnote brackets">
<dt class="label" id="sivia"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Sivia, Devinderjit, and John Skilling. Data Analysis : A Bayesian Tutorial, OUP Oxford, 2006</p>
</dd>
<dt class="label" id="errors"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>We’ll make the reasonable assumption that errors are Gaussian. In a Frequentist perspective, <span class="math notranslate nohighlight">\(e_i\)</span> is the standard deviation of the results of a single measurement event in the limit of repetitions of <em>that event</em>. In the Bayesian perspective, <span class="math notranslate nohighlight">\(e_i\)</span> is the standard deviation of the (Gaussian) probability distribution describing our knowledge of that particular measurement given its observed value.</p>
</dd>
</dl>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="chapter10.html" title="previous page"><span class="section-number">9. </span>Recurrent Neural Networks</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Morten Hjorth-Jensen<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="_static/js/index.js"></script>
    
  </body>
</html>
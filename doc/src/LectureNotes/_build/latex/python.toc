\select@language {english}
\contentsline {chapter}{\numberline {1}Learning outcomes}{3}{chapter.1}
\contentsline {chapter}{\numberline {2}Machine Learning, a small (and probably biased) introduction}{5}{chapter.2}
\contentsline {chapter}{\numberline {3}A Frequentist approach to data analysis}{7}{chapter.3}
\contentsline {chapter}{\numberline {4}What is a good model?}{9}{chapter.4}
\contentsline {chapter}{\numberline {5}Choice of Programming Language}{11}{chapter.5}
\contentsline {chapter}{\numberline {6}Data handling, machine learning and ethical aspects}{13}{chapter.6}
\contentsline {section}{\numberline {6.1}Elements of Probability Theory and Statistical Data Analysis}{14}{section.6.1}
\contentsline {subsection}{\numberline {6.1.1}Domains and probabilities}{14}{subsection.6.1.1}
\contentsline {subsection}{\numberline {6.1.2}Tossing the dice}{14}{subsection.6.1.2}
\contentsline {subsection}{\numberline {6.1.3}Stochastic variables}{15}{subsection.6.1.3}
\contentsline {subsection}{\numberline {6.1.4}Stochastic variables and the main concepts, the discrete case}{15}{subsection.6.1.4}
\contentsline {subsection}{\numberline {6.1.5}Stochastic variables and the main concepts, the continuous case}{15}{subsection.6.1.5}
\contentsline {subsection}{\numberline {6.1.6}The cumulative probability}{15}{subsection.6.1.6}
\contentsline {subsection}{\numberline {6.1.7}Properties of PDFs}{15}{subsection.6.1.7}
\contentsline {subsection}{\numberline {6.1.8}Important distributions, the uniform distribution}{16}{subsection.6.1.8}
\contentsline {subsection}{\numberline {6.1.9}Gaussian distribution}{16}{subsection.6.1.9}
\contentsline {subsection}{\numberline {6.1.10}Exponential distribution}{18}{subsection.6.1.10}
\contentsline {subsection}{\numberline {6.1.11}Expectation values}{18}{subsection.6.1.11}
\contentsline {subsection}{\numberline {6.1.12}Stochastic variables and the main concepts, mean values}{18}{subsection.6.1.12}
\contentsline {subsection}{\numberline {6.1.13}Stochastic variables and the main concepts, central moments, the variance}{18}{subsection.6.1.13}
\contentsline {subsection}{\numberline {6.1.14}Probability Distribution Functions}{19}{subsection.6.1.14}
\contentsline {subsection}{\numberline {6.1.15}Probability Distribution Functions}{19}{subsection.6.1.15}
\contentsline {subsection}{\numberline {6.1.16}The three famous Probability Distribution Functions}{19}{subsection.6.1.16}
\contentsline {subsection}{\numberline {6.1.17}Probability Distribution Functions, the normal distribution}{20}{subsection.6.1.17}
\contentsline {subsection}{\numberline {6.1.18}Probability Distribution Functions, the normal distribution}{20}{subsection.6.1.18}
\contentsline {subsection}{\numberline {6.1.19}Probability Distribution Functions, the cumulative distribution}{20}{subsection.6.1.19}
\contentsline {subsection}{\numberline {6.1.20}Probability Distribution Functions, other important distribution}{21}{subsection.6.1.20}
\contentsline {subsection}{\numberline {6.1.21}Probability Distribution Functions, the binomial distribution}{21}{subsection.6.1.21}
\contentsline {subsection}{\numberline {6.1.22}Probability Distribution Functions, Poisson\IeC {\textquoteright }s distribution}{21}{subsection.6.1.22}
\contentsline {subsection}{\numberline {6.1.23}Probability Distribution Functions, Poisson\IeC {\textquoteright }s distribution}{22}{subsection.6.1.23}
\contentsline {subsection}{\numberline {6.1.24}Meet the covariance!}{22}{subsection.6.1.24}
\contentsline {subsection}{\numberline {6.1.25}Meet the covariance in matrix disguise}{22}{subsection.6.1.25}
\contentsline {subsection}{\numberline {6.1.26}Covariance}{23}{subsection.6.1.26}
\contentsline {subsection}{\numberline {6.1.27}Meet the covariance, uncorrelated events}{24}{subsection.6.1.27}
\contentsline {subsection}{\numberline {6.1.28}Numerical experiments and the covariance}{24}{subsection.6.1.28}
\contentsline {subsection}{\numberline {6.1.29}Numerical experiments and the covariance}{24}{subsection.6.1.29}
\contentsline {subsection}{\numberline {6.1.30}Numerical experiments and the covariance, actual situations}{24}{subsection.6.1.30}
\contentsline {subsection}{\numberline {6.1.31}Numerical experiments and the covariance, our observables}{25}{subsection.6.1.31}
\contentsline {subsection}{\numberline {6.1.32}Numerical experiments and the covariance, the sample variance}{25}{subsection.6.1.32}
\contentsline {subsection}{\numberline {6.1.33}Numerical experiments and the covariance, central limit theorem}{26}{subsection.6.1.33}
\contentsline {subsection}{\numberline {6.1.34}Definition of Correlation Functions and Standard Deviation}{26}{subsection.6.1.34}
\contentsline {subsection}{\numberline {6.1.35}Definition of Correlation Functions and Standard Deviation}{26}{subsection.6.1.35}
\contentsline {subsection}{\numberline {6.1.36}Definition of Correlation Functions and Standard Deviation}{26}{subsection.6.1.36}
\contentsline {subsection}{\numberline {6.1.37}Definition of Correlation Functions and Standard Deviation}{27}{subsection.6.1.37}
\contentsline {subsection}{\numberline {6.1.38}Definition of Correlation Functions and Standard Deviation, sample variance}{27}{subsection.6.1.38}
\contentsline {subsection}{\numberline {6.1.39}Definition of Correlation Functions and Standard Deviation}{27}{subsection.6.1.39}
\contentsline {subsection}{\numberline {6.1.40}Code to compute the Covariance matrix and the Covariance}{28}{subsection.6.1.40}
\contentsline {subsection}{\numberline {6.1.41}Random Numbers}{29}{subsection.6.1.41}
\contentsline {subsection}{\numberline {6.1.42}Random Numbers, better name: pseudo random numbers}{29}{subsection.6.1.42}
\contentsline {subsection}{\numberline {6.1.43}Random number generator RNG}{29}{subsection.6.1.43}
\contentsline {subsection}{\numberline {6.1.44}Random number generator RNG and periodic outputs}{29}{subsection.6.1.44}
\contentsline {subsection}{\numberline {6.1.45}Random number generator RNG and its period}{30}{subsection.6.1.45}
\contentsline {subsection}{\numberline {6.1.46}Random number generator RNG, other examples}{30}{subsection.6.1.46}
\contentsline {subsection}{\numberline {6.1.47}Random number generator RNG, other examples}{30}{subsection.6.1.47}
\contentsline {subsection}{\numberline {6.1.48}Random number generator RNG, RAN0}{31}{subsection.6.1.48}
\contentsline {subsection}{\numberline {6.1.49}Random number generator RNG, RAN0}{31}{subsection.6.1.49}
\contentsline {subsection}{\numberline {6.1.50}Random number generator RNG, RAN0}{31}{subsection.6.1.50}
\contentsline {subsection}{\numberline {6.1.51}Random number generator RNG, RAN0}{32}{subsection.6.1.51}
\contentsline {subsection}{\numberline {6.1.52}Random number generator RNG, RAN0 code}{32}{subsection.6.1.52}
\contentsline {subsection}{\numberline {6.1.53}Properties of Selected Random Number Generators}{33}{subsection.6.1.53}
\contentsline {subsection}{\numberline {6.1.54}Properties of Selected Random Number Generators}{33}{subsection.6.1.54}
\contentsline {subsection}{\numberline {6.1.55}Properties of Selected Random Number Generators}{33}{subsection.6.1.55}
\contentsline {subsection}{\numberline {6.1.56}Simple demonstration of RNGs using python}{33}{subsection.6.1.56}
\contentsline {subsection}{\numberline {6.1.57}Properties of Selected Random Number Generators}{34}{subsection.6.1.57}
\contentsline {subsection}{\numberline {6.1.58}Autocorrelation function}{34}{subsection.6.1.58}
\contentsline {subsection}{\numberline {6.1.59}Correlation function and which random number generators should I use}{36}{subsection.6.1.59}
\contentsline {subsection}{\numberline {6.1.60}Which RNG should I use?}{37}{subsection.6.1.60}
\contentsline {subsection}{\numberline {6.1.61}How to use the Mersenne generator}{37}{subsection.6.1.61}
\contentsline {subsection}{\numberline {6.1.62}Why blocking?}{37}{subsection.6.1.62}
\contentsline {subsection}{\numberline {6.1.63}Why blocking?}{38}{subsection.6.1.63}
\contentsline {subsection}{\numberline {6.1.64}Code to demonstrate the calculation of the autocorrelation function}{38}{subsection.6.1.64}
\contentsline {subsection}{\numberline {6.1.65}What is blocking?}{40}{subsection.6.1.65}
\contentsline {subsection}{\numberline {6.1.66}What is blocking?}{40}{subsection.6.1.66}
\contentsline {subsection}{\numberline {6.1.67}What is blocking?}{40}{subsection.6.1.67}
\contentsline {subsection}{\numberline {6.1.68}Implementation}{41}{subsection.6.1.68}
\contentsline {subsection}{\numberline {6.1.69}Actual implementation with code, main function}{41}{subsection.6.1.69}
\contentsline {subsection}{\numberline {6.1.70}The Bootstrap method}{42}{subsection.6.1.70}
\contentsline {subsection}{\numberline {6.1.71}Bootstrapping}{42}{subsection.6.1.71}
\contentsline {subsection}{\numberline {6.1.72}Bootstrapping, recipe}{42}{subsection.6.1.72}
\contentsline {subsection}{\numberline {6.1.73}Bootstrapping, code}{43}{subsection.6.1.73}
\contentsline {subsection}{\numberline {6.1.74}Jackknife, code}{43}{subsection.6.1.74}
\contentsline {section}{\numberline {6.2}Getting started, our first data and Machine Learning encounters}{43}{section.6.2}
\contentsline {subsection}{\numberline {6.2.1}Introduction}{43}{subsection.6.2.1}
\contentsline {subsection}{\numberline {6.2.2}What is Machine Learning?}{44}{subsection.6.2.2}
\contentsline {subsection}{\numberline {6.2.3}Types of Machine Learning}{44}{subsection.6.2.3}
\contentsline {subsection}{\numberline {6.2.4}Software and needed installations}{45}{subsection.6.2.4}
\contentsline {subsection}{\numberline {6.2.5}Python installers}{45}{subsection.6.2.5}
\contentsline {subsection}{\numberline {6.2.6}Useful Python libraries}{46}{subsection.6.2.6}
\contentsline {subsection}{\numberline {6.2.7}Installing R, C++, cython or Julia}{46}{subsection.6.2.7}
\contentsline {subsection}{\numberline {6.2.8}Installing R, C++, cython, Numba etc}{46}{subsection.6.2.8}
\contentsline {subsection}{\numberline {6.2.9}Numpy examples and Important Matrix and vector handling packages}{47}{subsection.6.2.9}
\contentsline {subsection}{\numberline {6.2.10}Basic Matrix Features}{47}{subsection.6.2.10}
\contentsline {subsubsection}{Some famous Matrices}{47}{subsubsection*.3}
\contentsline {subsubsection}{More Basic Matrix Features}{48}{subsubsection*.4}
\contentsline {subsection}{\numberline {6.2.11}Numpy and arrays}{48}{subsection.6.2.11}
\contentsline {subsection}{\numberline {6.2.12}Matrices in Python}{49}{subsection.6.2.12}
\contentsline {subsection}{\numberline {6.2.13}Meet the Pandas}{51}{subsection.6.2.13}
\contentsline {subsection}{\numberline {6.2.14}Reading Data and fitting}{53}{subsection.6.2.14}
\contentsline {subsubsection}{Simple linear regression model using \sphinxstylestrong {scikit\sphinxhyphen {}learn}}{54}{subsubsection*.5}
\contentsline {subsubsection}{To our real data: nuclear binding energies. Brief reminder on masses and binding energies}{58}{subsubsection*.6}
\contentsline {subsubsection}{Organizing our data}{59}{subsubsection*.7}
\contentsline {subsubsection}{Seeing the wood for the trees}{62}{subsubsection*.8}
\contentsline {subsubsection}{And what about using neural networks?}{62}{subsubsection*.9}
\contentsline {subsection}{\numberline {6.2.15}A first summary}{63}{subsection.6.2.15}
\contentsline {section}{\numberline {6.3}Linear Regression and more Advanced Regression Analysis}{63}{section.6.3}
\contentsline {subsection}{\numberline {6.3.1}Why Linear Regression (aka Ordinary Least Squares and family)}{63}{subsection.6.3.1}
\contentsline {subsection}{\numberline {6.3.2}Regression analysis, overarching aims}{64}{subsection.6.3.2}
\contentsline {subsection}{\numberline {6.3.3}Regression analysis, overarching aims II}{64}{subsection.6.3.3}
\contentsline {subsection}{\numberline {6.3.4}Examples}{64}{subsection.6.3.4}
\contentsline {subsection}{\numberline {6.3.5}General linear models}{65}{subsection.6.3.5}
\contentsline {subsection}{\numberline {6.3.6}Rewriting the fitting procedure as a linear algebra problem}{65}{subsection.6.3.6}
\contentsline {subsection}{\numberline {6.3.7}Rewriting the fitting procedure as a linear algebra problem, more details}{65}{subsection.6.3.7}
\contentsline {subsection}{\numberline {6.3.8}Generalizing the fitting procedure as a linear algebra problem}{66}{subsection.6.3.8}
\contentsline {subsection}{\numberline {6.3.9}Generalizing the fitting procedure as a linear algebra problem}{66}{subsection.6.3.9}
\contentsline {subsection}{\numberline {6.3.10}Optimizing our parameters}{66}{subsection.6.3.10}
\contentsline {subsection}{\numberline {6.3.11}Our model for the nuclear binding energies}{67}{subsection.6.3.11}
\contentsline {subsection}{\numberline {6.3.12}Optimizing our parameters, more details}{69}{subsection.6.3.12}
\contentsline {subsection}{\numberline {6.3.13}Interpretations and optimizing our parameters}{69}{subsection.6.3.13}
\contentsline {subsection}{\numberline {6.3.14}Interpretations and optimizing our parameters}{70}{subsection.6.3.14}
\contentsline {subsection}{\numberline {6.3.15}Some useful matrix and vector expressions}{70}{subsection.6.3.15}
\contentsline {subsection}{\numberline {6.3.16}Interpretations and optimizing our parameters}{70}{subsection.6.3.16}
\contentsline {subsection}{\numberline {6.3.17}Own code for Ordinary Least Squares}{71}{subsection.6.3.17}
\contentsline {subsection}{\numberline {6.3.18}Adding error analysis and training set up}{72}{subsection.6.3.18}
\contentsline {subsection}{\numberline {6.3.19}The \(\chi ^2\) function}{72}{subsection.6.3.19}
\contentsline {subsection}{\numberline {6.3.20}The \(\chi ^2\) function}{73}{subsection.6.3.20}
\contentsline {subsection}{\numberline {6.3.21}The \(\chi ^2\) function}{73}{subsection.6.3.21}
\contentsline {subsection}{\numberline {6.3.22}The \(\chi ^2\) function}{73}{subsection.6.3.22}
\contentsline {subsection}{\numberline {6.3.23}The \(\chi ^2\) function}{74}{subsection.6.3.23}
\contentsline {subsection}{\numberline {6.3.24}The \(\chi ^2\) function}{74}{subsection.6.3.24}
\contentsline {subsection}{\numberline {6.3.25}Fitting an Equation of State for Dense Nuclear Matter}{75}{subsection.6.3.25}
\contentsline {subsection}{\numberline {6.3.26}The code}{75}{subsection.6.3.26}
\contentsline {subsection}{\numberline {6.3.27}Splitting our Data in Training and Test data}{77}{subsection.6.3.27}
\contentsline {subsection}{\numberline {6.3.28}The Boston housing data example}{79}{subsection.6.3.28}
\contentsline {subsection}{\numberline {6.3.29}Housing data, the code}{79}{subsection.6.3.29}
\contentsline {subsection}{\numberline {6.3.30}Reducing the number of degrees of freedom, overarching view}{85}{subsection.6.3.30}
\contentsline {subsection}{\numberline {6.3.31}Preprocessing our data}{85}{subsection.6.3.31}
\contentsline {subsection}{\numberline {6.3.32}More preprocessing}{86}{subsection.6.3.32}
\contentsline {subsection}{\numberline {6.3.33}Simple preprocessing examples, Franke function and regression}{86}{subsection.6.3.33}
\contentsline {subsection}{\numberline {6.3.34}The singular value decomposition}{88}{subsection.6.3.34}
\contentsline {subsection}{\numberline {6.3.35}Linear Regression Problems}{88}{subsection.6.3.35}
\contentsline {subsection}{\numberline {6.3.36}Fixing the singularity}{89}{subsection.6.3.36}
\contentsline {subsection}{\numberline {6.3.37}Basic math of the SVD}{89}{subsection.6.3.37}
\contentsline {subsection}{\numberline {6.3.38}The SVD, a Fantastic Algorithm}{90}{subsection.6.3.38}
\contentsline {subsection}{\numberline {6.3.39}Economy\sphinxhyphen {}size SVD}{90}{subsection.6.3.39}
\contentsline {subsection}{\numberline {6.3.40}Codes for the SVD}{91}{subsection.6.3.40}
\contentsline {subsection}{\numberline {6.3.41}Mathematical Properties}{92}{subsection.6.3.41}
\contentsline {subsection}{\numberline {6.3.42}Ridge and LASSO Regression}{92}{subsection.6.3.42}
\contentsline {subsection}{\numberline {6.3.43}More on Ridge Regression}{93}{subsection.6.3.43}
\contentsline {subsection}{\numberline {6.3.44}Interpreting the Ridge results}{94}{subsection.6.3.44}
\contentsline {subsection}{\numberline {6.3.45}More interpretations}{94}{subsection.6.3.45}
\contentsline {subsection}{\numberline {6.3.46}A better understanding of regularization}{95}{subsection.6.3.46}
\contentsline {subsection}{\numberline {6.3.47}Decomposing the OLS and Ridge expressions}{95}{subsection.6.3.47}
\contentsline {subsection}{\numberline {6.3.48}Introducing the Covariance and Correlation functions}{95}{subsection.6.3.48}
\contentsline {subsection}{\numberline {6.3.49}Correlation Function and Design/Feature Matrix}{96}{subsection.6.3.49}
\contentsline {subsection}{\numberline {6.3.50}Covariance Matrix Examples}{96}{subsection.6.3.50}
\contentsline {subsection}{\numberline {6.3.51}Correlation Matrix}{97}{subsection.6.3.51}
\contentsline {subsection}{\numberline {6.3.52}Correlation Matrix with Pandas}{98}{subsection.6.3.52}
\contentsline {subsection}{\numberline {6.3.53}Correlation Matrix with Pandas and the Franke function}{99}{subsection.6.3.53}
\contentsline {subsection}{\numberline {6.3.54}Rewriting the Covariance and/or Correlation Matrix}{100}{subsection.6.3.54}
\contentsline {subsection}{\numberline {6.3.55}Linking with SVD}{101}{subsection.6.3.55}
\contentsline {subsection}{\numberline {6.3.56}Where are we going?}{101}{subsection.6.3.56}
\contentsline {subsection}{\numberline {6.3.57}Resampling methods}{101}{subsection.6.3.57}
\contentsline {subsection}{\numberline {6.3.58}Resampling approaches can be computationally expensive}{101}{subsection.6.3.58}
\contentsline {subsection}{\numberline {6.3.59}Why resampling methods ?}{102}{subsection.6.3.59}
\contentsline {subsection}{\numberline {6.3.60}Statistical analysis}{102}{subsection.6.3.60}
\contentsline {subsection}{\numberline {6.3.61}Linking the regression analysis with a statistical interpretation}{102}{subsection.6.3.61}
\contentsline {subsection}{\numberline {6.3.62}Assumptions made}{102}{subsection.6.3.62}
\contentsline {subsection}{\numberline {6.3.63}Expectation value and variance}{103}{subsection.6.3.63}
\contentsline {subsection}{\numberline {6.3.64}Expectation value and variance for \(\boldsymbol {\beta }\)}{103}{subsection.6.3.64}
\contentsline {subsection}{\numberline {6.3.65}Resampling methods}{104}{subsection.6.3.65}
\contentsline {subsection}{\numberline {6.3.66}Resampling methods: Jackknife and Bootstrap}{105}{subsection.6.3.66}
\contentsline {subsection}{\numberline {6.3.67}Resampling methods: Jackknife}{105}{subsection.6.3.67}
\contentsline {subsection}{\numberline {6.3.68}Jackknife code example}{105}{subsection.6.3.68}
\contentsline {subsection}{\numberline {6.3.69}Resampling methods: Bootstrap}{106}{subsection.6.3.69}
\contentsline {subsection}{\numberline {6.3.70}Resampling methods: Bootstrap background}{106}{subsection.6.3.70}
\contentsline {subsection}{\numberline {6.3.71}Resampling methods: More Bootstrap background}{106}{subsection.6.3.71}
\contentsline {subsection}{\numberline {6.3.72}Resampling methods: Bootstrap approach}{107}{subsection.6.3.72}
\contentsline {subsection}{\numberline {6.3.73}Resampling methods: Bootstrap steps}{107}{subsection.6.3.73}
\contentsline {subsection}{\numberline {6.3.74}Code example for the Bootstrap method}{107}{subsection.6.3.74}
\contentsline {subsection}{\numberline {6.3.75}Various steps in cross\sphinxhyphen {}validation}{110}{subsection.6.3.75}
\contentsline {subsection}{\numberline {6.3.76}How to set up the cross\sphinxhyphen {}validation for Ridge and/or Lasso}{110}{subsection.6.3.76}
\contentsline {subsection}{\numberline {6.3.77}Cross\sphinxhyphen {}validation in brief}{111}{subsection.6.3.77}
\contentsline {subsection}{\numberline {6.3.78}Code Example for Cross\sphinxhyphen {}validation and \(k\)\sphinxhyphen {}fold Cross\sphinxhyphen {}validation}{111}{subsection.6.3.78}
\contentsline {subsection}{\numberline {6.3.79}The bias\sphinxhyphen {}variance tradeoff}{113}{subsection.6.3.79}
\contentsline {subsection}{\numberline {6.3.80}Example code for Bias\sphinxhyphen {}Variance tradeoff}{114}{subsection.6.3.80}
\contentsline {subsection}{\numberline {6.3.81}Understanding what happens}{115}{subsection.6.3.81}
\contentsline {subsection}{\numberline {6.3.82}Summing up}{116}{subsection.6.3.82}
\contentsline {subsection}{\numberline {6.3.83}Another Example from Scikit\sphinxhyphen {}Learn\IeC {\textquoteright }s Repository}{116}{subsection.6.3.83}
\contentsline {subsection}{\numberline {6.3.84}More examples on bootstrap and cross\sphinxhyphen {}validation and errors}{117}{subsection.6.3.84}
\contentsline {subsection}{\numberline {6.3.85}The same example but now with cross\sphinxhyphen {}validation}{119}{subsection.6.3.85}
\contentsline {subsection}{\numberline {6.3.86}Cross\sphinxhyphen {}validation with Ridge}{120}{subsection.6.3.86}
\contentsline {subsection}{\numberline {6.3.87}The Ising model}{121}{subsection.6.3.87}
\contentsline {subsection}{\numberline {6.3.88}Reformulating the problem to suit regression}{122}{subsection.6.3.88}
\contentsline {subsection}{\numberline {6.3.89}Linear regression}{123}{subsection.6.3.89}
\contentsline {subsection}{\numberline {6.3.90}Singular Value decomposition}{123}{subsection.6.3.90}
\contentsline {subsection}{\numberline {6.3.91}The one\sphinxhyphen {}dimensional Ising model}{124}{subsection.6.3.91}
\contentsline {subsection}{\numberline {6.3.92}Ridge regression}{126}{subsection.6.3.92}
\contentsline {subsection}{\numberline {6.3.93}LASSO regression}{127}{subsection.6.3.93}
\contentsline {subsection}{\numberline {6.3.94}Performance as function of the regularization parameter}{127}{subsection.6.3.94}
\contentsline {subsection}{\numberline {6.3.95}Finding the optimal value of \(\lambda \)}{128}{subsection.6.3.95}
\contentsline {section}{\numberline {6.4}Logistic Regression}{129}{section.6.4}
\contentsline {subsection}{\numberline {6.4.1}Introduction}{129}{subsection.6.4.1}
\contentsline {subsection}{\numberline {6.4.2}Basics}{129}{subsection.6.4.2}
\contentsline {subsection}{\numberline {6.4.3}The logistic function}{130}{subsection.6.4.3}
\contentsline {subsection}{\numberline {6.4.4}Two parameters}{133}{subsection.6.4.4}
\contentsline {subsection}{\numberline {6.4.5}Maximum likelihood}{133}{subsection.6.4.5}
\contentsline {subsection}{\numberline {6.4.6}Including more classes}{135}{subsection.6.4.6}
\contentsline {section}{\numberline {6.5}Neural networks, from the simple perceptron to deep learning}{135}{section.6.5}
\contentsline {subsection}{\numberline {6.5.1}To do list}{135}{subsection.6.5.1}
\contentsline {subsection}{\numberline {6.5.2}Neural networks}{136}{subsection.6.5.2}
\contentsline {subsection}{\numberline {6.5.3}Artificial neurons}{136}{subsection.6.5.3}
\contentsline {subsection}{\numberline {6.5.4}Neural network types}{137}{subsection.6.5.4}
\contentsline {subsection}{\numberline {6.5.5}Feed\sphinxhyphen {}forward neural networks}{137}{subsection.6.5.5}
\contentsline {subsection}{\numberline {6.5.6}Convolutional Neural Network}{137}{subsection.6.5.6}
\contentsline {subsection}{\numberline {6.5.7}Recurrent neural networks}{137}{subsection.6.5.7}
\contentsline {subsection}{\numberline {6.5.8}Other types of networks}{138}{subsection.6.5.8}
\contentsline {subsection}{\numberline {6.5.9}Multilayer perceptrons}{138}{subsection.6.5.9}
\contentsline {subsection}{\numberline {6.5.10}Why multilayer perceptrons?}{138}{subsection.6.5.10}
\contentsline {subsection}{\numberline {6.5.11}Mathematical model}{138}{subsection.6.5.11}
\contentsline {subsection}{\numberline {6.5.12}Mathematical model}{138}{subsection.6.5.12}
\contentsline {subsection}{\numberline {6.5.13}Mathematical model}{139}{subsection.6.5.13}
\contentsline {subsection}{\numberline {6.5.14}Mathematical model}{140}{subsection.6.5.14}
\contentsline {subsection}{\numberline {6.5.15}Mathematical model}{140}{subsection.6.5.15}
\contentsline {subsubsection}{Matrix\sphinxhyphen {}vector notation}{141}{subsubsection*.10}
\contentsline {subsubsection}{Matrix\sphinxhyphen {}vector notation and activation}{141}{subsubsection*.11}
\contentsline {subsubsection}{Activation functions}{142}{subsubsection*.12}
\contentsline {subsubsection}{Activation functions, Logistic and Hyperbolic ones}{142}{subsubsection*.13}
\contentsline {subsubsection}{Relevance}{142}{subsubsection*.14}
\contentsline {subsection}{\numberline {6.5.16}The multilayer perceptron (MLP)}{146}{subsection.6.5.16}
\contentsline {subsection}{\numberline {6.5.17}From one to many layers, the universal approximation theorem}{146}{subsection.6.5.17}
\contentsline {subsection}{\numberline {6.5.18}Deriving the back propagation code for a multilayer perceptron model}{146}{subsection.6.5.18}
\contentsline {subsection}{\numberline {6.5.19}Definitions}{147}{subsection.6.5.19}
\contentsline {subsection}{\numberline {6.5.20}Derivatives and the chain rule}{147}{subsection.6.5.20}
\contentsline {subsection}{\numberline {6.5.21}Derivative of the cost function}{147}{subsection.6.5.21}
\contentsline {subsection}{\numberline {6.5.22}Bringing it together, first back propagation equation}{148}{subsection.6.5.22}
\contentsline {subsection}{\numberline {6.5.23}Derivatives in terms of \(z_j^L\)}{148}{subsection.6.5.23}
\contentsline {subsection}{\numberline {6.5.24}Bringing it together}{149}{subsection.6.5.24}
\contentsline {subsection}{\numberline {6.5.25}Final back propagating equation}{150}{subsection.6.5.25}
\contentsline {subsection}{\numberline {6.5.26}Setting up the Back propagation algorithm}{150}{subsection.6.5.26}
\contentsline {subsection}{\numberline {6.5.27}Setting up a Multi\sphinxhyphen {}layer perceptron model for classification}{151}{subsection.6.5.27}
\contentsline {subsection}{\numberline {6.5.28}Defining the cost function}{151}{subsection.6.5.28}
\contentsline {subsection}{\numberline {6.5.29}Example: binary classification problem}{152}{subsection.6.5.29}
\contentsline {subsection}{\numberline {6.5.30}The Softmax function}{152}{subsection.6.5.30}
\contentsline {subsection}{\numberline {6.5.31}Developing a code for doing neural networks with back propagation}{153}{subsection.6.5.31}
\contentsline {subsection}{\numberline {6.5.32}Collect and pre\sphinxhyphen {}process data}{153}{subsection.6.5.32}
\contentsline {subsection}{\numberline {6.5.33}Train and test datasets}{155}{subsection.6.5.33}
\contentsline {subsection}{\numberline {6.5.34}Define model and architecture}{155}{subsection.6.5.34}
\contentsline {subsection}{\numberline {6.5.35}Layers}{156}{subsection.6.5.35}
\contentsline {subsection}{\numberline {6.5.36}Weights and biases}{157}{subsection.6.5.36}
\contentsline {subsection}{\numberline {6.5.37}Feed\sphinxhyphen {}forward pass}{157}{subsection.6.5.37}
\contentsline {subsection}{\numberline {6.5.38}Matrix multiplications}{158}{subsection.6.5.38}
\contentsline {subsection}{\numberline {6.5.39}Choose cost function and optimizer}{159}{subsection.6.5.39}
\contentsline {subsection}{\numberline {6.5.40}Optimizing the cost function}{159}{subsection.6.5.40}
\contentsline {subsection}{\numberline {6.5.41}Regularization}{160}{subsection.6.5.41}
\contentsline {subsection}{\numberline {6.5.42}Matrix multiplication}{160}{subsection.6.5.42}
\contentsline {subsection}{\numberline {6.5.43}Improving performance}{162}{subsection.6.5.43}
\contentsline {subsection}{\numberline {6.5.44}Full object\sphinxhyphen {}oriented implementation}{163}{subsection.6.5.44}
\contentsline {subsection}{\numberline {6.5.45}Evaluate model performance on test data}{165}{subsection.6.5.45}
\contentsline {subsection}{\numberline {6.5.46}Adjust hyperparameters}{165}{subsection.6.5.46}
\contentsline {subsection}{\numberline {6.5.47}Visualization}{170}{subsection.6.5.47}
\contentsline {subsection}{\numberline {6.5.48}scikit\sphinxhyphen {}learn implementation}{170}{subsection.6.5.48}
\contentsline {subsection}{\numberline {6.5.49}Visualization}{171}{subsection.6.5.49}
\contentsline {subsection}{\numberline {6.5.50}Building neural networks in Tensorflow and Keras}{172}{subsection.6.5.50}
\contentsline {subsection}{\numberline {6.5.51}Tensorflow}{172}{subsection.6.5.51}
\contentsline {subsection}{\numberline {6.5.52}Collect and pre\sphinxhyphen {}process data}{172}{subsection.6.5.52}
\contentsline {subsection}{\numberline {6.5.53}Using TensorFlow backend}{173}{subsection.6.5.53}
\contentsline {subsection}{\numberline {6.5.54}Optimizing and using gradient descent}{176}{subsection.6.5.54}
\contentsline {subsection}{\numberline {6.5.55}Using Keras}{177}{subsection.6.5.55}
\contentsline {subsection}{\numberline {6.5.56}Which activation function should I use?}{179}{subsection.6.5.56}
\contentsline {subsection}{\numberline {6.5.57}Is the Logistic activation function (Sigmoid) our choice?}{179}{subsection.6.5.57}
\contentsline {subsection}{\numberline {6.5.58}The derivative of the Logistic funtion}{179}{subsection.6.5.58}
\contentsline {subsection}{\numberline {6.5.59}The RELU function family}{180}{subsection.6.5.59}
\contentsline {subsection}{\numberline {6.5.60}Which activation function should we use?}{180}{subsection.6.5.60}
\contentsline {subsection}{\numberline {6.5.61}A top\sphinxhyphen {}down perspective on Neural networks}{180}{subsection.6.5.61}
\contentsline {subsection}{\numberline {6.5.62}Limitations of supervised learning with deep networks}{181}{subsection.6.5.62}
\contentsline {section}{\numberline {6.6}Support Vector Machines, overarching aims}{181}{section.6.6}
\contentsline {subsection}{\numberline {6.6.1}Hyperplanes and all that}{182}{subsection.6.6.1}
\contentsline {subsection}{\numberline {6.6.2}What is a hyperplane?}{183}{subsection.6.6.2}
\contentsline {subsection}{\numberline {6.6.3}A \(p\)\sphinxhyphen {}dimensional space of features}{184}{subsection.6.6.3}
\contentsline {subsection}{\numberline {6.6.4}The two\sphinxhyphen {}dimensional case}{184}{subsection.6.6.4}
\contentsline {subsection}{\numberline {6.6.5}Getting into the details}{185}{subsection.6.6.5}
\contentsline {subsection}{\numberline {6.6.6}First attempt at a minimization approach}{185}{subsection.6.6.6}
\contentsline {subsection}{\numberline {6.6.7}Solving the equations}{185}{subsection.6.6.7}
\contentsline {subsection}{\numberline {6.6.8}Code Example}{185}{subsection.6.6.8}
\contentsline {subsection}{\numberline {6.6.9}Problems with the Simpler Approach}{186}{subsection.6.6.9}
\contentsline {subsection}{\numberline {6.6.10}A better approach}{186}{subsection.6.6.10}
\contentsline {subsection}{\numberline {6.6.11}A quick Reminder on Lagrangian Multipliers}{186}{subsection.6.6.11}
\contentsline {subsection}{\numberline {6.6.12}Adding the Multiplier}{187}{subsection.6.6.12}
\contentsline {subsection}{\numberline {6.6.13}Setting up the Problem}{187}{subsection.6.6.13}
\contentsline {subsection}{\numberline {6.6.14}The problem to solve}{188}{subsection.6.6.14}
\contentsline {subsection}{\numberline {6.6.15}The last steps}{188}{subsection.6.6.15}
\contentsline {subsection}{\numberline {6.6.16}A soft classifier}{189}{subsection.6.6.16}
\contentsline {subsection}{\numberline {6.6.17}Soft optmization problem}{189}{subsection.6.6.17}
\contentsline {subsection}{\numberline {6.6.18}Kernels and non\sphinxhyphen {}linearity}{190}{subsection.6.6.18}
\contentsline {subsection}{\numberline {6.6.19}The equations}{192}{subsection.6.6.19}
\contentsline {subsection}{\numberline {6.6.20}The problem to solve}{192}{subsection.6.6.20}
\contentsline {subsection}{\numberline {6.6.21}Different kernels and Mercer\IeC {\textquoteright }s theorem}{193}{subsection.6.6.21}
\contentsline {subsection}{\numberline {6.6.22}The moons example}{193}{subsection.6.6.22}
\contentsline {subsection}{\numberline {6.6.23}Mathematical optimization of convex functions}{199}{subsection.6.6.23}
\contentsline {subsection}{\numberline {6.6.24}How do we solve these problems?}{199}{subsection.6.6.24}
\contentsline {subsection}{\numberline {6.6.25}A simple example}{200}{subsection.6.6.25}
\contentsline {subsection}{\numberline {6.6.26}Back to the more realistic cases}{201}{subsection.6.6.26}
\contentsline {section}{\numberline {6.7}Dimensionality Reduction}{201}{section.6.7}
\contentsline {subsection}{\numberline {6.7.1}Reducing the number of degrees of freedom, overarching view}{201}{subsection.6.7.1}
\contentsline {subsection}{\numberline {6.7.2}Preprocessing our data}{202}{subsection.6.7.2}
\contentsline {subsection}{\numberline {6.7.3}More preprocessing}{202}{subsection.6.7.3}
\contentsline {subsection}{\numberline {6.7.4}Simple preprocessing examples, Franke function and regression}{202}{subsection.6.7.4}
\contentsline {subsection}{\numberline {6.7.5}Simple preprocessing examples, breast cancer data and classification, Support Vector Machines}{204}{subsection.6.7.5}
\contentsline {subsection}{\numberline {6.7.6}More on Cancer Data, now with Logistic Regression}{206}{subsection.6.7.6}
\contentsline {subsection}{\numberline {6.7.7}Why should we think of reducing the dimensionality}{207}{subsection.6.7.7}
\contentsline {subsection}{\numberline {6.7.8}Basic ideas of the Principal Component Analysis (PCA)}{210}{subsection.6.7.8}
\contentsline {subsection}{\numberline {6.7.9}Introducing the Covariance and Correlation functions}{210}{subsection.6.7.9}
\contentsline {subsection}{\numberline {6.7.10}Correlation Function and Design/Feature Matrix}{211}{subsection.6.7.10}
\contentsline {subsection}{\numberline {6.7.11}Covariance Matrix Examples}{211}{subsection.6.7.11}
\contentsline {subsection}{\numberline {6.7.12}Correlation Matrix}{212}{subsection.6.7.12}
\contentsline {subsection}{\numberline {6.7.13}Correlation Matrix with Pandas}{213}{subsection.6.7.13}
\contentsline {subsection}{\numberline {6.7.14}Correlation Matrix with Pandas and the Franke function}{214}{subsection.6.7.14}
\contentsline {subsection}{\numberline {6.7.15}Rewriting the Covariance and/or Correlation Matrix}{215}{subsection.6.7.15}
\contentsline {subsection}{\numberline {6.7.16}Towards the PCA theorem}{216}{subsection.6.7.16}
\contentsline {subsection}{\numberline {6.7.17}The Algorithm before theorem}{216}{subsection.6.7.17}
\contentsline {subsection}{\numberline {6.7.18}Writing our own PCA code}{217}{subsection.6.7.18}
\contentsline {subsubsection}{Compute the sample mean and center the data}{217}{subsubsection*.15}
\contentsline {subsubsection}{Compute the sample covariance}{218}{subsubsection*.16}
\contentsline {subsubsection}{Diagonalize the sample covariance matrix to obtain the principal components}{219}{subsubsection*.17}
\contentsline {subsection}{\numberline {6.7.19}Classical PCA Theorem}{220}{subsection.6.7.19}
\contentsline {subsection}{\numberline {6.7.20}Proof of the PCA Theorem}{221}{subsection.6.7.20}
\contentsline {subsection}{\numberline {6.7.21}PCA Proof continued}{221}{subsection.6.7.21}
\contentsline {subsection}{\numberline {6.7.22}The final step}{221}{subsection.6.7.22}
\contentsline {subsection}{\numberline {6.7.23}Geometric Interpretation and link with Singular Value Decomposition}{222}{subsection.6.7.23}
\contentsline {subsection}{\numberline {6.7.24}Principal Component Analysis}{222}{subsection.6.7.24}
\contentsline {subsection}{\numberline {6.7.25}PCA and scikit\sphinxhyphen {}learn}{224}{subsection.6.7.25}
\contentsline {subsection}{\numberline {6.7.26}Back to the Cancer Data}{224}{subsection.6.7.26}
\contentsline {subsection}{\numberline {6.7.27}More on the PCA}{225}{subsection.6.7.27}
\contentsline {subsection}{\numberline {6.7.28}Incremental PCA}{226}{subsection.6.7.28}
\contentsline {subsection}{\numberline {6.7.29}Randomized PCA}{226}{subsection.6.7.29}
\contentsline {subsection}{\numberline {6.7.30}Kernel PCA}{226}{subsection.6.7.30}
\contentsline {subsection}{\numberline {6.7.31}LLE}{226}{subsection.6.7.31}
\contentsline {subsection}{\numberline {6.7.32}Other techniques}{226}{subsection.6.7.32}

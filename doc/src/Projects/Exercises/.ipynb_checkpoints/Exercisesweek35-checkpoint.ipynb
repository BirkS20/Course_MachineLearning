{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "183246e3",
   "metadata": {},
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html Exercisesweek35.do.txt  -->\n",
    "<!-- dom:TITLE: Exercises week 35 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ef9d73",
   "metadata": {},
   "source": [
    "# Exercises week 35\n",
    "**Morten Hjorth-Jensen**, Department of Physics, University of Oslo and Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University\n",
    "\n",
    "Date: **Aug 30, 2022**\n",
    "\n",
    "Copyright 1999-2022, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7ab0d9",
   "metadata": {},
   "source": [
    "## Exercises for week 35\n",
    "\n",
    "The exercises here are meant to prepare you for work with project 1. The first exercise is a follow-up of exercise 2 from week 35 August 30-September 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90896979",
   "metadata": {},
   "source": [
    "## Exercise 1: Setting up various Python environments\n",
    "\n",
    "The first exercise here is of a mere technical art. We want you to have \n",
    "* git as a version control software and to establish a user account on a provider like GitHub. Other providers like GitLab etc are equally fine. You can also use the University of Oslo [GitHub facilities](https://www.uio.no/tjenester/it/maskin/filer/versjonskontroll/github.html). \n",
    "\n",
    "* Install various Python packages\n",
    "\n",
    "We will make extensive use of Python as programming language and its\n",
    "myriad of available libraries.  You will find\n",
    "IPython/Jupyter notebooks invaluable in your work.  You can run **R**\n",
    "codes in the Jupyter/IPython notebooks, with the immediate benefit of\n",
    "visualizing your data. You can also use compiled languages like C++,\n",
    "Rust, Fortran etc if you prefer. The focus in these lectures will be\n",
    "on Python.\n",
    "\n",
    "If you have Python installed (we recommend Python3) and you feel\n",
    "pretty familiar with installing different packages, we recommend that\n",
    "you install the following Python packages via **pip** as \n",
    "\n",
    "1. pip install numpy scipy matplotlib ipython scikit-learn sympy pandas pillow \n",
    "\n",
    "For **Tensorflow**, we recommend following the instructions in the text of \n",
    "[Aurelien Geron, Hands‑On Machine Learning with Scikit‑Learn and TensorFlow, O'Reilly](http://shop.oreilly.com/product/0636920052289.do)\n",
    "\n",
    "We will come back to **tensorflow** later. \n",
    "\n",
    "For Python3, replace **pip** with **pip3**.\n",
    "\n",
    "For OSX users we recommend, after having installed Xcode, to\n",
    "install **brew**. Brew allows for a seamless installation of additional\n",
    "software via for example \n",
    "\n",
    "1. brew install python3\n",
    "\n",
    "For Linux users, with its variety of distributions like for example the widely popular Ubuntu distribution,\n",
    "you can use **pip** as well and simply install Python as \n",
    "\n",
    "1. sudo apt-get install python3  (or python for Python2.7)\n",
    "\n",
    "If you don't want to perform these operations separately and venture\n",
    "into the hassle of exploring how to set up dependencies and paths, we\n",
    "recommend two widely used distrubutions which set up all relevant\n",
    "dependencies for Python, namely \n",
    "\n",
    "* [Anaconda](https://docs.anaconda.com/), \n",
    "\n",
    "which is an open source\n",
    "distribution of the Python and R programming languages for large-scale\n",
    "data processing, predictive analytics, and scientific computing, that\n",
    "aims to simplify package management and deployment. Package versions\n",
    "are managed by the package management system **conda**. \n",
    "\n",
    "* [Enthought canopy](https://www.enthought.com/product/canopy/) \n",
    "\n",
    "is a Python\n",
    "distribution for scientific and analytic computing distribution and\n",
    "analysis environment, available for free and under a commercial\n",
    "license.\n",
    "\n",
    "We recommend using **Anaconda** if you are not too familiar with setting paths in a terminal environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c71fe5",
   "metadata": {},
   "source": [
    "## Exercise 2: making your own data and exploring scikit-learn\n",
    "\n",
    "We will generate our own dataset for a function $y(x)$ where $x \\in [0,1]$ and defined by random numbers computed with the uniform distribution. The function $y$ is a quadratic polynomial in $x$ with added stochastic noise according to the normal distribution $\\cal {N}(0,1)$.\n",
    "The following simple Python instructions define our $x$ and $y$ values (with 100 data points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f45655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(100,1)\n",
    "y = 2.0+5*x*x+0.1*np.random.randn(100,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d6fa8a",
   "metadata": {},
   "source": [
    "1. Write your own code (following the examples under the [regression notes](https://compphysics.github.io/MachineLearning/doc/LectureNotes/_build/html/chapter1.html)) for computing the parametrization of the data set fitting a second-order polynomial. \n",
    "\n",
    "2. Use thereafter **scikit-learn** (see again the examples in the regression slides) and compare with your own code.   When compairing with _scikit_learn_, make sure you set the option for the intercept to **FALSE**, see <https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html>. This feature will be explained in more detail during the lectures of week 35 and week 36. You can find more in <https://compphysics.github.io/MachineLearning/doc/LectureNotes/_build/html/chapter3.html#more-on-rescaling-data>.\n",
    "\n",
    "3. Using scikit-learn, compute also the mean square error, a risk metric corresponding to the expected value of the squared (quadratic) error defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4b8013",
   "metadata": {},
   "source": [
    "$$\n",
    "MSE(\\boldsymbol{y},\\boldsymbol{\\tilde{y}}) = \\frac{1}{n}\n",
    "\\sum_{i=0}^{n-1}(y_i-\\tilde{y}_i)^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a719548d",
   "metadata": {},
   "source": [
    "and the $R^2$ score function.\n",
    "If $\\tilde{\\boldsymbol{y}}_i$ is the predicted value of the $i-th$ sample and $y_i$ is the corresponding true value, then the score $R^2$ is defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7a2f27",
   "metadata": {},
   "source": [
    "$$\n",
    "R^2(\\boldsymbol{y}, \\tilde{\\boldsymbol{y}}) = 1 - \\frac{\\sum_{i=0}^{n - 1} (y_i - \\tilde{y}_i)^2}{\\sum_{i=0}^{n - 1} (y_i - \\bar{y})^2},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50431786",
   "metadata": {},
   "source": [
    "where we have defined the mean value  of $\\boldsymbol{y}$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c423648a",
   "metadata": {},
   "source": [
    "$$\n",
    "\\bar{y} =  \\frac{1}{n} \\sum_{i=0}^{n - 1} y_i.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de4c37a",
   "metadata": {},
   "source": [
    "You can use the functionality included in scikit-learn. If you feel for it, you can use your own program and define functions which compute the above two functions. \n",
    "Discuss the meaning of these results. Try also to vary the coefficient in front of the added stochastic noise term and discuss the quality of the fits.\n",
    "\n",
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**\n",
    "The code here is an example of where we define our own design matrix and fit parameters $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5877919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def save_fig(fig_id):\n",
    "    plt.savefig(image_path(fig_id) + \".png\", format='png')\n",
    "\n",
    "def R2(y_data, y_model):\n",
    "    return 1 - np.sum((y_data - y_model) ** 2) / np.sum((y_data - np.mean(y_data)) ** 2)\n",
    "def MSE(y_data,y_model):\n",
    "    n = np.size(y_model)\n",
    "    return np.sum((y_data-y_model)**2)/n\n",
    "\n",
    "x = np.random.rand(100)\n",
    "y = 2.0+5*x*x+0.1*np.random.randn(100)\n",
    "\n",
    "\n",
    "#  The design matrix now as function of a given polynomial\n",
    "X = np.zeros((len(x),3))\n",
    "X[:,0] = 1.0\n",
    "X[:,1] = x\n",
    "X[:,2] = x**2\n",
    "# We split the data in test and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "# matrix inversion to find beta\n",
    "beta = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ y_train\n",
    "print(beta)\n",
    "# and then make the prediction\n",
    "ytilde = X_train @ beta\n",
    "print(\"Training R2\")\n",
    "print(R2(y_train,ytilde))\n",
    "print(\"Training MSE\")\n",
    "print(MSE(y_train,ytilde))\n",
    "ypredict = X_test @ beta\n",
    "print(\"Test R2\")\n",
    "print(R2(y_test,ypredict))\n",
    "print(\"Test MSE\")\n",
    "print(MSE(y_test,ypredict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c46a53",
   "metadata": {},
   "source": [
    "<!-- --- end solution of exercise --- -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9cf150",
   "metadata": {},
   "source": [
    "## Exercise 3: Normalizing our data\n",
    "\n",
    "A much used approach before starting to train the data is  to preprocess our\n",
    "data. Normally the data may need a rescaling and/or may be sensitive\n",
    "to extreme values. Scaling the data renders our inputs much more\n",
    "suitable for the algorithms we want to employ.\n",
    "\n",
    "**Scikit-Learn** has several functions which allow us to rescale the\n",
    "data, normally resulting in much better results in terms of various\n",
    "accuracy scores.  The **StandardScaler** function in **Scikit-Learn**\n",
    "ensures that for each feature/predictor we study the mean value is\n",
    "zero and the variance is one (every column in the design/feature\n",
    "matrix).  This scaling has the drawback that it does not ensure that\n",
    "we have a particular maximum or minimum in our data set. Another\n",
    "function included in **Scikit-Learn** is the **MinMaxScaler** which\n",
    "ensures that all features are exactly between $0$ and $1$. The\n",
    "\n",
    "The **Normalizer** scales each data\n",
    "point such that the feature vector has a euclidean length of one. In other words, it\n",
    "projects a data point on the circle (or sphere in the case of higher dimensions) with a\n",
    "radius of 1. This means every data point is scaled by a different number (by the\n",
    "inverse of it’s length).\n",
    "This normalization is often used when only the direction (or angle) of the data matters,\n",
    "not the length of the feature vector.\n",
    "\n",
    "The **RobustScaler** works similarly to the StandardScaler in that it\n",
    "ensures statistical properties for each feature that guarantee that\n",
    "they are on the same scale. However, the RobustScaler uses the median\n",
    "and quartiles, instead of mean and variance. This makes the\n",
    "RobustScaler ignore data points that are very different from the rest\n",
    "(like measurement errors). These odd data points are also called\n",
    "outliers, and might often lead to trouble for other scaling\n",
    "techniques.\n",
    "\n",
    "It also common to split the data in a **training** set and a **testing** set. A typical split is to use $80\\%$ of the data for training and the rest\n",
    "for testing. This can be done as follows with our design matrix $\\boldsymbol{X}$ and data $\\boldsymbol{y}$ (remember to import **scikit-learn**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e944be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceecc12",
   "metadata": {},
   "source": [
    "Then we can use the standard scaler to scale our data as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f85e4fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b49308",
   "metadata": {},
   "source": [
    "In this exercise we want you to to compute the MSE for the training\n",
    "data and the test data as function of the complexity of a polynomial,\n",
    "that is the degree of a given polynomial. We want you also to compute the $R2$ score as function of the complexity of the model for both training data and test data.  You should also run the calculation with and without scaling. \n",
    "\n",
    "One of \n",
    "the aims is to reproduce Figure 2.11 of [Hastie et al](https://github.com/CompPhysics/MLErasmus/blob/master/doc/Textbooks/elementsstat.pdf).\n",
    "\n",
    "Our data is defined by $x\\in [-3,3]$ with a total of for example $100$ data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e7aff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed()\n",
    "n = 100\n",
    "maxdegree = 14\n",
    "# Make data set.\n",
    "x = np.linspace(-3, 3, n).reshape(-1, 1)\n",
    "y = np.exp(-x**2) + 1.5 * np.exp(-(x-2)**2)+ np.random.normal(0, 0.1, x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e814b9ad",
   "metadata": {},
   "source": [
    "where $y$ is the function we want to fit with a given polynomial.\n",
    "\n",
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**\n",
    "We present here the solution for the last exercise. All elements here can be used to solve exercises a) and b) as well.\n",
    "Note that in this example we have used the polynomial fitting functions of **scikit-learn**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2f3f2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "np.random.seed(2018)\n",
    "n = 30\n",
    "maxdegree = 14\n",
    "# Make data set.\n",
    "x = np.linspace(-3, 3, n).reshape(-1, 1)\n",
    "y = np.exp(-x**2) + 1.5 * np.exp(-(x-2)**2)+ np.random.normal(0, 0.1, x.shape)\n",
    "TestError = np.zeros(maxdegree)\n",
    "TrainError = np.zeros(maxdegree)\n",
    "polydegree = np.zeros(maxdegree)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "\n",
    "for degree in range(maxdegree):\n",
    "    model = make_pipeline(PolynomialFeatures(degree=degree), LinearRegression(fit_intercept=False))\n",
    "    clf = model.fit(x_train,y_train)\n",
    "    y_fit = clf.predict(x_train)\n",
    "    y_pred = clf.predict(x_test) \n",
    "    polydegree[degree] = degree\n",
    "    TestError[degree] = np.mean( np.mean((y_test - y_pred)**2) )\n",
    "    TrainError[degree] = np.mean( np.mean((y_train - y_fit)**2) )\n",
    "\n",
    "plt.plot(polydegree, TestError, label='Test Error')\n",
    "plt.plot(polydegree, TrainError, label='Train Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579a3858",
   "metadata": {},
   "source": [
    "<!-- --- end solution of exercise --- -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43068ed2",
   "metadata": {},
   "source": [
    "**a)**\n",
    "Write a first code which sets up a design matrix $X$ defined by a fifth-order polynomial.  Scale your data and split it in training and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cbbec3",
   "metadata": {},
   "source": [
    "**b)**\n",
    "Perform an ordinary least squares and compute the means squared error and the $R2$ factor for the training data and the test data, with and without scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a10c8c",
   "metadata": {},
   "source": [
    "**c)**\n",
    "Add now a model which allows you to make polynomials up to degree $15$.  Perform a standard OLS fitting of the training data and compute the MSE and $R2$ for the training and test data and plot both test and training data MSE and $R2$ as functions of the polynomial degree. Compare what you see with Figure 2.11 of Hastie et al. Comment your results. For which polynomial degree do you find an optimal MSE (smallest value)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65d87f6",
   "metadata": {},
   "source": [
    "## Exercise 4: Adding Ridge Regression\n",
    "\n",
    "This exercise is a continuation of exercise 2. We will use the same function to\n",
    "generate our data set, still staying with a simple function $y(x)$\n",
    "which we want to fit using linear regression, but now extending the\n",
    "analysis to include the Ridge regression method.\n",
    "\n",
    "We will thus again generate our own dataset for a function $y(x)$ where \n",
    "$x \\in [0,1]$ and defined by random numbers computed with the uniform\n",
    "distribution. The function $y$ is a quadratic polynomial in $x$ with\n",
    "added stochastic noise according to the normal distribution $\\cal{N}(0,1)$.\n",
    "\n",
    "The following simple Python instructions define our $x$ and $y$ values (with 100 data points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90c894cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(100)\n",
    "y = 2.0+5*x*x+0.1*np.random.randn(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2972685a",
   "metadata": {},
   "source": [
    "Write your own code for the Ridge method (see chapter 3.4 of Hastie *et al.*, equations (3.43) and (3.44)) and compute the parametrization for different values of $\\lambda$. Compare and analyze your results with those from exercise 3. Study the dependence on $\\lambda$ while also varying the strength of the noise in your expression for $y(x)$. \n",
    "\n",
    "Repeat the above but using the functionality of\n",
    "**Scikit-Learn**. Compare your code with the results from\n",
    "**Scikit-Learn**. Remember to run with the same random numbers for\n",
    "generating $x$ and $y$.  Observe also that when you compare with **Scikit-Learn**, you need to pay attention to how the intercept is dealt with.\n",
    "\n",
    "Finally, using **Scikit-Learn** or your own code, compute also the mean square error, a risk metric corresponding to the expected value of the squared (quadratic) error defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dc6da2",
   "metadata": {},
   "source": [
    "$$\n",
    "MSE(\\hat{y},\\hat{\\tilde{y}}) = \\frac{1}{n}\n",
    "\\sum_{i=0}^{n-1}(y_i-\\tilde{y}_i)^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b8dd61",
   "metadata": {},
   "source": [
    "and the $R^2$ score function.\n",
    "If $\\tilde{\\hat{y}}_i$ is the predicted value of the $i-th$ sample and $y_i$ is the corresponding true value, then the score $R^2$ is defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ad51e4",
   "metadata": {},
   "source": [
    "$$\n",
    "R^2(\\hat{y}, \\tilde{\\hat{y}}) = 1 - \\frac{\\sum_{i=0}^{n - 1} (y_i - \\tilde{y}_i)^2}{\\sum_{i=0}^{n - 1} (y_i - \\bar{y})^2},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634d4494",
   "metadata": {},
   "source": [
    "where we have defined the mean value  of $\\hat{y}$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9c856a",
   "metadata": {},
   "source": [
    "$$\n",
    "\\bar{y} =  \\frac{1}{n} \\sum_{i=0}^{n - 1} y_i.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4c2d5a",
   "metadata": {},
   "source": [
    "Discuss these quantities as functions of the variable $\\lambda$ in Ridge regression.\n",
    "\n",
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**\n",
    "The code here allows you to perform your own Ridge calculation and\n",
    "perform calculations for various values of the regularization\n",
    "parameter $\\lambda$. This program can easily be extended upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87b8b63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.03099776 -0.17917768  5.18029127]\n",
      "Training R2 for OLS\n",
      "0.9947777005941899\n",
      "Training MSE for OLS\n",
      "0.00916347050835222\n",
      "Test R2 for OLS\n",
      "0.9959114574014767\n",
      "Test MSE OLS\n",
      "0.008675369724976584\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "\n",
    "def R2(y_data, y_model):\n",
    "    return 1 - np.sum((y_data - y_model) ** 2) / np.sum((y_data - np.mean(y_data)) ** 2)\n",
    "def MSE(y_data,y_model):\n",
    "    n = np.size(y_model)\n",
    "    return np.sum((y_data-y_model)**2)/n\n",
    "\n",
    "\n",
    "# A seed just to ensure that the random numbers are the same for every run.\n",
    "# Useful for eventual debugging.\n",
    "np.random.seed(3155)\n",
    "\n",
    "x = np.random.rand(100)\n",
    "y = 2.0+5*x*x+0.1*np.random.randn(100)\n",
    "\n",
    "# number of features p (here degree of polynomial\n",
    "p = 3\n",
    "#  The design matrix now as function of a given polynomial\n",
    "X = np.zeros((len(x),p))\n",
    "X[:,0] = 1.0\n",
    "X[:,1] = x\n",
    "X[:,2] = x*x\n",
    "# We split the data in test and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# matrix inversion to find beta\n",
    "OLSbeta = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ y_train\n",
    "print(OLSbeta)\n",
    "# and then make the prediction\n",
    "ytildeOLS = X_train @ OLSbeta\n",
    "print(\"Training R2 for OLS\")\n",
    "print(R2(y_train,ytildeOLS))\n",
    "print(\"Training MSE for OLS\")\n",
    "print(MSE(y_train,ytildeOLS))\n",
    "ypredictOLS = X_test @ OLSbeta\n",
    "print(\"Test R2 for OLS\")\n",
    "print(R2(y_test,ypredictOLS))\n",
    "print(\"Test MSE OLS\")\n",
    "print(MSE(y_test,ypredictOLS))\n",
    "\n",
    "\n",
    "# Repeat now for Ridge regression and various values of the regularization parameter\n",
    "I = np.eye(p,p)\n",
    "# Decide which values of lambda to use\n",
    "nlambdas = 20\n",
    "OwnMSEPredict = np.zeros(nlambdas)\n",
    "OwnMSETrain = np.zeros(nlambdas)\n",
    "MSERidgePredict =  np.zeros(nlambdas)\n",
    "lambdas = np.logspace(-4, 1, nlambdas)\n",
    "for i in range(nlambdas):\n",
    "    lmb = lambdas[i]\n",
    "    OwnRidgebeta = np.linalg.inv(X_train.T @ X_train+lmb*I) @ X_train.T @ y_train\n",
    "    # and then make the prediction\n",
    "    OwnytildeRidge = X_train @ OwnRidgebeta\n",
    "    OwnypredictRidge = X_test @ OwnRidgebeta\n",
    "    OwnMSEPredict[i] = MSE(y_test,OwnypredictRidge)\n",
    "    OwnMSETrain[i] = MSE(y_train,OwnytildeRidge)\n",
    "    # Make the fit using Ridge from Sklearn\n",
    "    RegRidge = linear_model.Ridge(lmb,fit_intercept=True)\n",
    "    RegRidge.fit(X_train,y_train)\n",
    "    # and then make the prediction\n",
    "    ypredictRidge = RegRidge.predict(X_test)\n",
    "    # Compute the MSE and print it\n",
    "    MSERidgePredict[i] = MSE(y_test,ypredictRidge)\n",
    "\n",
    "# Now plot the results\n",
    "plt.figure()\n",
    "plt.plot(np.log10(lambdas), OwnMSETrain, label = 'MSE Ridge train, Own code')\n",
    "plt.plot(np.log10(lambdas), OwnMSEPredict, 'r--', label = 'MSE Ridge Test, Own code')\n",
    "plt.plot(np.log10(lambdas), MSERidgePredict, 'g--', label = 'MSE Ridge Test, Sklearn code')\n",
    "plt.xlabel('log10(lambda)')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d52c72",
   "metadata": {},
   "source": [
    "<!-- --- end solution of exercise --- -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f051f8",
   "metadata": {},
   "source": [
    "## Exercise 5: Analytical exercises\n",
    "\n",
    "In this exercise we derive the expressions for various derivatives of\n",
    "products of vectors and matrices. Such derivatives are central to the\n",
    "optimization of various cost functions. Although we will often use\n",
    "automatic differentiation in actual calculations, to be able to have\n",
    "analytical expressions is extremely helpful in case we have simpler\n",
    "derivatives as well as when we analyze various properties (like second\n",
    "derivatives) of the chosen cost functions.  Vectors are always written\n",
    "as boldfaced lower case letters and matrices as upper case boldfaced\n",
    "letters.\n",
    "\n",
    "Show that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83557a92",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial (\\boldsymbol{b}^T\\boldsymbol{a})}{\\partial \\boldsymbol{a}} = \\boldsymbol{b},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89e2123",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3879d604",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial (\\boldsymbol{a}^T\\boldsymbol{A}\\boldsymbol{a})}{\\partial \\boldsymbol{a}} = (\\boldsymbol{A}+\\boldsymbol{A}^T)\\boldsymbol{a},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d2f8f4",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164cfcc4",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\left(\\boldsymbol{x}-\\boldsymbol{A}\\boldsymbol{s}\\right)^T\\left(\\boldsymbol{x}-\\boldsymbol{A}\\boldsymbol{s}\\right)}{\\partial \\boldsymbol{s}} = -2\\left(\\boldsymbol{x}-\\boldsymbol{A}\\boldsymbol{s}\\right)^T\\boldsymbol{A},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7581bc8",
   "metadata": {},
   "source": [
    "and finally find the second derivative of this function with respect to the vector $\\boldsymbol{s}$.\n",
    "\n",
    "**Hint**: In these exercises it is always useful to write out with summation indices the various quantities.\n",
    "As an example, consider the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba345019",
   "metadata": {},
   "source": [
    "$$\n",
    "f(\\boldsymbol{x}) =\\boldsymbol{A}\\boldsymbol{x},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83752696",
   "metadata": {},
   "source": [
    "which reads for a specific component $f_i$ (we define the matrix $\\boldsymbol{A}$ to have dimension $n\\times n$ and the vector $\\boldsymbol{x}$ to have length $n$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1127005",
   "metadata": {},
   "source": [
    "$$\n",
    "f_i =\\sum_{j=0}^{n-1}a_{ij}x_j,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73536022",
   "metadata": {},
   "source": [
    "which leads to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f691f124",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial f_i}{\\partial x_j}= a_{ij},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accc72b6",
   "metadata": {},
   "source": [
    "and written out in terms of the vector $\\boldsymbol{x}$ we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80706772",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial f(\\boldsymbol{x})}{\\partial \\boldsymbol{x}}= \\boldsymbol{A}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ba2b0d",
   "metadata": {},
   "source": [
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**\n",
    "For the first derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5678a9",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial (\\boldsymbol{b}^T\\boldsymbol{a})}{\\partial \\boldsymbol{a}} = \\boldsymbol{b},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50be3447",
   "metadata": {},
   "source": [
    "we can write out the inner product as (assuming all elements are real)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0d4080",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{b}^T\\boldsymbol{a}=\\sum_i b_ia_i,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f018905",
   "metadata": {},
   "source": [
    "taking the derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a613eb18",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\left( \\sum_i b_ia_i\\right)}{\\partial a_k}= b_k,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f20f0a3",
   "metadata": {},
   "source": [
    "leading to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa37e6ce",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\boldsymbol{b}^T\\boldsymbol{a}}{\\partial \\boldsymbol{a}}= \\begin{bmatrix} b_0 \\\\ b_1 \\\\ b_2 \\\\ \\dots \\\\ \\dots \\\\ b_{n-1}\\end{bmatrix} = \\boldsymbol{b}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b416e7",
   "metadata": {},
   "source": [
    "For the second exercise we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca90f81",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial (\\boldsymbol{a}^T\\boldsymbol{A}\\boldsymbol{a})}{\\partial \\boldsymbol{a}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3cadb3",
   "metadata": {},
   "source": [
    "Defining a vector $\\boldsymbol{f}=\\boldsymbol{A}\\boldsymbol{a}$ with components $f_i=\\sum_ja_{ij}a_i$  we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cf6b64",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial (\\boldsymbol{a}^T\\boldsymbol{f})}{\\partial \\boldsymbol{a}}=\\boldsymbol{a}^T\\boldsymbol{A}+\\boldsymbol{f}^T=\\boldsymbol{a}^T\\left(\\boldsymbol{A}+\\boldsymbol{A}^T\\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a20a20f",
   "metadata": {},
   "source": [
    "since $f$ depends on $a$ and we have used the chain rule for derivatives on the derivative of $f$ with respect to $a$.\n",
    "\n",
    "<!-- --- end solution of exercise --- -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

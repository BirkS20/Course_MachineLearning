Topics to consider adding:

a) add about kernel methods after having discussed ridge and lasso regression

1) words on Bayesian ML

A group of machine learning algorithms where the fits and
optimizations are based on Bayesian statistics (Bayesâ€™ Theorem)
instead of traditional statistics and optimizations Assume outputs can
be described as distributions instead of as linear and nonlinear
combinations of inputs and hyperparameters are fit using priors
instead of being set by user Benefits: No hyperparameter tuning, no
validation data set, produced uncertainties on predictions


2) Discuss Kernel regression first and then add Bayesian Ridge regression
Bayesian Ridge Regression
Bayesian version of ridge regression (regularized linear regression)
Finds parameters and hyperparameters using Gaussian distributions 
Different results than ridge regression but does not depend on user-ser hyperparameter

3) GP 
Gaussian Processes
Bayesian version of kernel ridge regression or support vector machines
Similar to Bayesian ridge regression but uses the kernel trick to modify the inputs
Kernel: Modified Rational Quadratic Kernel

5) Update lectures on CNNs and RNNs

4) Add about generative models, AE, VAE and GANS, discuss energy based models and how to compute the partition function.
5) add discussions about Kullback-Leibner


Notes to self: look up thesis by Jane for RBMs and FNNs applied to many-body problems and more

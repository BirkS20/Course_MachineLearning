A group of machine learning algorithms where the fits and optimizations are based on Bayesian statistics (Bayesâ€™ Theorem) instead of traditional statistics and optimizations 
Assume outputs can be described as distributions instead of as linear and nonlinear combinations of inputs and hyperparameters are fit using priors instead of being set by user
Benefits: No hyperparameter tuning, no validation data set, produced uncertainties on predictions 


Bayesian Ridge Regression
Bayesian version of ridge regression (regularized linear regression)
Finds parameters and hyperparameters using Gaussian distributions 
Different results than ridge regression but does not depend on user-ser hyperparameter


Gaussian Processes
Bayesian version of kernel ridge regression or support vector machines
Similar to Bayesian ridge regression but uses the kernel trick to modify the inputs
Kernel: Modified Rational Quadratic Kernel
